{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook para o PAN - Atribuição Autoral - 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "#python basic libs\n",
    "from __future__ import print_function\n",
    "\n",
    "from tempfile import mkdtemp\n",
    "from shutil import rmtree\n",
    "import os;\n",
    "from os.path import join as pathjoin;\n",
    "\n",
    "import re;\n",
    "import glob;\n",
    "import json;\n",
    "import codecs;\n",
    "from collections import defaultdict;\n",
    "import pprint;\n",
    "\n",
    "\n",
    "from pprint import pprint\n",
    "from time import time\n",
    "import logging\n",
    "\n",
    "\n",
    "#data analysis libs\n",
    "import numpy as np;\n",
    "import pandas as pd;\n",
    "import matplotlib.pyplot as plt;\n",
    "import random;\n",
    "\n",
    "#machine learning libs\n",
    "#feature extraction\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "#preprocessing and transformation\n",
    "from sklearn.preprocessing import normalize, MaxAbsScaler, MinMaxScaler;\n",
    "from sklearn.preprocessing import LabelBinarizer;\n",
    "from sklearn.decomposition import PCA;\n",
    "from sklearn.metrics.pairwise import cosine_similarity;\n",
    "\n",
    "\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "\n",
    "#classifiers\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.multiclass import OneVsOneClassifier, OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from sklearn.feature_selection import RFE,SelectFpr,SelectPercentile, chi2;\n",
    "\n",
    "#\n",
    "from sklearn.ensemble import AdaBoostClassifier, BaggingClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "#model valuation\n",
    "from sklearn.model_selection import train_test_split;\n",
    "from sklearn.metrics import roc_auc_score, f1_score, precision_score, recall_score, accuracy_score;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Darwin-17.4.0-x86_64-i386-64bit\n",
      "NumPy 1.14.0\n",
      "SciPy 1.0.0\n",
      "Scikit-Learn 0.19.1\n"
     ]
    }
   ],
   "source": [
    "import platform; print(platform.platform())\n",
    "print(\"NumPy\", np.__version__)\n",
    "import scipy; print(\"SciPy\", scipy.__version__)\n",
    "import sklearn; print(\"Scikit-Learn\", sklearn.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### paths configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseDir = '/Users/joseeleandrocustodio/Dropbox/mestrado/02 - Pesquisa/code';\n",
    "\n",
    "inputDir= pathjoin(baseDir,'pan18aa');\n",
    "outputDir= pathjoin(baseDir,'out',\"oficial\");\n",
    "if not os.path.exists(outputDir):\n",
    "    os.mkdir(outputDir);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readCollectionsOfProblems(path):\n",
    "    # Reading information about the collection\n",
    "    infocollection = path+os.sep+'collection-info.json'\n",
    "    with open(infocollection, 'r') as f:\n",
    "        problems  = [\n",
    "            {\n",
    "                'problem': attrib['problem-name'],\n",
    "                'language': attrib['language'],\n",
    "                'encoding': attrib['encoding'],\n",
    "            }\n",
    "            for attrib in json.load(f)\n",
    "            \n",
    "        ]\n",
    "    return problems;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "problems = readCollectionsOfProblems(inputDir);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'encoding': u'UTF-8', 'language': u'en', 'problem': u'problem00001'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "problems[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readProblem(path, problem):\n",
    "    # Reading information about the problem\n",
    "    infoproblem = path+os.sep+problem+os.sep+'problem-info.json'\n",
    "    candidates = []\n",
    "    with open(infoproblem, 'r') as f:\n",
    "        fj = json.load(f)\n",
    "        unk_folder = fj['unknown-folder']\n",
    "        for attrib in fj['candidate-authors']:\n",
    "            candidates.append(attrib['author-name'])\n",
    "    return unk_folder, candidates;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_files(path,label):\n",
    "    # Reads all text files located in the 'path' and assigns them to 'label' class\n",
    "    files = glob.glob(pathjoin(path,label,'*.txt'))\n",
    "    texts=[]\n",
    "    for i,v in enumerate(files):\n",
    "        f=codecs.open(v,'r',encoding='utf-8')\n",
    "        texts.append((f.read(),label, os.path.basename(v)))\n",
    "        f.close()\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for index,problem in enumerate(problems):\n",
    "    unk_folder, candidates_folder = readProblem(inputDir, problem['problem']); \n",
    "    problem['candidates_folder_count'] = len(candidates_folder);\n",
    "    problem['candidates'] = [];\n",
    "    for candidate in candidates_folder:\n",
    "        problem['candidates'].extend(read_files(pathjoin(inputDir, problem['problem']),candidate));\n",
    "    \n",
    "    problem['unknown'] = read_files(pathjoin(inputDir, problem['problem']),unk_folder);    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>candidates</th>\n",
       "      <th>candidates_folder_count</th>\n",
       "      <th>encoding</th>\n",
       "      <th>language</th>\n",
       "      <th>problem</th>\n",
       "      <th>unknown</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[(graceful ones.\\n\\n\"One more,\" Marvelous said...</td>\n",
       "      <td>20</td>\n",
       "      <td>UTF-8</td>\n",
       "      <td>en</td>\n",
       "      <td>problem00001</td>\n",
       "      <td>[(after all, his best friends. And what in the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[(a mission.\"\\n\\nJensen just raises an eyebrow...</td>\n",
       "      <td>5</td>\n",
       "      <td>UTF-8</td>\n",
       "      <td>en</td>\n",
       "      <td>problem00002</td>\n",
       "      <td>[(“Potter was attractive,” Draco thought, sigh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[(qui l'avait tué mais tout était de la faute ...</td>\n",
       "      <td>20</td>\n",
       "      <td>UTF-8</td>\n",
       "      <td>fr</td>\n",
       "      <td>problem00003</td>\n",
       "      <td>[(son réveil. Sa main pulse et Draco frotte l'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[(. Le canapé est vide et lorsqu'il passe deva...</td>\n",
       "      <td>5</td>\n",
       "      <td>UTF-8</td>\n",
       "      <td>fr</td>\n",
       "      <td>problem00004</td>\n",
       "      <td>[(abasourdie.\\n\\nTout d'abord, elle crut que s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[(Eppure lui la mappa l’aveva stampata, dannaz...</td>\n",
       "      <td>20</td>\n",
       "      <td>UTF-8</td>\n",
       "      <td>it</td>\n",
       "      <td>problem00005</td>\n",
       "      <td>[(– Oh. Cazzo.\\nSirius era così sconvolto che ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[(Yato ha trovato una lettera sul suo comodino...</td>\n",
       "      <td>5</td>\n",
       "      <td>UTF-8</td>\n",
       "      <td>it</td>\n",
       "      <td>problem00006</td>\n",
       "      <td>[(così la tua vista, Moony?\\n– Cercavo di esse...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[(zmienił zdanie. Niech się stworzonko pobawi....</td>\n",
       "      <td>20</td>\n",
       "      <td>UTF-8</td>\n",
       "      <td>pl</td>\n",
       "      <td>problem00007</td>\n",
       "      <td>[(dawniej pełna radości i ciepła, a teraz wiec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[(Słowem, które Sherlock najczęściej słyszał w...</td>\n",
       "      <td>5</td>\n",
       "      <td>UTF-8</td>\n",
       "      <td>pl</td>\n",
       "      <td>problem00008</td>\n",
       "      <td>[(, uderzającego o żebra niczym dzwon- niemal ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[(pero no lo ama como ama a Guignol –explicó e...</td>\n",
       "      <td>20</td>\n",
       "      <td>UTF-8</td>\n",
       "      <td>sp</td>\n",
       "      <td>problem00009</td>\n",
       "      <td>[(–La nariz puntiaguda del elfo casi rozaba el...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[(incapaz de señalar un momento exacto, un pun...</td>\n",
       "      <td>5</td>\n",
       "      <td>UTF-8</td>\n",
       "      <td>sp</td>\n",
       "      <td>problem00010</td>\n",
       "      <td>[(tan parecidas hizo que su trasero latiese de...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          candidates  candidates_folder_count  \\\n",
       "0  [(graceful ones.\\n\\n\"One more,\" Marvelous said...                       20   \n",
       "1  [(a mission.\"\\n\\nJensen just raises an eyebrow...                        5   \n",
       "2  [(qui l'avait tué mais tout était de la faute ...                       20   \n",
       "3  [(. Le canapé est vide et lorsqu'il passe deva...                        5   \n",
       "4  [(Eppure lui la mappa l’aveva stampata, dannaz...                       20   \n",
       "5  [(Yato ha trovato una lettera sul suo comodino...                        5   \n",
       "6  [(zmienił zdanie. Niech się stworzonko pobawi....                       20   \n",
       "7  [(Słowem, które Sherlock najczęściej słyszał w...                        5   \n",
       "8  [(pero no lo ama como ama a Guignol –explicó e...                       20   \n",
       "9  [(incapaz de señalar un momento exacto, un pun...                        5   \n",
       "\n",
       "  encoding language       problem  \\\n",
       "0    UTF-8       en  problem00001   \n",
       "1    UTF-8       en  problem00002   \n",
       "2    UTF-8       fr  problem00003   \n",
       "3    UTF-8       fr  problem00004   \n",
       "4    UTF-8       it  problem00005   \n",
       "5    UTF-8       it  problem00006   \n",
       "6    UTF-8       pl  problem00007   \n",
       "7    UTF-8       pl  problem00008   \n",
       "8    UTF-8       sp  problem00009   \n",
       "9    UTF-8       sp  problem00010   \n",
       "\n",
       "                                             unknown  \n",
       "0  [(after all, his best friends. And what in the...  \n",
       "1  [(“Potter was attractive,” Draco thought, sigh...  \n",
       "2  [(son réveil. Sa main pulse et Draco frotte l'...  \n",
       "3  [(abasourdie.\\n\\nTout d'abord, elle crut que s...  \n",
       "4  [(– Oh. Cazzo.\\nSirius era così sconvolto che ...  \n",
       "5  [(così la tua vista, Moony?\\n– Cercavo di esse...  \n",
       "6  [(dawniej pełna radości i ciepła, a teraz wiec...  \n",
       "7  [(, uderzającego o żebra niczym dzwon- niemal ...  \n",
       "8  [(–La nariz puntiaguda del elfo casi rozaba el...  \n",
       "9  [(tan parecidas hizo que su trasero latiese de...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(problems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#*******************************************************************************************************\n",
    "import warnings\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "def eval_measures(gt, pred):\n",
    "    \"\"\"Compute macro-averaged F1-scores, macro-averaged precision, \n",
    "    macro-averaged recall, and micro-averaged accuracy according the ad hoc\n",
    "    rules discussed at the top of this file.\n",
    "    Parameters\n",
    "    ----------\n",
    "    gt : dict\n",
    "        Ground truth, where keys indicate text file names\n",
    "        (e.g. `unknown00002.txt`), and values represent\n",
    "        author labels (e.g. `candidate00003`)\n",
    "    pred : dict\n",
    "        Predicted attribution, where keys indicate text file names\n",
    "        (e.g. `unknown00002.txt`), and values represent\n",
    "        author labels (e.g. `candidate00003`)\n",
    "    Returns\n",
    "    -------\n",
    "    f1 : float\n",
    "        Macro-averaged F1-score\n",
    "    precision : float\n",
    "        Macro-averaged precision\n",
    "    recall : float\n",
    "        Macro-averaged recall\n",
    "    accuracy : float\n",
    "        Micro-averaged F1-score\n",
    "    \"\"\"\n",
    "\n",
    "    actual_authors = list(gt.values())\n",
    "    encoder = LabelEncoder().fit(['<UNK>'] + actual_authors)\n",
    "\n",
    "    text_ids, gold_authors, silver_authors = [], [], []\n",
    "    for text_id in sorted(gt):\n",
    "        text_ids.append(text_id)\n",
    "        gold_authors.append(gt[text_id])\n",
    "        try:\n",
    "            silver_authors.append(pred[text_id])\n",
    "        except KeyError:\n",
    "            # missing attributions get <UNK>:\n",
    "            silver_authors.append('<UNK>')\n",
    "\n",
    "    assert len(text_ids) == len(gold_authors)\n",
    "    assert len(text_ids) == len(silver_authors)\n",
    "\n",
    "    # replace non-existent silver authors with '<UNK>':\n",
    "    silver_authors = [a if a in encoder.classes_ else '<UNK>' \n",
    "                      for a in silver_authors]\n",
    "\n",
    "    gold_author_ints   = encoder.transform(gold_authors)\n",
    "    silver_author_ints = encoder.transform(silver_authors)\n",
    "\n",
    "    # get F1 for individual classes (and suppress warnings):\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter('ignore')\n",
    "        f1 = f1_score(gold_author_ints,\n",
    "                  silver_author_ints,\n",
    "                  labels=list(set(gold_author_ints)),\n",
    "                  average='macro')\n",
    "        precision = precision_score(gold_author_ints,\n",
    "                  silver_author_ints,\n",
    "                  labels=list(set(gold_author_ints)),\n",
    "                  average='macro')\n",
    "        recall = recall_score(gold_author_ints,\n",
    "                  silver_author_ints,\n",
    "                  labels=list(set(gold_author_ints)),\n",
    "                  average='macro')\n",
    "        accuracy = accuracy_score(gold_author_ints,\n",
    "                  silver_author_ints)\n",
    "\n",
    "    return f1,precision,recall,accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(ground_truth_file,predictions_file):\n",
    "    # Calculates evaluation measures for a single attribution problem\n",
    "    gt = {}\n",
    "    with open(ground_truth_file, 'r') as f:\n",
    "        for attrib in json.load(f)['ground_truth']:\n",
    "            gt[attrib['unknown-text']] = attrib['true-author']\n",
    "\n",
    "    pred = {}\n",
    "    with open(predictions_file, 'r') as f:\n",
    "        for attrib in json.load(f):\n",
    "            if attrib['unknown-text'] not in pred:\n",
    "                pred[attrib['unknown-text']] = attrib['predicted-author']\n",
    "    f1,precision,recall,accuracy =  eval_measures(gt,pred)\n",
    "    return f1, precision, recall, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "from scipy.sparse import issparse\n",
    "\n",
    "\n",
    "class DenseTransformer(BaseEstimator):\n",
    "    \"\"\"Convert a sparse array into a dense array.\"\"\"\n",
    "\n",
    "    def __init__(self, return_copy=True):\n",
    "        self.return_copy = return_copy\n",
    "        self.is_fitted = False\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        \"\"\" Return a dense version of the input array.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n",
    "            Training vectors, where n_samples is the number of samples and\n",
    "            n_features is the number of features.\n",
    "        y : array-like, shape = [n_samples] (default: None)\n",
    "        Returns\n",
    "        ---------\n",
    "        X_dense : dense version of the input X array.\n",
    "        \"\"\"\n",
    "        if issparse(X):\n",
    "            return X.toarray()\n",
    "        elif self.return_copy:\n",
    "            return X.copy()\n",
    "        else:\n",
    "            return X\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\" Mock method. Does nothing.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n",
    "            Training vectors, where n_samples is the number of samples and\n",
    "            n_features is the number of features.\n",
    "        y : array-like, shape = [n_samples] (default: None)\n",
    "        Returns\n",
    "        ---------\n",
    "        self\n",
    "        \"\"\"\n",
    "        self.is_fitted = True\n",
    "        return self\n",
    "\n",
    "    def fit_transform(self, X, y=None):\n",
    "        \"\"\" Return a dense version of the input array.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n",
    "            Training vectors, where n_samples is the number of samples and\n",
    "            n_features is the number of features.\n",
    "        y : array-like, shape = [n_samples] (default: None)\n",
    "        Returns\n",
    "        ---------\n",
    "        X_dense : dense version of the input X array.\n",
    "        \"\"\"\n",
    "        return self.transform(X=X, y=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runML(problem):\n",
    "    print (\"\\nProblem: %s,  language: %s, \" %(problem['problem'],problem['language']))\n",
    "    \n",
    "    train_docs, train_labels, _   = zip(*problem['candidates'])\n",
    "    problem['training_docs_size'] = len(train_docs);\n",
    "    test_docs, _, test_filename   = zip(*problem['unknown'])\n",
    "    \n",
    "    cachedir = mkdtemp()\n",
    "    pipeline = Pipeline([\n",
    "        ('vect',   TfidfVectorizer(analyzer='word',max_df=1.0, min_df=0.05, norm='l2', sublinear_tf=True)),\n",
    "        ('dense',  DenseTransformer()),\n",
    "        ('scaler', MinMaxScaler()),\n",
    "        ('selector', SelectFromModel(LinearSVC(C=0.1, penalty=\"l1\", dual=False))),\n",
    "        ('transf', PCA(0.99)),\n",
    "        ('clf', LogisticRegression(random_state=0,multi_class='multinomial', solver='newton-cg')),\n",
    "    ], memory=cachedir)\n",
    "    \n",
    "    \n",
    "    # uncommenting more parameters will give better exploring power but will\n",
    "    # increase processing time in a combinatorial way\n",
    "    parameters = {\n",
    "        'transf__n_components': (0.1,0.5,0.9,0.99),\n",
    "    }\n",
    "    \n",
    "    grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=1)\n",
    "    \n",
    "    print(\"Performing grid search...\")\n",
    "    t0 = time()\n",
    "    grid_search.fit(train_docs, train_labels)\n",
    "    print(\"done in %0.3fs\" % (time() - t0))\n",
    "\n",
    "    print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "    print(\"Best parameters set:\")\n",
    "    best_parameters = grid_search.best_estimator_.get_params()\n",
    "    for param_name in sorted(parameters.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "        \n",
    "    train_pred=grid_search.predict(train_docs);\n",
    "    test_pred=grid_search.predict(test_docs);\n",
    "    \n",
    "    \n",
    "    # Writing output file\n",
    "    out_data=[]\n",
    "    for i,v in enumerate(test_pred):\n",
    "        out_data.append(\n",
    "                {'unknown-text': test_filename[i],\n",
    "                 'predicted-author': v\n",
    "                }\n",
    "                )\n",
    "    answerFile = pathjoin(outputDir,'answers-'+problem['problem']+'.json');\n",
    "    with open(answerFile, 'w') as f:\n",
    "        json.dump(out_data, f, indent=4)\n",
    "        #allProblems.extend(out_data)\n",
    "    \n",
    "    \n",
    "    #evaluation train\n",
    "    f1,precision,recall,accuracy=evaluate(\n",
    "                pathjoin(inputDir, problem['problem'], 'ground-truth.json'),\n",
    "                answerFile)\n",
    "    rmtree(cachedir)\n",
    "    return {\n",
    "                'problem-name'   : problem['problem'],\n",
    "                \"train_doc_size\":len(train_docs),\n",
    "                \"language\":problem['language'],\n",
    "                'macro-f1'       : round(f1,3),\n",
    "                'macro-precision': round(precision,3),\n",
    "                'macro-recall'   : round(recall,3),\n",
    "                'micro-accuracy' : round(accuracy,3),\n",
    "                'AuthorCount':len(set(train_labels))\n",
    "        };"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Problem: problem00001,  language: en, \n",
      "Performing grid search...\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  12 out of  12 | elapsed:    1.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 2.606s\n",
      "Best score: 0.329\n",
      "Best parameters set:\n",
      "\ttransf__n_components: 0.99\n",
      "\n",
      "Problem: problem00002,  language: en, \n",
      "Performing grid search...\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  12 out of  12 | elapsed:    0.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 1.137s\n",
      "Best score: 0.771\n",
      "Best parameters set:\n",
      "\ttransf__n_components: 0.9\n",
      "\n",
      "Problem: problem00003,  language: fr, \n",
      "Performing grid search...\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  12 out of  12 | elapsed:    1.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 2.161s\n",
      "Best score: 0.221\n",
      "Best parameters set:\n",
      "\ttransf__n_components: 0.9\n",
      "\n",
      "Problem: problem00004,  language: fr, \n",
      "Performing grid search...\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  12 out of  12 | elapsed:    0.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 1.230s\n",
      "Best score: 0.429\n",
      "Best parameters set:\n",
      "\ttransf__n_components: 0.9\n",
      "\n",
      "Problem: problem00005,  language: it, \n",
      "Performing grid search...\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  12 out of  12 | elapsed:    1.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 2.265s\n",
      "Best score: 0.207\n",
      "Best parameters set:\n",
      "\ttransf__n_components: 0.99\n",
      "\n",
      "Problem: problem00006,  language: it, \n",
      "Performing grid search...\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  12 out of  12 | elapsed:    0.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 1.254s\n",
      "Best score: 0.429\n",
      "Best parameters set:\n",
      "\ttransf__n_components: 0.5\n",
      "\n",
      "Problem: problem00007,  language: pl, \n",
      "Performing grid search...\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  12 out of  12 | elapsed:    2.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 2.559s\n",
      "Best score: 0.436\n",
      "Best parameters set:\n",
      "\ttransf__n_components: 0.99\n",
      "\n",
      "Problem: problem00008,  language: pl, \n",
      "Performing grid search...\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  12 out of  12 | elapsed:    1.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 1.405s\n",
      "Best score: 0.429\n",
      "Best parameters set:\n",
      "\ttransf__n_components: 0.99\n",
      "\n",
      "Problem: problem00009,  language: sp, \n",
      "Performing grid search...\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  12 out of  12 | elapsed:    1.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 2.382s\n",
      "Best score: 0.293\n",
      "Best parameters set:\n",
      "\ttransf__n_components: 0.9\n",
      "\n",
      "Problem: problem00010,  language: sp, \n",
      "Performing grid search...\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  12 out of  12 | elapsed:    0.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 1.264s\n",
      "Best score: 0.429\n",
      "Best parameters set:\n",
      "\ttransf__n_components: 0.9\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AuthorCount</th>\n",
       "      <th>language</th>\n",
       "      <th>macro-f1</th>\n",
       "      <th>macro-precision</th>\n",
       "      <th>macro-recall</th>\n",
       "      <th>micro-accuracy</th>\n",
       "      <th>problem-name</th>\n",
       "      <th>train_doc_size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20</td>\n",
       "      <td>en</td>\n",
       "      <td>0.302</td>\n",
       "      <td>0.316</td>\n",
       "      <td>0.448</td>\n",
       "      <td>0.390</td>\n",
       "      <td>problem00001</td>\n",
       "      <td>140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>en</td>\n",
       "      <td>0.485</td>\n",
       "      <td>0.517</td>\n",
       "      <td>0.577</td>\n",
       "      <td>0.619</td>\n",
       "      <td>problem00002</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20</td>\n",
       "      <td>fr</td>\n",
       "      <td>0.283</td>\n",
       "      <td>0.342</td>\n",
       "      <td>0.270</td>\n",
       "      <td>0.306</td>\n",
       "      <td>problem00003</td>\n",
       "      <td>140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>fr</td>\n",
       "      <td>0.387</td>\n",
       "      <td>0.469</td>\n",
       "      <td>0.557</td>\n",
       "      <td>0.381</td>\n",
       "      <td>problem00004</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20</td>\n",
       "      <td>it</td>\n",
       "      <td>0.154</td>\n",
       "      <td>0.199</td>\n",
       "      <td>0.320</td>\n",
       "      <td>0.175</td>\n",
       "      <td>problem00005</td>\n",
       "      <td>140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>it</td>\n",
       "      <td>0.451</td>\n",
       "      <td>0.519</td>\n",
       "      <td>0.549</td>\n",
       "      <td>0.587</td>\n",
       "      <td>problem00006</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>20</td>\n",
       "      <td>pl</td>\n",
       "      <td>0.161</td>\n",
       "      <td>0.201</td>\n",
       "      <td>0.211</td>\n",
       "      <td>0.204</td>\n",
       "      <td>problem00007</td>\n",
       "      <td>140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5</td>\n",
       "      <td>pl</td>\n",
       "      <td>0.422</td>\n",
       "      <td>0.424</td>\n",
       "      <td>0.667</td>\n",
       "      <td>0.467</td>\n",
       "      <td>problem00008</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>20</td>\n",
       "      <td>sp</td>\n",
       "      <td>0.152</td>\n",
       "      <td>0.235</td>\n",
       "      <td>0.197</td>\n",
       "      <td>0.205</td>\n",
       "      <td>problem00009</td>\n",
       "      <td>140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5</td>\n",
       "      <td>sp</td>\n",
       "      <td>0.377</td>\n",
       "      <td>0.425</td>\n",
       "      <td>0.400</td>\n",
       "      <td>0.438</td>\n",
       "      <td>problem00010</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   AuthorCount language  macro-f1  macro-precision  macro-recall  \\\n",
       "0           20       en     0.302            0.316         0.448   \n",
       "1            5       en     0.485            0.517         0.577   \n",
       "2           20       fr     0.283            0.342         0.270   \n",
       "3            5       fr     0.387            0.469         0.557   \n",
       "4           20       it     0.154            0.199         0.320   \n",
       "5            5       it     0.451            0.519         0.549   \n",
       "6           20       pl     0.161            0.201         0.211   \n",
       "7            5       pl     0.422            0.424         0.667   \n",
       "8           20       sp     0.152            0.235         0.197   \n",
       "9            5       sp     0.377            0.425         0.400   \n",
       "\n",
       "   micro-accuracy  problem-name  train_doc_size  \n",
       "0           0.390  problem00001             140  \n",
       "1           0.619  problem00002              35  \n",
       "2           0.306  problem00003             140  \n",
       "3           0.381  problem00004              35  \n",
       "4           0.175  problem00005             140  \n",
       "5           0.587  problem00006              35  \n",
       "6           0.204  problem00007             140  \n",
       "7           0.467  problem00008              35  \n",
       "8           0.205  problem00009             140  \n",
       "9           0.438  problem00010              35  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = [];\n",
    "for problem in problems:\n",
    "    result.append(runML(problem));\n",
    "pd.DataFrame(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>macro-f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.317400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.127023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.152000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.191500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.339500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.413250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.485000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        macro-f1\n",
       "count  10.000000\n",
       "mean    0.317400\n",
       "std     0.127023\n",
       "min     0.152000\n",
       "25%     0.191500\n",
       "50%     0.339500\n",
       "75%     0.413250\n",
       "max     0.485000"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(result)[['macro-f1']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(result)[['macro-f1']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(result)\\\n",
    "    .sort_values(by=['language','problem-name'])[['language','problem-name','macro-f1']]\\\n",
    "    .plot(kind='bar', x=['language','problem-name'], legend=True, figsize=(20,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/><br/><br/><br/>\n",
    "\n",
    "#  Abordagem desafiante 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NgramSplitter(object):\n",
    "    def __init__(self, text, ngram=(3,3), vocabulary=None):\n",
    "        self.text = text\n",
    "        self.ngram_min = ngram[0]\n",
    "        self.ngram_max = ngram[1];\n",
    "        self.vocabulary = vocabulary;\n",
    "    \n",
    "    def text2ngrams(self,text):\n",
    "        vect = [\n",
    "            text[t:t+j]\n",
    "                for t in xrange(len(text)-self.ngram_max+1)\n",
    "                for j in xrange(self.ngram_min, self.ngram_max+1)\n",
    "        ]\n",
    "        \n",
    "        if self.vocabulary is not None:\n",
    "            return [word for word in vect if word in self.vocabulary];\n",
    "        else:\n",
    "            return [word for word in vect if word]\n",
    " \n",
    "    def __iter__(self):\n",
    "        if isinstance(self.text,list):\n",
    "            for s in self.text:\n",
    "                yield self.text2ngrams(s);\n",
    "        elif isinstance(self.text,str) or isinstance(self.text,unicode):\n",
    "            yield self.text2ngrams(self.text);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2VecClassifier(BaseEstimator, ClassifierMixin):  \n",
    "    \"\"\"A classifier that uses classes embeddings to classify instances\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "            self,\n",
    "            ngram = (3,4),\n",
    "            analyzer = 'char',\n",
    "            min_df = 0.3,\n",
    "            max_df = 1.0,\n",
    "        \n",
    "            min_count =2,\n",
    "            embeddingSize =750,\n",
    "            window=10,\n",
    "            algorithm = 0,\n",
    "            iter =10\n",
    "        ):\n",
    "        \"\"\"\n",
    "        Called when initializing the classifier\n",
    "        \"\"\"\n",
    "        self.algorithm     = algorithm\n",
    "        self.min_count     = min_count\n",
    "        self.embeddingSize = embeddingSize\n",
    "        self.window        = window\n",
    "        self.iter          = iter\n",
    "        self.analyzer      = analyzer\n",
    "        self.vocabulary_   = {}\n",
    "        self.ngram         = ngram\n",
    "        self.min_df        = min_df\n",
    "        self.max_df        = max_df\n",
    "\n",
    "    def _buildVectorModel(self, document):\n",
    "        sentenseGenerator = NgramSplitter(document,self.ngram, self.vocabulary_);\n",
    "        \n",
    "        model = Word2Vec(\n",
    "            sentenseGenerator,\n",
    "            sg       = self.algorithm,\n",
    "            iter     = self.iter,        \n",
    "            min_count= self.min_count,\n",
    "            window   = self.window,\n",
    "            size     = self.embeddingSize,\n",
    "            seed=0\n",
    "        );\n",
    "        return model.wv;\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Sumarize one text per labels and transform the text into word vectors\n",
    "        \"\"\"\n",
    "        \n",
    "        #creating author profile\n",
    "        profile = defaultdict(unicode);\n",
    "        for text, label in zip(X,y):\n",
    "            profile[label]+=text;\n",
    "            \n",
    "        #build a global vocaculary / Using count vectorizer to create a fixed vocabulary\n",
    "        vectorizer = CountVectorizer(\n",
    "                analyzer=self.analyzer,\n",
    "                ngram_range=self.ngram,\n",
    "                min_df=self.min_df,\n",
    "                max_df=self.max_df,\n",
    "                lowercase=False\n",
    "        )\n",
    "        vectorizer.fit(X);\n",
    "        self.vocabulary_ = vectorizer.vocabulary_\n",
    "        \n",
    "        # profile vector represent each author in the embedding space\n",
    "        self.profileVectors_ = {y: self._buildVectorModel(profile[y]) for y in y};\n",
    "\n",
    "        return self\n",
    "    \n",
    "    \n",
    "    def _minmax(self, a):\n",
    "        a = (a - a.min())/(a.max() - a.min());\n",
    "        return a;\n",
    "        \n",
    "    def _simpleCosine(self,a, b):\n",
    "        '''\n",
    "        calculates cosine between array a and b.\n",
    "        This function is used because sklearn similiraty function compares all elements vs all elements\n",
    "        what will not be used. So this function becames handy.\n",
    "        '''\n",
    "        a = a / np.sqrt(np.sum(a **2));\n",
    "        b = b / np.sqrt(np.sum(b **2));\n",
    "        cos = np.sum(np.array(a) * np.array(b));\n",
    "        return cos;\n",
    "    \n",
    "    def _KLD(self,p, q):\n",
    "        p = self._minmax(p); p = p/p.sum();\n",
    "        q = self._minmax(q); q = q/q.sum();\n",
    "        \n",
    "        cond = ((q != 0)&(p != 0));\n",
    "        k1 = np.sum(np.where(cond, p * np.log(p / q), 0));\n",
    "        return k1;\n",
    "    \n",
    "    def _manhattan(self,p, q):\n",
    "        p = self._minmax(p); p = p/p.sum();\n",
    "        q = self._minmax(q); q = q/q.sum();\n",
    "        return np.mean(np.abs(p-q));\n",
    "    \n",
    "    \n",
    "    def _guassian(self, C,D):\n",
    "        cond = C-D !=0;\n",
    "        bc = np.where(cond,(C-D+1)**2/(2*np.maximum(C,D+1)),1);\n",
    "        return np.sum(-np.log(bc));\n",
    "\n",
    "\n",
    "    def score(self, X, y=None):\n",
    "        # counts number of values bigger than mean\n",
    "        return(sum(self.predict(X)))\n",
    "    \n",
    "    def _softMax(self,a):\n",
    "        a = self._minmax(a);\n",
    "        a = np.exp(a)/np.sum(np.exp(a))\n",
    "        return a;\n",
    "    \n",
    "    def _predict1Doc(self, docVect):\n",
    "        vocabDoc = set(docVect.vocab.keys());\n",
    "    \n",
    "        metrics = [];\n",
    "        \n",
    "        def c(aa,bb, funct):\n",
    "            voc = set(aa.vocab.keys()) & set(bb.vocab.keys())\n",
    "            f = np.array([\n",
    "                funct(aa[v], bb[v])\n",
    "                for v in voc\n",
    "            ]);\n",
    "            f = np.sum(f)\n",
    "            return f;\n",
    "    \n",
    "        for label in self.profileVectors_:\n",
    "            labelVocab = set(self.profileVectors_[label].vocab.keys());\n",
    "            intersect  = vocabDoc & labelVocab;\n",
    "            union      = len(vocabDoc | labelVocab);\n",
    "            jaccard    = 1.0*len(intersect) / union;\n",
    "            \n",
    "            metrics.append({\n",
    "                'label'       : label,\n",
    "                'jaccard'     : jaccard,\n",
    "                'lenIntersect': len(intersect),\n",
    "                'lenUnion'    : union,\n",
    "                'lenMax'      : max(len(labelVocab), len(vocabDoc)),\n",
    "                'similarity'  : c(docVect, self.profileVectors_[label], self._simpleCosine),\n",
    "                'KLD'         : c(docVect, self.profileVectors_[label], self._KLD),\n",
    "                'manhattan'   : c(docVect, self.profileVectors_[label], self._manhattan),\n",
    "                'guassian'    : c(docVect, self.profileVectors_[label], self._guassian),\n",
    "                \n",
    "            })\n",
    "        #softmax norm\n",
    "        similarity = self._softMax(np.array([c['similarity'] for c in metrics ]));\n",
    "        guassian   = self._softMax(np.array([c['guassian'] for c in metrics ]));\n",
    "        manhattan  = self._softMax(np.array([c['manhattan'] for c in metrics ]));\n",
    "    \n",
    "        #appending normalized sum of distance\n",
    "        for i,c in enumerate(metrics):\n",
    "            c.update({\n",
    "                'similarityNorm': similarity[i],\n",
    "                'guassianNorm': guassian[i],\n",
    "                'manhattanNorm': manhattan[i]\n",
    "            })\n",
    "    \n",
    "        return metrics;\n",
    "    \n",
    "    def predict(self, X, y=None):\n",
    "        try:\n",
    "            getattr(self, \"profileVectors_\")\n",
    "        except AttributeError:\n",
    "            raise RuntimeError(\"You must train classifer before predicting data!\")\n",
    "            \n",
    "        docVectors    = [self._buildVectorModel(x) for x in X];\n",
    "        self.metrics_ = [self._predict1Doc(v)      for v in docVectors];\n",
    "        \n",
    "        result = [];\n",
    "        for r in self.metrics_:\n",
    "            best = r[0];\n",
    "            best['bestMatch'] = True;\n",
    "            for rr in r:\n",
    "                if rr != best:\n",
    "                    rr['bestMatch'] = False;\n",
    "                if rr['similarityNorm'] > best['similarityNorm'] :\n",
    "                    best['bestMatch'] = False;\n",
    "                    best = rr;\n",
    "                    best['bestMatch'] = True;\n",
    "            result.append(best);\n",
    "            \n",
    "        self.predited_ = result;\n",
    "\n",
    "        return([r['label'] for r in result])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem = problems[8];\n",
    "print (\"Problem: %s,  language: %s, \" %(problem['problem'],problem['language']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2VecClassifier();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_docs, train_labels,_ = zip(*problem['candidates']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_docs,train_labels);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainPred = model.predict(train_docs);\n",
    "trainMetrics = model.metrics_;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame(zip(train_labels,trainPred), columns=['label','pred'])\n",
    "df.label = df.label.apply(lambda x: int(re.sub(r'\\D','',x)));\n",
    "df.pred = df.pred.apply(lambda x: int(re.sub(r'\\D','',x)));\n",
    "df.plot.scatter(x='label',y='pred');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m  = trainMetrics\n",
    "df = pd.DataFrame([item for s in m for item in s])\n",
    "df['doc']      = [i               for i,s in enumerate(m) for item in s]\n",
    "df['solution'] = [train_labels[i] for i,s in enumerate(m) for item in s]\n",
    "df.sort_values(by=['doc','similarityNorm', 'manhattan'], ascending=[True,False,True], inplace=True)\n",
    "df['distance'] = [i for i in range(len(set(train_labels)))]* len(trainMetrics)\n",
    "df[df.doc == 55]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df[df.bestMatch].copy();\n",
    "df2['correct'] = df2.apply(lambda x: x['label'] == x['solution'], axis=1)\n",
    "df2[['correct','doc']].groupby(by='correct').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df2 = df[df.bestMatch].copy();\n",
    "df2['correct'] = df2.apply(lambda x: x['label'] == x['solution'], axis=1)\n",
    "df2[['correct','doc']].groupby(by='correct').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.solution == df.label].plot.scatter(x='distance', y='manhattanNorm')\n",
    "df[df.solution == df.label].plot.scatter(x='distance', y='guassianNorm')\n",
    "df[df.solution == df.label].plot.scatter(x='distance', y='similarityNorm')\n",
    "df[df.solution == df.label].plot.scatter(x='manhattanNorm', y='guassianNorm', c='distance',colormap='Reds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code from baseline\n",
    "gt = {}\n",
    "with open(pathjoin(inputDir, problem['problem'], 'ground-truth.json'), 'r') as f:\n",
    "    for attrib in json.load(f)['ground_truth']:\n",
    "        gt[attrib['unknown-text']] = attrib['true-author']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_docs, _, test_filename = zip(*problem['unknown'])\n",
    "test_labels = [gt[v] for v in test_filename]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "testPred = model.predict(test_docs);\n",
    "testMetrics = model.metrics_;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m  = testMetrics\n",
    "df = pd.DataFrame([item for s in m for item in s])\n",
    "df['doc']      = [i               for i,s in enumerate(m) for item in s]\n",
    "df['solution'] = [train_labels[i] for i,s in enumerate(m) for item in s]\n",
    "df.sort_values(by=['doc','similarityNorm', 'KLD'], ascending=[True,False,True], inplace=True)\n",
    "df['distance'] = [i for i in range(len(set(train_labels)))]* len(testMetrics)\n",
    "df[df.doc == 55]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1,precision,recall,accuracy =  eval_measures(gt,{k: v for k,v in zip(test_filename, testPred)  })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame([{\n",
    "                'macro-f1'       : round(f1,3),\n",
    "                'macro-precision': round(precision,3),\n",
    "                'macro-recall'   : round(recall,3),\n",
    "                'micro-accuracy' : round(accuracy,3)\n",
    "             }])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df[df.bestMatch].copy();\n",
    "df2['correct'] = df2.apply(lambda x: x['label'] == x['solution'], axis=1)\n",
    "df2[['correct','doc']].groupby(by='correct').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.solution == df.label].plot.scatter(x='distance', y='guassianNorm')\n",
    "df[df.solution == df.label].plot.scatter(x='distance', y='manhattanNorm')\n",
    "df[df.solution == df.label].plot.scatter(x='distance', y='similarityNorm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.solution == df.label]\\\n",
    "    .plot\\\n",
    "    .scatter(\n",
    "        x='guassianNorm',\n",
    "        y='similarityNorm',\n",
    "        c='distance',\n",
    "        colormap='Reds',\n",
    "        figsize=(20,5));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
