{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook para o PAN - Atribuição Autoral - 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "#python basic libs\n",
    "from __future__ import print_function\n",
    "\n",
    "from tempfile import mkdtemp\n",
    "from shutil import rmtree\n",
    "import os;\n",
    "from os.path import join as pathjoin;\n",
    "\n",
    "import re;\n",
    "import glob;\n",
    "import json;\n",
    "import codecs;\n",
    "from collections import defaultdict;\n",
    "import pprint;\n",
    "\n",
    "\n",
    "from pprint import pprint\n",
    "from time import time\n",
    "import logging\n",
    "\n",
    "\n",
    "#data analysis libs\n",
    "import numpy as np;\n",
    "import pandas as pd;\n",
    "import matplotlib.pyplot as plt;\n",
    "import random;\n",
    "\n",
    "#machine learning libs\n",
    "#feature extraction\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "\n",
    "#preprocessing and transformation\n",
    "from sklearn.preprocessing import normalize, Normalizer, MaxAbsScaler, MinMaxScaler, LabelBinarizer;\n",
    "from sklearn.decomposition import PCA;\n",
    "from sklearn.metrics.pairwise import cosine_similarity;\n",
    "\n",
    "\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "\n",
    "#classifiers\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from sklearn.feature_selection import RFE,SelectFpr,SelectPercentile, chi2;\n",
    "\n",
    "#\n",
    "from sklearn.ensemble import AdaBoostClassifier, BaggingClassifier, VotingClassifier\n",
    "\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "#model valuation\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import roc_auc_score, f1_score, precision_score, recall_score, accuracy_score;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Darwin-17.5.0-x86_64-i386-64bit\n",
      "NumPy 1.14.2\n",
      "SciPy 1.0.1\n",
      "Scikit-Learn 0.19.1\n"
     ]
    }
   ],
   "source": [
    "import platform; print(platform.platform())\n",
    "print(\"NumPy\", np.__version__)\n",
    "import scipy; print(\"SciPy\", scipy.__version__)\n",
    "import sklearn; print(\"Scikit-Learn\", sklearn.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### paths configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseDir = '/Users/joseeleandrocustodio/Dropbox/mestrado/02 - Pesquisa/code';\n",
    "\n",
    "inputDir= pathjoin(baseDir,'pan18aa');\n",
    "outputDir= pathjoin(baseDir,'out',\"oficial\");\n",
    "if not os.path.exists(outputDir):\n",
    "    os.mkdir(outputDir);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readCollectionsOfProblems(path):\n",
    "    # Reading information about the collection\n",
    "    infocollection = path+os.sep+'collection-info.json'\n",
    "    with open(infocollection, 'r') as f:\n",
    "        problems  = [\n",
    "            {\n",
    "                'problem': attrib['problem-name'],\n",
    "                'language': attrib['language'],\n",
    "                'encoding': attrib['encoding'],\n",
    "            }\n",
    "            for attrib in json.load(f)\n",
    "            \n",
    "        ]\n",
    "    return problems;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "problems = readCollectionsOfProblems(inputDir);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'encoding': u'UTF-8', 'language': u'en', 'problem': u'problem00001'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "problems[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readProblem(path, problem):\n",
    "    # Reading information about the problem\n",
    "    infoproblem = path+os.sep+problem+os.sep+'problem-info.json'\n",
    "    candidates = []\n",
    "    with open(infoproblem, 'r') as f:\n",
    "        fj = json.load(f)\n",
    "        unk_folder = fj['unknown-folder']\n",
    "        for attrib in fj['candidate-authors']:\n",
    "            candidates.append(attrib['author-name'])\n",
    "    return unk_folder, candidates;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_files(path,label):\n",
    "    # Reads all text files located in the 'path' and assigns them to 'label' class\n",
    "    files = glob.glob(pathjoin(path,label,'*.txt'))\n",
    "    texts=[]\n",
    "    for i,v in enumerate(files):\n",
    "        f=codecs.open(v,'r',encoding='utf-8')\n",
    "        texts.append((f.read(),label, os.path.basename(v)))\n",
    "        f.close()\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for index,problem in enumerate(problems):\n",
    "    unk_folder, candidates_folder = readProblem(inputDir, problem['problem']); \n",
    "    problem['candidates_folder_count'] = len(candidates_folder);\n",
    "    problem['candidates'] = [];\n",
    "    for candidate in candidates_folder:\n",
    "        problem['candidates'].extend(read_files(pathjoin(inputDir, problem['problem']),candidate));\n",
    "    \n",
    "    problem['unknown'] = read_files(pathjoin(inputDir, problem['problem']),unk_folder);    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>candidates</th>\n",
       "      <th>candidates_folder_count</th>\n",
       "      <th>encoding</th>\n",
       "      <th>language</th>\n",
       "      <th>problem</th>\n",
       "      <th>unknown</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[(graceful ones.\\n\\n\"One more,\" Marvelous said...</td>\n",
       "      <td>20</td>\n",
       "      <td>UTF-8</td>\n",
       "      <td>en</td>\n",
       "      <td>problem00001</td>\n",
       "      <td>[(after all, his best friends. And what in the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[(a mission.\"\\n\\nJensen just raises an eyebrow...</td>\n",
       "      <td>5</td>\n",
       "      <td>UTF-8</td>\n",
       "      <td>en</td>\n",
       "      <td>problem00002</td>\n",
       "      <td>[(“Potter was attractive,” Draco thought, sigh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[(qui l'avait tué mais tout était de la faute ...</td>\n",
       "      <td>20</td>\n",
       "      <td>UTF-8</td>\n",
       "      <td>fr</td>\n",
       "      <td>problem00003</td>\n",
       "      <td>[(son réveil. Sa main pulse et Draco frotte l'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[(. Le canapé est vide et lorsqu'il passe deva...</td>\n",
       "      <td>5</td>\n",
       "      <td>UTF-8</td>\n",
       "      <td>fr</td>\n",
       "      <td>problem00004</td>\n",
       "      <td>[(abasourdie.\\n\\nTout d'abord, elle crut que s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[(Eppure lui la mappa l’aveva stampata, dannaz...</td>\n",
       "      <td>20</td>\n",
       "      <td>UTF-8</td>\n",
       "      <td>it</td>\n",
       "      <td>problem00005</td>\n",
       "      <td>[(– Oh. Cazzo.\\nSirius era così sconvolto che ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[(Yato ha trovato una lettera sul suo comodino...</td>\n",
       "      <td>5</td>\n",
       "      <td>UTF-8</td>\n",
       "      <td>it</td>\n",
       "      <td>problem00006</td>\n",
       "      <td>[(così la tua vista, Moony?\\n– Cercavo di esse...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[(zmienił zdanie. Niech się stworzonko pobawi....</td>\n",
       "      <td>20</td>\n",
       "      <td>UTF-8</td>\n",
       "      <td>pl</td>\n",
       "      <td>problem00007</td>\n",
       "      <td>[(dawniej pełna radości i ciepła, a teraz wiec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[(Słowem, które Sherlock najczęściej słyszał w...</td>\n",
       "      <td>5</td>\n",
       "      <td>UTF-8</td>\n",
       "      <td>pl</td>\n",
       "      <td>problem00008</td>\n",
       "      <td>[(, uderzającego o żebra niczym dzwon- niemal ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[(pero no lo ama como ama a Guignol –explicó e...</td>\n",
       "      <td>20</td>\n",
       "      <td>UTF-8</td>\n",
       "      <td>sp</td>\n",
       "      <td>problem00009</td>\n",
       "      <td>[(–La nariz puntiaguda del elfo casi rozaba el...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[(incapaz de señalar un momento exacto, un pun...</td>\n",
       "      <td>5</td>\n",
       "      <td>UTF-8</td>\n",
       "      <td>sp</td>\n",
       "      <td>problem00010</td>\n",
       "      <td>[(tan parecidas hizo que su trasero latiese de...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          candidates  candidates_folder_count  \\\n",
       "0  [(graceful ones.\\n\\n\"One more,\" Marvelous said...                       20   \n",
       "1  [(a mission.\"\\n\\nJensen just raises an eyebrow...                        5   \n",
       "2  [(qui l'avait tué mais tout était de la faute ...                       20   \n",
       "3  [(. Le canapé est vide et lorsqu'il passe deva...                        5   \n",
       "4  [(Eppure lui la mappa l’aveva stampata, dannaz...                       20   \n",
       "5  [(Yato ha trovato una lettera sul suo comodino...                        5   \n",
       "6  [(zmienił zdanie. Niech się stworzonko pobawi....                       20   \n",
       "7  [(Słowem, które Sherlock najczęściej słyszał w...                        5   \n",
       "8  [(pero no lo ama como ama a Guignol –explicó e...                       20   \n",
       "9  [(incapaz de señalar un momento exacto, un pun...                        5   \n",
       "\n",
       "  encoding language       problem  \\\n",
       "0    UTF-8       en  problem00001   \n",
       "1    UTF-8       en  problem00002   \n",
       "2    UTF-8       fr  problem00003   \n",
       "3    UTF-8       fr  problem00004   \n",
       "4    UTF-8       it  problem00005   \n",
       "5    UTF-8       it  problem00006   \n",
       "6    UTF-8       pl  problem00007   \n",
       "7    UTF-8       pl  problem00008   \n",
       "8    UTF-8       sp  problem00009   \n",
       "9    UTF-8       sp  problem00010   \n",
       "\n",
       "                                             unknown  \n",
       "0  [(after all, his best friends. And what in the...  \n",
       "1  [(“Potter was attractive,” Draco thought, sigh...  \n",
       "2  [(son réveil. Sa main pulse et Draco frotte l'...  \n",
       "3  [(abasourdie.\\n\\nTout d'abord, elle crut que s...  \n",
       "4  [(– Oh. Cazzo.\\nSirius era così sconvolto che ...  \n",
       "5  [(così la tua vista, Moony?\\n– Cercavo di esse...  \n",
       "6  [(dawniej pełna radości i ciepła, a teraz wiec...  \n",
       "7  [(, uderzającego o żebra niczym dzwon- niemal ...  \n",
       "8  [(–La nariz puntiaguda del elfo casi rozaba el...  \n",
       "9  [(tan parecidas hizo que su trasero latiese de...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(problems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#*******************************************************************************************************\n",
    "import warnings\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "def eval_measures(gt, pred):\n",
    "    \"\"\"Compute macro-averaged F1-scores, macro-averaged precision, \n",
    "    macro-averaged recall, and micro-averaged accuracy according the ad hoc\n",
    "    rules discussed at the top of this file.\n",
    "    Parameters\n",
    "    ----------\n",
    "    gt : dict\n",
    "        Ground truth, where keys indicate text file names\n",
    "        (e.g. `unknown00002.txt`), and values represent\n",
    "        author labels (e.g. `candidate00003`)\n",
    "    pred : dict\n",
    "        Predicted attribution, where keys indicate text file names\n",
    "        (e.g. `unknown00002.txt`), and values represent\n",
    "        author labels (e.g. `candidate00003`)\n",
    "    Returns\n",
    "    -------\n",
    "    f1 : float\n",
    "        Macro-averaged F1-score\n",
    "    precision : float\n",
    "        Macro-averaged precision\n",
    "    recall : float\n",
    "        Macro-averaged recall\n",
    "    accuracy : float\n",
    "        Micro-averaged F1-score\n",
    "    \"\"\"\n",
    "\n",
    "    actual_authors = list(gt.values())\n",
    "    encoder = LabelEncoder().fit(['<UNK>'] + actual_authors)\n",
    "\n",
    "    text_ids, gold_authors, silver_authors = [], [], []\n",
    "    for text_id in sorted(gt):\n",
    "        text_ids.append(text_id)\n",
    "        gold_authors.append(gt[text_id])\n",
    "        try:\n",
    "            silver_authors.append(pred[text_id])\n",
    "        except KeyError:\n",
    "            # missing attributions get <UNK>:\n",
    "            silver_authors.append('<UNK>')\n",
    "\n",
    "    assert len(text_ids) == len(gold_authors)\n",
    "    assert len(text_ids) == len(silver_authors)\n",
    "\n",
    "    # replace non-existent silver authors with '<UNK>':\n",
    "    silver_authors = [a if a in encoder.classes_ else '<UNK>' \n",
    "                      for a in silver_authors]\n",
    "\n",
    "    gold_author_ints   = encoder.transform(gold_authors)\n",
    "    silver_author_ints = encoder.transform(silver_authors)\n",
    "\n",
    "    # get F1 for individual classes (and suppress warnings):\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter('ignore')\n",
    "        f1 = f1_score(gold_author_ints,\n",
    "                  silver_author_ints,\n",
    "                  labels=list(set(gold_author_ints)),\n",
    "                  average='macro')\n",
    "        precision = precision_score(gold_author_ints,\n",
    "                  silver_author_ints,\n",
    "                  labels=list(set(gold_author_ints)),\n",
    "                  average='macro')\n",
    "        recall = recall_score(gold_author_ints,\n",
    "                  silver_author_ints,\n",
    "                  labels=list(set(gold_author_ints)),\n",
    "                  average='macro')\n",
    "        accuracy = accuracy_score(gold_author_ints,\n",
    "                  silver_author_ints)\n",
    "\n",
    "    return f1,precision,recall,accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(ground_truth_file,predictions_file):\n",
    "    # Calculates evaluation measures for a single attribution problem\n",
    "    gt = {}\n",
    "    with open(ground_truth_file, 'r') as f:\n",
    "        for attrib in json.load(f)['ground_truth']:\n",
    "            gt[attrib['unknown-text']] = attrib['true-author']\n",
    "\n",
    "    pred = {}\n",
    "    with open(predictions_file, 'r') as f:\n",
    "        for attrib in json.load(f):\n",
    "            if attrib['unknown-text'] not in pred:\n",
    "                pred[attrib['unknown-text']] = attrib['predicted-author']\n",
    "    f1,precision,recall,accuracy =  eval_measures(gt,pred)\n",
    "    return f1, precision, recall, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "from scipy.sparse import issparse\n",
    "\n",
    "\n",
    "class DenseTransformer(BaseEstimator):\n",
    "    def __init__(self, return_copy=True):\n",
    "        self.return_copy = return_copy\n",
    "        self.is_fitted = False\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        if issparse(X):\n",
    "            return X.toarray()\n",
    "        elif self.return_copy:\n",
    "            return X.copy()\n",
    "        else:\n",
    "            return X\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.is_fitted = True\n",
    "        return self\n",
    "\n",
    "    def fit_transform(self, X, y=None):\n",
    "        return self.transform(X=X, y=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "from scipy.sparse import issparse\n",
    "\n",
    "\n",
    "class ObfuscationTransformer(BaseEstimator):\n",
    "    def __init__(self,re_from=r'(\\b)(\\w{0,2})\\w+(\\w{1,3})(\\b)', re_to=r'\\1\\2XX\\3\\4', return_copy=True):\n",
    "        self.re_from = re_from\n",
    "        self.re_to = re_to\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        X = np.array(X).copy();\n",
    "        for i in range(len(X)):\n",
    "            X[i] = re.sub(self.re_from,self.re_to, X[i])\n",
    "        \n",
    "        return X;\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def fit_transform(self, X, y=None):\n",
    "        return self.transform(X=X, y=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runML(problem):\n",
    "    print (\"Problem: %s,  language: %s, \" %(problem['problem'],problem['language']))\n",
    "    \n",
    "    train_docs, train_labels, _   = zip(*problem['candidates'])\n",
    "    problem['training_docs_size'] = len(train_docs);\n",
    "    test_docs, _, test_filename   = zip(*problem['unknown'])\n",
    "    \n",
    "    \n",
    "    pipeline1 = Pipeline([\n",
    "        ('vect',   TfidfVectorizer(\n",
    "                        analyzer='char',\n",
    "                        ngram_range=(3,5),\n",
    "                        min_df=0.01,\n",
    "                        max_df=1.0,\n",
    "                        norm='l1',\n",
    "                        lowercase =False,\n",
    "                        sublinear_tf=True)),\n",
    "        ('dense',  DenseTransformer()),\n",
    "        ('scaler', MaxAbsScaler()),\n",
    "        ('transf', PCA(0.99)),\n",
    "        ('clf', LogisticRegression(random_state=0,multi_class='multinomial', solver='newton-cg')),\n",
    "    ])\n",
    "    \n",
    "    pipeline2 = Pipeline([\n",
    "        ('obs',ObfuscationTransformer(re_from=r'\\w',re_to='x')),\n",
    "        ('vect',   TfidfVectorizer(\n",
    "                        analyzer='char',\n",
    "                        ngram_range=(3,5),\n",
    "                        min_df=0.01,\n",
    "                        max_df=1.0,\n",
    "                        norm='l1',\n",
    "                        lowercase =False,\n",
    "                        sublinear_tf=True)),\n",
    "        ('dense',  DenseTransformer()),\n",
    "        ('scaler', MaxAbsScaler()),\n",
    "        ('transf', PCA(0.99)),\n",
    "        ('clf', LogisticRegressionCV(random_state=0,multi_class='multinomial', solver='newton-cg')),\n",
    "    ])\n",
    "    \n",
    "\n",
    "    \n",
    "    pipeline3 = Pipeline([\n",
    "        ('vect',   TfidfVectorizer(\n",
    "                    analyzer='word',\n",
    "                    ngram_range=(1,3),\n",
    "                    norm='l1',\n",
    "                    min_df=2,\n",
    "                    max_df=1.0,\n",
    "                    lowercase =True,\n",
    "                    sublinear_tf=True)),\n",
    "        ('dense',  DenseTransformer()),\n",
    "        ('scaler', MaxAbsScaler()),\n",
    "        ('transf', PCA(0.99)),\n",
    "        ('clf', LogisticRegression(random_state=0,multi_class='multinomial', solver='newton-cg')),\n",
    "    ]);\n",
    "    \n",
    "    t0 = time()\n",
    "\n",
    "    for p in [pipeline1,pipeline2, pipeline3]:\n",
    "        p.fit(train_docs, train_labels)\n",
    "\n",
    "    xtrain_mix = np.hstack([p.predict_proba(train_docs) for p in [pipeline1, pipeline2, pipeline3]])\n",
    "    xtest_mix  = np.hstack([p.predict_proba(test_docs) for p in [pipeline1, pipeline2, pipeline3]])\n",
    "\n",
    "    clfFinal = Pipeline([\n",
    "        ('pca', PCA(0.9999)),\n",
    "        ('clf',LogisticRegression(random_state=0,multi_class='multinomial', solver='newton-cg'))\n",
    "    ]);\n",
    "    clfFinal.fit(xtrain_mix, train_labels);\n",
    "\n",
    "    train_pred=clfFinal.predict(xtrain_mix);\n",
    "    test_pred =clfFinal.predict(xtest_mix);\n",
    "        \n",
    "    print(\"done in %0.3fs \\n\" % (time() - t0))\n",
    "    # Writing output file\n",
    "    out_data=[]\n",
    "    for i,v in enumerate(test_pred):\n",
    "        out_data.append(\n",
    "                {'unknown-text': test_filename[i],\n",
    "                 'predicted-author': v\n",
    "                }\n",
    "                )\n",
    "    answerFile = pathjoin(outputDir,'answers-'+problem['problem']+'.json');\n",
    "    with open(answerFile, 'w') as f:\n",
    "        json.dump(out_data, f, indent=4)\n",
    "        #allProblems.extend(out_data)\n",
    "    \n",
    "    \n",
    "    #evaluation train\n",
    "    f1,precision,recall,accuracy=evaluate(\n",
    "                pathjoin(inputDir, problem['problem'], 'ground-truth.json'),\n",
    "                answerFile)\n",
    "    return {\n",
    "                'problem-name'  :       problem['problem'],\n",
    "                \"language\"      :       problem['language'],\n",
    "                'AuthorCount'   :       len(set(train_labels)),\n",
    "                \"train_doc_size\":       len(train_docs),\n",
    "                \"train_caract_per_doc\": sum([len(l) for l in train_docs])/len(train_docs),\n",
    "                \"test_doc_size\" :       len(test_docs),\n",
    "                \"test_caract_per_doc\":  sum([len(l) for l in test_docs])/len(test_docs),\n",
    "                \n",
    "                'macro-f1'       : round(f1,3),\n",
    "                'macro-precision': round(precision,3),\n",
    "                'macro-recall'   : round(recall,3),\n",
    "                'micro-accuracy' : round(accuracy,3),\n",
    "                \n",
    "        };"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Problem: problem00001,  language: en, \n",
      "done in 13.199s \n",
      "\n",
      "Problem: problem00002,  language: en, \n",
      "done in 3.076s \n",
      "\n",
      "Problem: problem00003,  language: fr, \n",
      "done in 11.767s \n",
      "\n",
      "Problem: problem00004,  language: fr, \n",
      "done in 3.144s \n",
      "\n",
      "Problem: problem00005,  language: it, \n",
      "done in 13.364s \n",
      "\n",
      "Problem: problem00006,  language: it, \n",
      "done in 3.913s \n",
      "\n",
      "Problem: problem00007,  language: pl, \n",
      "done in 16.309s \n",
      "\n",
      "Problem: problem00008,  language: pl, \n",
      "done in 3.583s \n",
      "\n",
      "Problem: problem00009,  language: sp, \n",
      "done in 14.450s \n",
      "\n",
      "Problem: problem00010,  language: sp, \n",
      "done in 4.675s \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AuthorCount</th>\n",
       "      <th>language</th>\n",
       "      <th>macro-f1</th>\n",
       "      <th>macro-precision</th>\n",
       "      <th>macro-recall</th>\n",
       "      <th>micro-accuracy</th>\n",
       "      <th>problem-name</th>\n",
       "      <th>test_caract_per_doc</th>\n",
       "      <th>test_doc_size</th>\n",
       "      <th>train_caract_per_doc</th>\n",
       "      <th>train_doc_size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20</td>\n",
       "      <td>en</td>\n",
       "      <td>0.511</td>\n",
       "      <td>0.471</td>\n",
       "      <td>0.704</td>\n",
       "      <td>0.610</td>\n",
       "      <td>problem00001</td>\n",
       "      <td>4370</td>\n",
       "      <td>105</td>\n",
       "      <td>4327</td>\n",
       "      <td>140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>en</td>\n",
       "      <td>0.575</td>\n",
       "      <td>0.617</td>\n",
       "      <td>0.650</td>\n",
       "      <td>0.524</td>\n",
       "      <td>problem00002</td>\n",
       "      <td>4296</td>\n",
       "      <td>21</td>\n",
       "      <td>4342</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20</td>\n",
       "      <td>fr</td>\n",
       "      <td>0.650</td>\n",
       "      <td>0.660</td>\n",
       "      <td>0.709</td>\n",
       "      <td>0.653</td>\n",
       "      <td>problem00003</td>\n",
       "      <td>4508</td>\n",
       "      <td>49</td>\n",
       "      <td>4492</td>\n",
       "      <td>140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>fr</td>\n",
       "      <td>0.867</td>\n",
       "      <td>0.864</td>\n",
       "      <td>0.920</td>\n",
       "      <td>0.810</td>\n",
       "      <td>problem00004</td>\n",
       "      <td>4532</td>\n",
       "      <td>21</td>\n",
       "      <td>4522</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20</td>\n",
       "      <td>it</td>\n",
       "      <td>0.559</td>\n",
       "      <td>0.529</td>\n",
       "      <td>0.687</td>\n",
       "      <td>0.750</td>\n",
       "      <td>problem00005</td>\n",
       "      <td>4787</td>\n",
       "      <td>80</td>\n",
       "      <td>4720</td>\n",
       "      <td>140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>it</td>\n",
       "      <td>0.621</td>\n",
       "      <td>0.608</td>\n",
       "      <td>0.704</td>\n",
       "      <td>0.826</td>\n",
       "      <td>problem00006</td>\n",
       "      <td>4765</td>\n",
       "      <td>46</td>\n",
       "      <td>4847</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>20</td>\n",
       "      <td>pl</td>\n",
       "      <td>0.520</td>\n",
       "      <td>0.548</td>\n",
       "      <td>0.590</td>\n",
       "      <td>0.534</td>\n",
       "      <td>problem00007</td>\n",
       "      <td>5200</td>\n",
       "      <td>103</td>\n",
       "      <td>5145</td>\n",
       "      <td>140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5</td>\n",
       "      <td>pl</td>\n",
       "      <td>0.822</td>\n",
       "      <td>0.867</td>\n",
       "      <td>0.878</td>\n",
       "      <td>0.867</td>\n",
       "      <td>problem00008</td>\n",
       "      <td>5214</td>\n",
       "      <td>15</td>\n",
       "      <td>5049</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>20</td>\n",
       "      <td>sp</td>\n",
       "      <td>0.800</td>\n",
       "      <td>0.807</td>\n",
       "      <td>0.874</td>\n",
       "      <td>0.812</td>\n",
       "      <td>problem00009</td>\n",
       "      <td>4788</td>\n",
       "      <td>117</td>\n",
       "      <td>4794</td>\n",
       "      <td>140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5</td>\n",
       "      <td>sp</td>\n",
       "      <td>0.797</td>\n",
       "      <td>0.800</td>\n",
       "      <td>0.809</td>\n",
       "      <td>0.859</td>\n",
       "      <td>problem00010</td>\n",
       "      <td>4827</td>\n",
       "      <td>64</td>\n",
       "      <td>4955</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   AuthorCount language  macro-f1  macro-precision  macro-recall  \\\n",
       "0           20       en     0.511            0.471         0.704   \n",
       "1            5       en     0.575            0.617         0.650   \n",
       "2           20       fr     0.650            0.660         0.709   \n",
       "3            5       fr     0.867            0.864         0.920   \n",
       "4           20       it     0.559            0.529         0.687   \n",
       "5            5       it     0.621            0.608         0.704   \n",
       "6           20       pl     0.520            0.548         0.590   \n",
       "7            5       pl     0.822            0.867         0.878   \n",
       "8           20       sp     0.800            0.807         0.874   \n",
       "9            5       sp     0.797            0.800         0.809   \n",
       "\n",
       "   micro-accuracy  problem-name  test_caract_per_doc  test_doc_size  \\\n",
       "0           0.610  problem00001                 4370            105   \n",
       "1           0.524  problem00002                 4296             21   \n",
       "2           0.653  problem00003                 4508             49   \n",
       "3           0.810  problem00004                 4532             21   \n",
       "4           0.750  problem00005                 4787             80   \n",
       "5           0.826  problem00006                 4765             46   \n",
       "6           0.534  problem00007                 5200            103   \n",
       "7           0.867  problem00008                 5214             15   \n",
       "8           0.812  problem00009                 4788            117   \n",
       "9           0.859  problem00010                 4827             64   \n",
       "\n",
       "   train_caract_per_doc  train_doc_size  \n",
       "0                  4327             140  \n",
       "1                  4342              35  \n",
       "2                  4492             140  \n",
       "3                  4522              35  \n",
       "4                  4720             140  \n",
       "5                  4847              35  \n",
       "6                  5145             140  \n",
       "7                  5049              35  \n",
       "8                  4794             140  \n",
       "9                  4955              35  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = [];\n",
    "for problem in problems:\n",
    "    result.append(runML(problem));\n",
    "pd.DataFrame(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame(result)[['problem-name',\n",
    "                     \"language\",\n",
    "                     'AuthorCount',\n",
    "                     \"train_doc_size\",\"train_caract_per_doc\",\n",
    "                     \"test_doc_size\", \"test_caract_per_doc\",\n",
    "                     'macro-f1','macro-precision','macro-recall' ,'micro-accuracy']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(result)[['macro-f1']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem = problems[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Problem: %s,  language: %s, \" %(problem['problem'],problem['language']))\n",
    "\n",
    "train_docs, train_labels, _   = zip(*problem['candidates'])\n",
    "problem['training_docs_size'] = len(train_docs);\n",
    "test_docs, _, test_filename   = zip(*problem['unknown'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code from baseline\n",
    "gt = {}\n",
    "with open(pathjoin(inputDir, problem['problem'], 'ground-truth.json'), 'r') as f:\n",
    "    for attrib in json.load(f)['ground_truth']:\n",
    "        gt[attrib['unknown-text']] = attrib['true-author']\n",
    "\n",
    "test_docs, _, test_filename = zip(*problem['unknown'])\n",
    "test_labels = [gt[v] for v in test_filename]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline1 = Pipeline([\n",
    "        ('vect',   TfidfVectorizer(\n",
    "                analyzer='char',\n",
    "                min_df=0.05,\n",
    "                max_df=1.0,\n",
    "                ngram_range=(2,5),\n",
    "                lowercase=False,\n",
    "                norm='l2',\n",
    "                sublinear_tf=True)),\n",
    "        ('dense',  DenseTransformer()),\n",
    "        ('scaler', MaxAbsScaler()),\n",
    "        ('transf', PCA(0.99)),\n",
    "        ('clf', LogisticRegression(random_state=0,multi_class='multinomial', solver='newton-cg')),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline2 = Pipeline([\n",
    "        ('obs',ObfuscationTransformer(re_from=r'\\w|\\d',re_to='x')),\n",
    "        ('vect',   TfidfVectorizer(\n",
    "                analyzer='char',\n",
    "                min_df=0.05,\n",
    "                max_df=1.0,\n",
    "                ngram_range=(2,5),\n",
    "                lowercase=False,\n",
    "                norm='l1',\n",
    "                sublinear_tf=True)),\n",
    "        ('dense',  DenseTransformer()),\n",
    "        ('scaler', MaxAbsScaler()),\n",
    "        ('transf', PCA(0.99)),\n",
    "        ('clf', LogisticRegression(random_state=0,multi_class='multinomial', solver='newton-cg')),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline3 = Pipeline([\n",
    "        ('vect',   TfidfVectorizer(\n",
    "                analyzer='word',\n",
    "                min_df=0.05,\n",
    "                max_df=1.0,\n",
    "                ngram_range=(1,3),\n",
    "                lowercase=False,\n",
    "                norm='l2',\n",
    "                sublinear_tf=True)),\n",
    "        ('dense',  DenseTransformer()),\n",
    "        ('scaler', MaxAbsScaler()),\n",
    "        ('transf', PCA(0.99)),\n",
    "        ('clf', LogisticRegression(random_state=0,multi_class='multinomial', solver='newton-cg')),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time()\n",
    "pipeline1.fit(train_docs, train_labels);\n",
    "pipeline2.fit(train_docs, train_labels);\n",
    "pipeline3.fit(train_docs, train_labels);\n",
    "print(\"done in %0.3fs \\n\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(pipeline1.named_steps['transf'].explained_variance_ratio_.cumsum(),label=\"1\");\n",
    "plt.plot(pipeline2.named_steps['transf'].explained_variance_ratio_.cumsum(),label=\"2\");\n",
    "plt.plot(pipeline3.named_steps['transf'].explained_variance_ratio_.cumsum(),label='3');\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain_mix = np.hstack([p.predict_proba(train_docs) for p in [pipeline1, pipeline2, pipeline3]])\n",
    "xtest_mix  = np.hstack([p.predict_proba(test_docs) for p in [pipeline1, pipeline2, pipeline3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#clfFinal = LogisticRegression(random_state=0,multi_class='multinomial', solver='newton-cg', C=0.1);\n",
    "clfFinal = Pipeline([\n",
    "    #('prepro', Normalizer(norm='l2')),\n",
    "    #('pca', PCA(0.99)),\n",
    "    ('clf',MLPClassifier(activation='identity',hidden_layer_sizes=(xtrain_mix.shape[1]), random_state=0))\n",
    "]);\n",
    "#clfFinal = MLPClassifier(activation='identity',hidden_layer_sizes=(xtrain_mix.shape[1]), random_state=0)\n",
    "\n",
    "clfFinal.fit(xtrain_mix, train_labels);\n",
    "\n",
    "train_pred=clfFinal.predict(xtrain_mix);\n",
    "test_pred =clfFinal.predict(xtest_mix);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(clfFinal.named_steps['clf'].loss_curve_);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.sum(np.array(train_pred) == np.array(train_labels))*1.0/len(train_labels))\n",
    "print(np.sum(np.array(test_pred) == np.array(test_labels))*1.0/len(test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred =clfFinal.predict(xtest_mix);\n",
    "f1,precision,recall,accuracy =  eval_measures(gt,{k: v for k,v in zip(test_filename, test_pred)  })\n",
    "results = [{ 'caso' : 'total',\n",
    "                'macro-f1'       : round(f1,3),\n",
    "                'macro-precision': round(precision,3),\n",
    "                'macro-recall'   : round(recall,3),\n",
    "                'micro-accuracy' : round(accuracy,3)\n",
    "             }]\n",
    "for clf in [pipeline1, pipeline2, pipeline3]:\n",
    "    p = clf.predict(test_docs)\n",
    "    f1,precision,recall,accuracy =  eval_measures(gt,{k: v for k,v in zip(test_filename, p)  })\n",
    "    results.append({\n",
    "                'caso' : clf.named_steps['vect'].analyzer,\n",
    "                'macro-f1'       : round(f1,3),\n",
    "                'macro-precision': round(precision,3),\n",
    "                'macro-recall'   : round(recall,3),\n",
    "                'micro-accuracy' : round(accuracy,3)\n",
    "             });\n",
    "\n",
    "pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame(\n",
    "    zip(\n",
    "        pipeline1.predict(test_docs),\n",
    "        pipeline2.predict(test_docs),\n",
    "        pipeline3.predict(test_docs),\n",
    "        test_pred,\n",
    "        test_labels\n",
    "    ),\n",
    "    columns=['p1','p2','p3','ens', 'truth'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.p1  = np.where(df.p1 == df.truth,1,0)\n",
    "df.p2  = np.where(df.p2 == df.truth,1,0)\n",
    "df.p3  = np.where(df.p3 == df.truth,1,0)\n",
    "df.ens = np.where(df.ens == df.truth,1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(by='truth').agg(np.sum).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
