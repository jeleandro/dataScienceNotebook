{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook para o PAN - Atribuição Autoral - 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "#python basic libs\n",
    "from __future__ import print_function\n",
    "import os;\n",
    "from os.path import join as pathjoin;\n",
    "import glob;\n",
    "import json;\n",
    "import codecs;\n",
    "from collections import defaultdict;\n",
    "import pprint;\n",
    "\n",
    "\n",
    "#data analysis libs\n",
    "import numpy as np;\n",
    "import pandas as pd;\n",
    "import matplotlib.pyplot as plt;\n",
    "import random;\n",
    "\n",
    "#machine learning libs\n",
    "#feature extraction\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "#preprocessing and transformation\n",
    "from sklearn.preprocessing import normalize, MaxAbsScaler, MinMaxScaler;\n",
    "from sklearn.preprocessing import LabelBinarizer;\n",
    "from sklearn.decomposition import PCA;\n",
    "from sklearn.metrics.pairwise import cosine_similarity;\n",
    "\n",
    "#classifiers\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.multiclass import OneVsOneClassifier, OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import RFE,SelectFpr,SelectPercentile, chi2;\n",
    "\n",
    "#\n",
    "from sklearn.ensemble import AdaBoostClassifier, BaggingClassifier, ExtraTreesClassifier, GradientBoostingClassifier;\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "#model valuation\n",
    "from sklearn.model_selection import train_test_split;\n",
    "from sklearn.metrics import roc_auc_score, f1_score, precision_score, recall_score, accuracy_score;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Darwin-17.4.0-x86_64-i386-64bit\n",
      "NumPy 1.14.0\n",
      "SciPy 1.0.0\n",
      "Scikit-Learn 0.19.1\n"
     ]
    }
   ],
   "source": [
    "import platform; print(platform.platform())\n",
    "print(\"NumPy\", np.__version__)\n",
    "import scipy; print(\"SciPy\", scipy.__version__)\n",
    "import sklearn; print(\"Scikit-Learn\", sklearn.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### paths configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "baseDir = '/Users/joseeleandrocustodio/Dropbox/mestrado/02 - Pesquisa/code';\n",
    "\n",
    "inputDir= pathjoin(baseDir,'pan18aa');\n",
    "outputDir= pathjoin(baseDir,'out',\"oficial\");\n",
    "if not os.path.exists(outputDir):\n",
    "    os.mkdir(outputDir);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pprinter = pprint.PrettyPrinter(indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def readCollectionsOfProblems(path):\n",
    "    # Reading information about the collection\n",
    "    infocollection = path+os.sep+'collection-info.json'\n",
    "    with open(infocollection, 'r') as f:\n",
    "        problems  = [\n",
    "            {\n",
    "                'problem': attrib['problem-name'],\n",
    "                'language': attrib['language'],\n",
    "                'encoding': attrib['encoding'],\n",
    "            }\n",
    "            for attrib in json.load(f)\n",
    "            \n",
    "        ]\n",
    "    return problems;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "problems = readCollectionsOfProblems(inputDir);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'encoding': u'UTF-8', 'language': u'en', 'problem': u'problem00001'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "problems[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>encoding</th>\n",
       "      <th>problem</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>language</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>en</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fr</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>it</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pl</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sp</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          encoding  problem\n",
       "language                   \n",
       "en               2        2\n",
       "fr               2        2\n",
       "it               2        2\n",
       "pl               2        2\n",
       "sp               2        2"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(problems).groupby(by='language').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def readProblem(path, problem):\n",
    "    # Reading information about the problem\n",
    "    infoproblem = path+os.sep+problem+os.sep+'problem-info.json'\n",
    "    candidates = []\n",
    "    with open(infoproblem, 'r') as f:\n",
    "        fj = json.load(f)\n",
    "        unk_folder = fj['unknown-folder']\n",
    "        for attrib in fj['candidate-authors']:\n",
    "            candidates.append(attrib['author-name'])\n",
    "    return unk_folder, candidates;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_files(path,label):\n",
    "    # Reads all text files located in the 'path' and assigns them to 'label' class\n",
    "    files = glob.glob(pathjoin(path,label,'*.txt'))\n",
    "    texts=[]\n",
    "    for i,v in enumerate(files):\n",
    "        f=codecs.open(v,'r',encoding='utf-8')\n",
    "        texts.append((f.read(),label, os.path.basename(v)))\n",
    "        f.close()\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for index,problem in enumerate(problems):\n",
    "    unk_folder, candidates_folder = readProblem(inputDir, problem['problem']); \n",
    "    problem['candidates_folder_count'] = len(candidates_folder);\n",
    "    problem['candidates'] = [];\n",
    "    for candidate in candidates_folder:\n",
    "        problem['candidates'].extend(read_files(pathjoin(inputDir, problem['problem']),candidate));\n",
    "    \n",
    "    problem['unknown'] = read_files(pathjoin(inputDir, problem['problem']),unk_folder);    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>candidates</th>\n",
       "      <th>candidates_folder_count</th>\n",
       "      <th>encoding</th>\n",
       "      <th>language</th>\n",
       "      <th>problem</th>\n",
       "      <th>unknown</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[(graceful ones.\\n\\n\"One more,\" Marvelous said...</td>\n",
       "      <td>20</td>\n",
       "      <td>UTF-8</td>\n",
       "      <td>en</td>\n",
       "      <td>problem00001</td>\n",
       "      <td>[(after all, his best friends. And what in the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[(a mission.\"\\n\\nJensen just raises an eyebrow...</td>\n",
       "      <td>5</td>\n",
       "      <td>UTF-8</td>\n",
       "      <td>en</td>\n",
       "      <td>problem00002</td>\n",
       "      <td>[(“Potter was attractive,” Draco thought, sigh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[(qui l'avait tué mais tout était de la faute ...</td>\n",
       "      <td>20</td>\n",
       "      <td>UTF-8</td>\n",
       "      <td>fr</td>\n",
       "      <td>problem00003</td>\n",
       "      <td>[(son réveil. Sa main pulse et Draco frotte l'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[(. Le canapé est vide et lorsqu'il passe deva...</td>\n",
       "      <td>5</td>\n",
       "      <td>UTF-8</td>\n",
       "      <td>fr</td>\n",
       "      <td>problem00004</td>\n",
       "      <td>[(abasourdie.\\n\\nTout d'abord, elle crut que s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[(Eppure lui la mappa l’aveva stampata, dannaz...</td>\n",
       "      <td>20</td>\n",
       "      <td>UTF-8</td>\n",
       "      <td>it</td>\n",
       "      <td>problem00005</td>\n",
       "      <td>[(– Oh. Cazzo.\\nSirius era così sconvolto che ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[(Yato ha trovato una lettera sul suo comodino...</td>\n",
       "      <td>5</td>\n",
       "      <td>UTF-8</td>\n",
       "      <td>it</td>\n",
       "      <td>problem00006</td>\n",
       "      <td>[(così la tua vista, Moony?\\n– Cercavo di esse...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[(zmienił zdanie. Niech się stworzonko pobawi....</td>\n",
       "      <td>20</td>\n",
       "      <td>UTF-8</td>\n",
       "      <td>pl</td>\n",
       "      <td>problem00007</td>\n",
       "      <td>[(dawniej pełna radości i ciepła, a teraz wiec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[(Słowem, które Sherlock najczęściej słyszał w...</td>\n",
       "      <td>5</td>\n",
       "      <td>UTF-8</td>\n",
       "      <td>pl</td>\n",
       "      <td>problem00008</td>\n",
       "      <td>[(, uderzającego o żebra niczym dzwon- niemal ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[(pero no lo ama como ama a Guignol –explicó e...</td>\n",
       "      <td>20</td>\n",
       "      <td>UTF-8</td>\n",
       "      <td>sp</td>\n",
       "      <td>problem00009</td>\n",
       "      <td>[(–La nariz puntiaguda del elfo casi rozaba el...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[(incapaz de señalar un momento exacto, un pun...</td>\n",
       "      <td>5</td>\n",
       "      <td>UTF-8</td>\n",
       "      <td>sp</td>\n",
       "      <td>problem00010</td>\n",
       "      <td>[(tan parecidas hizo que su trasero latiese de...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          candidates  candidates_folder_count  \\\n",
       "0  [(graceful ones.\\n\\n\"One more,\" Marvelous said...                       20   \n",
       "1  [(a mission.\"\\n\\nJensen just raises an eyebrow...                        5   \n",
       "2  [(qui l'avait tué mais tout était de la faute ...                       20   \n",
       "3  [(. Le canapé est vide et lorsqu'il passe deva...                        5   \n",
       "4  [(Eppure lui la mappa l’aveva stampata, dannaz...                       20   \n",
       "5  [(Yato ha trovato una lettera sul suo comodino...                        5   \n",
       "6  [(zmienił zdanie. Niech się stworzonko pobawi....                       20   \n",
       "7  [(Słowem, które Sherlock najczęściej słyszał w...                        5   \n",
       "8  [(pero no lo ama como ama a Guignol –explicó e...                       20   \n",
       "9  [(incapaz de señalar un momento exacto, un pun...                        5   \n",
       "\n",
       "  encoding language       problem  \\\n",
       "0    UTF-8       en  problem00001   \n",
       "1    UTF-8       en  problem00002   \n",
       "2    UTF-8       fr  problem00003   \n",
       "3    UTF-8       fr  problem00004   \n",
       "4    UTF-8       it  problem00005   \n",
       "5    UTF-8       it  problem00006   \n",
       "6    UTF-8       pl  problem00007   \n",
       "7    UTF-8       pl  problem00008   \n",
       "8    UTF-8       sp  problem00009   \n",
       "9    UTF-8       sp  problem00010   \n",
       "\n",
       "                                             unknown  \n",
       "0  [(after all, his best friends. And what in the...  \n",
       "1  [(“Potter was attractive,” Draco thought, sigh...  \n",
       "2  [(son réveil. Sa main pulse et Draco frotte l'...  \n",
       "3  [(abasourdie.\\n\\nTout d'abord, elle crut que s...  \n",
       "4  [(– Oh. Cazzo.\\nSirius era così sconvolto che ...  \n",
       "5  [(così la tua vista, Moony?\\n– Cercavo di esse...  \n",
       "6  [(dawniej pełna radości i ciepła, a teraz wiec...  \n",
       "7  [(, uderzającego o żebra niczym dzwon- niemal ...  \n",
       "8  [(–La nariz puntiaguda del elfo casi rozaba el...  \n",
       "9  [(tan parecidas hizo que su trasero latiese de...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(problems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#*******************************************************************************************************\n",
    "import warnings\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "def eval_measures(gt, pred):\n",
    "    \"\"\"Compute macro-averaged F1-scores, macro-averaged precision, \n",
    "    macro-averaged recall, and micro-averaged accuracy according the ad hoc\n",
    "    rules discussed at the top of this file.\n",
    "    Parameters\n",
    "    ----------\n",
    "    gt : dict\n",
    "        Ground truth, where keys indicate text file names\n",
    "        (e.g. `unknown00002.txt`), and values represent\n",
    "        author labels (e.g. `candidate00003`)\n",
    "    pred : dict\n",
    "        Predicted attribution, where keys indicate text file names\n",
    "        (e.g. `unknown00002.txt`), and values represent\n",
    "        author labels (e.g. `candidate00003`)\n",
    "    Returns\n",
    "    -------\n",
    "    f1 : float\n",
    "        Macro-averaged F1-score\n",
    "    precision : float\n",
    "        Macro-averaged precision\n",
    "    recall : float\n",
    "        Macro-averaged recall\n",
    "    accuracy : float\n",
    "        Micro-averaged F1-score\n",
    "    \"\"\"\n",
    "\n",
    "    actual_authors = list(gt.values())\n",
    "    encoder = LabelEncoder().fit(['<UNK>'] + actual_authors)\n",
    "\n",
    "    text_ids, gold_authors, silver_authors = [], [], []\n",
    "    for text_id in sorted(gt):\n",
    "        text_ids.append(text_id)\n",
    "        gold_authors.append(gt[text_id])\n",
    "        try:\n",
    "            silver_authors.append(pred[text_id])\n",
    "        except KeyError:\n",
    "            # missing attributions get <UNK>:\n",
    "            silver_authors.append('<UNK>')\n",
    "\n",
    "    assert len(text_ids) == len(gold_authors)\n",
    "    assert len(text_ids) == len(silver_authors)\n",
    "\n",
    "    # replace non-existent silver authors with '<UNK>':\n",
    "    silver_authors = [a if a in encoder.classes_ else '<UNK>' \n",
    "                      for a in silver_authors]\n",
    "\n",
    "    gold_author_ints   = encoder.transform(gold_authors)\n",
    "    silver_author_ints = encoder.transform(silver_authors)\n",
    "\n",
    "    # get F1 for individual classes (and suppress warnings):\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter('ignore')\n",
    "        f1 = f1_score(gold_author_ints,\n",
    "                  silver_author_ints,\n",
    "                  labels=list(set(gold_author_ints)),\n",
    "                  average='macro')\n",
    "        precision = precision_score(gold_author_ints,\n",
    "                  silver_author_ints,\n",
    "                  labels=list(set(gold_author_ints)),\n",
    "                  average='macro')\n",
    "        recall = recall_score(gold_author_ints,\n",
    "                  silver_author_ints,\n",
    "                  labels=list(set(gold_author_ints)),\n",
    "                  average='macro')\n",
    "        accuracy = accuracy_score(gold_author_ints,\n",
    "                  silver_author_ints)\n",
    "\n",
    "    return f1,precision,recall,accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(ground_truth_file,predictions_file):\n",
    "    # Calculates evaluation measures for a single attribution problem\n",
    "    gt = {}\n",
    "    with open(ground_truth_file, 'r') as f:\n",
    "        for attrib in json.load(f)['ground_truth']:\n",
    "            gt[attrib['unknown-text']] = attrib['true-author']\n",
    "\n",
    "    pred = {}\n",
    "    with open(predictions_file, 'r') as f:\n",
    "        for attrib in json.load(f):\n",
    "            if attrib['unknown-text'] not in pred:\n",
    "                pred[attrib['unknown-text']] = attrib['predicted-author']\n",
    "    f1,precision,recall,accuracy =  eval_measures(gt,pred)\n",
    "    return f1, precision, recall, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree     import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC;\n",
    "def runML(problem):\n",
    "    print (\"Problem: %s,  language: %s, \" %(problem['problem'],problem['language']))\n",
    "    \n",
    "    #creating author profile\n",
    "#     profile = defaultdict(unicode);\n",
    "#     for text, author, _ in problem['candidates']:\n",
    "#         profile[author]+=text;\n",
    "    \n",
    "#     train_docs   = list(profile.values())\n",
    "#     train_labels = list(profile.keys())\n",
    "    \n",
    "    train_docs, train_labels, _  = zip(*problem['candidates'])\n",
    "    \n",
    "    problem['training_docs_size'] = len(train_docs);\n",
    "    \n",
    "    test_docs, _, test_filename = zip(*problem['unknown'])\n",
    "    \n",
    "    \n",
    "    #feature extraction\n",
    "    vectorizer = TfidfVectorizer(analyzer=\"char\",min_df=0.3,max_df=1.0, lowercase=False, ngram_range=(3,5));\n",
    "    train_mx   = vectorizer.fit_transform(train_docs);\n",
    "    test_max   = vectorizer.transform(test_docs);\n",
    "    \n",
    "    scaler = MaxAbsScaler();\n",
    "    train_mx = scaler.fit_transform(train_mx);\n",
    "    test_max = scaler.transform(test_max);\n",
    "    \n",
    "    selector = RFE(estimator=SVC(kernel=\"linear\", C=1), step=0.05, verbose=False)\n",
    "    \n",
    "    train_mx = selector.fit_transform(train_mx,train_labels);\n",
    "    test_max = selector.transform(test_max);\n",
    "    \n",
    "    #machine learning\n",
    "    clf = LogisticRegression(random_state=0,multi_class='multinomial', solver='newton-cg', C=0.5);\n",
    "    #clf = AdaBoostClassifier(clf, n_estimators=50, learning_rate=0.01)\n",
    "    #clf =  BaggingClassifier(clf, n_estimators=10, random_state=42)\n",
    "    #clf = VotingClassifier([clf1,clf2] , voting='hard')\n",
    "    clf.fit(train_mx,train_labels);\n",
    "    \n",
    "    train_pred=clf.predict(train_mx);\n",
    "    test_pred=clf.predict(test_max);\n",
    "    \n",
    "    # Writing output file\n",
    "    out_data=[]\n",
    "    for i,v in enumerate(test_pred):\n",
    "        out_data.append(\n",
    "                {'unknown-text': test_filename[i],\n",
    "                 'predicted-author': v\n",
    "                }\n",
    "                )\n",
    "    answerFile = pathjoin(outputDir,'answers-'+problem['problem']+'.json');\n",
    "    with open(answerFile, 'w') as f:\n",
    "        json.dump(out_data, f, indent=4)\n",
    "        #allProblems.extend(out_data)\n",
    "    \n",
    "    \n",
    "    #evaluation train\n",
    "    f1,precision,recall,accuracy=evaluate(\n",
    "                pathjoin(inputDir, problem['problem'], 'ground-truth.json'),\n",
    "                answerFile)\n",
    "#     f1,precision,recall,accuracy=eval_measures(\n",
    "#             {str(i):label for i,label in enumerate(train_labels)},\n",
    "#             {str(i):label for i,label in enumerate(train_pred)}\n",
    "#     )\n",
    "    return {\n",
    "                'problem-name'   : problem['problem'],\n",
    "                \"train_doc_size\":len(train_docs),\n",
    "                \"language\":problem['language'],\n",
    "                'macro-f1'       : round(f1,3),\n",
    "                'macro-precision': round(precision,3),\n",
    "                'macro-recall'   : round(recall,3),\n",
    "                'micro-accuracy' : round(accuracy,3),\n",
    "                'AuthorCount':len(set(train_labels))\n",
    "        };\n",
    "    \n",
    "    #evaluate_all(inputDir,outputDir, outputDir, instanceName)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Problem: problem00001,  language: en, \n",
      "Problem: problem00002,  language: en, \n",
      "Problem: problem00003,  language: fr, \n",
      "Problem: problem00004,  language: fr, \n",
      "Problem: problem00005,  language: it, \n",
      "Problem: problem00006,  language: it, \n",
      "Problem: problem00007,  language: pl, \n"
     ]
    }
   ],
   "source": [
    "result = [];\n",
    "for problem in problems:\n",
    "    result.append(runML(problem));\n",
    "pd.DataFrame(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(result)[['macro-f1']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(result)\\\n",
    "    .sort_values(by=['language','problem-name'])[['language','problem-name','macro-f1']]\\\n",
    "    .plot(kind='bar', x=['language','problem-name'], legend=True, figsize=(20,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/><br/><br/><br/>\n",
    "\n",
    "#  Abordagem desafiante 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NgramSplitter(object):\n",
    "    def __init__(self, text, ngram=(3,3), vocabulary=None):\n",
    "        self.text = text\n",
    "        self.ngram_min = ngram[0]\n",
    "        self.ngram_max = ngram[1];\n",
    "        self.vocabulary = vocabulary;\n",
    "    \n",
    "    def text2ngrams(self,text):\n",
    "        vect = [\n",
    "            text[t:t+j]\n",
    "                for t in xrange(len(text)-self.ngram_max+1)\n",
    "                for j in xrange(self.ngram_min, self.ngram_max+1)\n",
    "        ]\n",
    "        \n",
    "        if self.vocabulary is not None:\n",
    "            return [word for word in vect if word in self.vocabulary];\n",
    "        else:\n",
    "            return [word for word in vect if word]\n",
    " \n",
    "    def __iter__(self):\n",
    "        if isinstance(self.text,list):\n",
    "            for s in self.text:\n",
    "                yield self.text2ngrams(s);\n",
    "        elif isinstance(self.text,str) or isinstance(self.text,unicode):\n",
    "            yield self.text2ngrams(self.text);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def simpleCosine(a, b):\n",
    "    '''\n",
    "    calculates cosine between array a and b.\n",
    "    This function is used because sklearn similiraty function compares all elements vs all elements\n",
    "    what will not be used. So this function becames handy.\n",
    "    '''\n",
    "    a = a / np.sqrt(np.sum(a **2));\n",
    "    b = b / np.sqrt(np.sum(b **2));\n",
    "    cos = np.sum(np.array(a) * np.array(b));\n",
    "    return cos;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram = (3,5);\n",
    "embeddingSize  = 50\n",
    "problem = problems[8];\n",
    "print (\"Problem: %s,  language: %s, \" %(problem['problem'],problem['language']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the authors profile is used to generate author word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#creating author profile\n",
    "profile = defaultdict(unicode);\n",
    "for text, author, _ in problem['candidates']:\n",
    "    profile[author]+=text;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "profile_docs   = list(profile.values())\n",
    "profile_labels = list(profile.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_docs, train_labels,_ = zip(*problem['candidates']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#code from baseline\n",
    "gt = {}\n",
    "with open(pathjoin(inputDir, problem['problem'], 'ground-truth.json'), 'r') as f:\n",
    "    for attrib in json.load(f)['ground_truth']:\n",
    "        gt[attrib['unknown-text']] = attrib['true-author']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_docs, _, test_filename = zip(*problem['unknown'])\n",
    "test_labels = [gt[v] for v in test_filename]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using count vectorizer to create a fixed vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(analyzer=\"char\", ngram_range=ngram, min_df=0.4, max_df=1.0, lowercase=False)\n",
    "counts = vectorizer.fit_transform(profile_docs);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = dict(zip(\n",
    "    vectorizer.vocabulary_,\n",
    "    np.array(counts.sum(axis=0)).flatten()\n",
    ")\n",
    ")\n",
    "len(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#transform a document into a word vector using CBOW\n",
    "def doc2vectors(doc):\n",
    "    model = Word2Vec(\n",
    "        NgramSplitter(doc,ngram=ngram, vocabulary=vectorizer.vocabulary_),\n",
    "        sg=0,\n",
    "        min_count=2,\n",
    "        size=embeddingSize,\n",
    "        seed=0\n",
    "    );\n",
    "    return model.wv;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### profile vector represent each author in the embedding space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profileVectors = {author: doc2vectors(profile[author]) for author in profile};"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### testing author vector against internal documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "documentVectors =[doc2vectors(doc) for doc in train_docs];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare(profileVectors, doc):\n",
    "    vocabDoc = set(doc.vocab.keys());\n",
    "    \n",
    "    metrics = [];\n",
    "    \n",
    "    for author in profileVectors:\n",
    "        authorVocab = set(profileVectors[author].vocab.keys());\n",
    "        intersect = vocabDoc & authorVocab;\n",
    "        union = len(vocabDoc | authorVocab);\n",
    "        jaccard = 1.0*len(intersect) / union;\n",
    "        \n",
    "        cosine = [\n",
    "            simpleCosine(doc[word],profileVectors[author][word])\n",
    "            for word in intersect\n",
    "        ];\n",
    "            \n",
    "        metrics.append({\n",
    "            'candidate':author,\n",
    "            'jaccard'  :jaccard,\n",
    "            'lenIntersect':len(intersect),\n",
    "            'lenUnion'    :union,\n",
    "            'lenMax': max(len(authorVocab), len(vocabDoc)),\n",
    "            'distanceVector'    :np.sum(cosine)            \n",
    "        })\n",
    "    #softmax norm\n",
    "    cosine = np.array([c['distanceVector'] for c in metrics ]);\n",
    "    #minMax\n",
    "    cosine = (cosine - np.min(cosine))/(np.max(cosine) - np.min(cosine));\n",
    "    cosine = np.exp(cosine)/np.sum(np.exp(cosine));\n",
    "    \n",
    "    #appending normalized sum of distance\n",
    "    for i,c in enumerate(metrics):\n",
    "        c.update({'distanceVectorNorm': cosine[i]})\n",
    "    \n",
    "    return metrics; "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### testing one document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_doc_index = 4;\n",
    "print (\"Expected answer %s\" % train_labels[train_doc_index])\n",
    "metrics = compare(profileVectors, documentVectors[train_doc_index]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(metrics).sort_values(by='distanceVectorNorm', ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(profileVectors, documentVectors):\n",
    "    predictions = [];\n",
    "    \n",
    "    for document in documentVectors:\n",
    "        metrics = compare(profileVectors, document);\n",
    "        metrics.sort(key=lambda k: (k['distanceVectorNorm'])/1.0, reverse=True  )\n",
    "        predictions.append(metrics[0]['candidate']);\n",
    "    return predictions;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pred =predict(profileVectors, documentVectors)\n",
    "zip(train_labels, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documentVectorsTest =[doc2vectors(doc) for doc in test_docs];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "analyzing one incorrect answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test_index = 74\n",
    "print (\"Expected answer (candidate with highest score) %s\"% test_labels[test_index])\n",
    "df= pd.DataFrame(compare(profileVectors, documentVectorsTest[test_index]))\\\n",
    "    .sort_values(by='distanceVectorNorm',ascending=False)\\\n",
    "    .head(35) \\\n",
    "    .reset_index();\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob = []\n",
    "documentVectorsTestMetrics = [];\n",
    "for t,truth,instance in zip(documentVectorsTest,test_labels, test_filename):\n",
    "    for i,j in enumerate(sorted(compare(profileVectors, t), key=lambda x: x['distanceVector'], reverse=True)):\n",
    "        j.update({\"truth\":truth, 'instance':instance, 'distanceToTruth':i});\n",
    "        \n",
    "        jcopy = j.copy();\n",
    "        jcopy.update({'correct':truth == j['candidate']})\n",
    "        documentVectorsTestMetrics.append(jcopy)\n",
    "        \n",
    "        if truth != j['candidate']:\n",
    "            continue;\n",
    "        \n",
    "        prob.append(j);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(prob)[\n",
    "    ['instance','distanceToTruth','truth','distanceVector','distanceVectorNorm','jaccard','lenIntersect','lenMax','lenUnion']\n",
    "].sort_values(by=['instance','truth','distanceVector'], ascending=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(prob).sort_values(by=['distanceVector','jaccard'], ascending=False)\n",
    "df.plot.scatter(x='distanceToTruth', y='distanceVector')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(prob)\n",
    "#df.distanceToTruth = np.log(df.distanceToTruth+1)\n",
    "#df.distanceToTruth = df.distanceToTruth/df.distanceToTruth.max()\n",
    "df.plot.scatter(\n",
    "    x='distanceVectorNorm',\n",
    "    y='jaccard',c='distanceToTruth',\n",
    "    #figsize=(20,5),\n",
    "    #xlim=(0,0.25), ylim=(0,0.25),\n",
    "    cmap='viridis_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(prob).sort_values(by=['instance','truth','distanceVector'], ascending=False)\n",
    "df = df.groupby(by='distanceToTruth').size().reset_index();\n",
    "df.columns=['distanceToTruth', 'counter']\n",
    "df['cumm'] = df['counter'].cumsum();\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot.line(x='distanceToTruth',marker='.');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predTest =predict(profileVectors, documentVectorsTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(zip(test_labels, predTest), columns=['Truth', 'Pred'])\n",
    "df['comp'] = df.Truth == df.Pred\n",
    "df.groupby(by='comp').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1,precision,recall,accuracy =  eval_measures(gt,{k: v for k,v in zip(test_filename, predTest)  })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame([{\n",
    "                'macro-f1'       : round(f1,3),\n",
    "                'macro-precision': round(precision,3),\n",
    "                'macro-recall'   : round(recall,3),\n",
    "                'micro-accuracy' : round(accuracy,3)\n",
    "             }])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(documentVectorsTestMetrics)\n",
    "len(df[df.correct & (df.distanceToTruth==0)].truth.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abordagem 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embeddingSize  = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AuthorNgram(object):\n",
    "    def __init__(self, text, label, vocabulary=None):\n",
    "        self.text = text;\n",
    "        self.label = label;\n",
    "        self.ngram_min = 3;\n",
    "        self.ngram_max = 5;\n",
    "        self.vocabulary = vocabulary;\n",
    "        self.window = 3;\n",
    "    \n",
    "    def text2ngrams(self,text):\n",
    "        vect = [\n",
    "            text[t:t+j]\n",
    "                for t in xrange(len(text)-self.ngram_max+1)\n",
    "                for j in xrange(self.ngram_min, self.ngram_max+1)\n",
    "        ]\n",
    "        \n",
    "        if self.vocabulary is not None:\n",
    "            return [word for word in vect if word in self.vocabulary];\n",
    "        else:\n",
    "            return [word for word in vect if word]\n",
    "        \n",
    "    def textPlusAuthor(self,text,label):\n",
    "        vect = self.text2ngrams(text);\n",
    "        vect = [\n",
    "                vect[t:t+self.window] +[label]+ vect[t+self.window:t+self.window*2] \n",
    "                for t in xrange(len(vect)-2*self.window+1)\n",
    "        ];\n",
    "        return vect;\n",
    "    \n",
    "    def __len__(self):\n",
    "        l = 0;\n",
    "        for i in range (self.ngram_min, self.ngram_max +1):\n",
    "            l += len(self.text) - i\n",
    "        return l\n",
    " \n",
    "    def __iter__(self):\n",
    "        if isinstance(self.text,list):\n",
    "            for s in self.text:\n",
    "                for vect in self.textPlusAuthor(s, self.label):\n",
    "                    yield vect;\n",
    "        elif isinstance(self.text,str) or isinstance(self.text,unicode):\n",
    "                for vect in self.textPlusAuthor(self.text, self.label):\n",
    "                    yield vect;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in AuthorNgram(['a casa caiu'],\"_text_\"):\n",
    "    print (s);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i,s in enumerate(AuthorNgram(train_docs[0],\"_text_\")):\n",
    "    print (s);\n",
    "    if i == 20:\n",
    "        break;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getEmbedding(doc, label):\n",
    "    model = Word2Vec(\n",
    "        sg=0,\n",
    "        min_count=2,\n",
    "        size=embeddingSize,\n",
    "        seed=0\n",
    "    );\n",
    "    corpus_count = len(AuthorNgram(doc,label, vocabulary=vocabulary.keys()))\n",
    "    model.build_vocab_from_freq(vocabulary)\n",
    "    model.train(\n",
    "        AuthorNgram(doc,label, vocabulary=vocabulary.keys()),\n",
    "        total_examples=corpus_count,\n",
    "        epochs=10\n",
    "    )\n",
    "    \n",
    "    \n",
    "    return model.wv[label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authorEmbedding = {\n",
    "    author:getEmbedding(profile[author], author)\n",
    "    for author in profile\n",
    "    \n",
    "};"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authorVectors= np.vstack(authorEmbedding.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainVectors = [getEmbedding(doc, \"_train\"+str(i)+\"_\") for i,doc in enumerate(train_docs)]\n",
    "testVectors  = [getEmbedding(doc, \"_test\"+str(i)+\"_\") for i,doc in enumerate(test_docs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainVectors =np.vstack(trainVectors)\n",
    "testVectors  =np.vstack(testVectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainPred = cosine_similarity(normalize(trainVectors,'l2'), normalize(authorVectors,'l2'))\n",
    "testPred  = cosine_similarity(normalize(testVectors,'l2'), normalize(authorVectors,'l2'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosineToLabels(matrix, labels):\n",
    "    #min_ = matrix.min(axis=1, keepdims=True)\n",
    "    #max_ = matrix.max(axis=1, keepdims=True)\n",
    "    #matrix = (matrix - min_)/(max_ - min_);\n",
    "    length = len(labels);\n",
    "    labels = np.array(labels);\n",
    "    temp = (length-1-np.argsort(matrix, axis=1))==0\n",
    "    \n",
    "    return [labels[i][0] for i in temp ];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainPredLabels = cosineToLabels(trainPred,authorEmbedding.keys())\n",
    "testPredLabels = cosineToLabels(testPred,authorEmbedding.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(np.array(train_labels) == np.array(trainPredLabels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainPred.min(axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip(train_labels, trainPredLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abordagem 2 - por LSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD;\n",
    "from scipy.sparse.linalg import svds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LSAWordVec:\n",
    "    def __init__(self):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lsaVectorizer = TfidfVectorizer(analyzer=\"char\", ngram_range=ngram, min_df=0.4, max_df=1.0, lowercase=False)\n",
    "profileLSAVectors = lsaVectorizer.fit_transform([profile[author] for author in profile]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "profileLabels =[author for author in profile]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(profileLSAVectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "svd = TruncatedSVD(n_components=20, random_state=42, n_iter=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profileLSAVectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svdProfile = svd.fit_transform(profileLSAVectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(svdProfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documentVectorsLSA = svd.transform(lsaVectorizer.transform(train_docs));\n",
    "documentVectorsTest = svd.transform(lsaVectorizer.transform(test_docs));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compareLSA(svdProfile, doc):\n",
    "    metrics = [];\n",
    "    \n",
    "    for author in profileVectors:\n",
    "        authorVocab = (profileVectors[author] >0).sum();\n",
    "        intersect = (profileVectors[author] >0) & (doc > 0)>0;\n",
    "        union = len(vocabDoc | authorVocab);\n",
    "        jaccard = 1.0*len(intersect) / union;\n",
    "        \n",
    "        cosine = [\n",
    "            simpleCosine(doc[word],profileVectors[author][word])\n",
    "            for word in intersect\n",
    "        ];\n",
    "            \n",
    "        metrics.append({\n",
    "            'candidate':author,\n",
    "            'jaccard'  :jaccard,\n",
    "            'lenIntersect':len(intersect),\n",
    "            'lenUnion'    :union,\n",
    "            'lenMax': max(len(authorVocab), len(vocabDoc)),\n",
    "            'distanceVector'    :np.sum(cosine)            \n",
    "        })\n",
    "    #softmax norm\n",
    "    cosine = np.array([c['distanceVector'] for c in metrics ]);\n",
    "    #minMax\n",
    "    cosine = (cosine - np.min(cosine))/(np.max(cosine) - np.min(cosine));\n",
    "    cosine = np.exp(cosine)/np.sum(np.exp(cosine));\n",
    "    \n",
    "    #appending normalized sum of distance\n",
    "    for i,c in enumerate(metrics):\n",
    "        c.update({'distanceVectorNorm': cosine[i]})\n",
    "    \n",
    "    return metrics; "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predictLSA(svdProfile, documentVectors):\n",
    "    cosines = cosine_similarity(documentVectors,svdProfile)\n",
    "    sortedIndex = np.argsort(svdTrainPred, axis=1);\n",
    "    \n",
    "    labels = np.array(profileLabels);\n",
    "    \n",
    "    \n",
    "    \n",
    "    predictions = [];\n",
    "    \n",
    "    for i,cosine_ in enumerate(cosines):\n",
    "        \n",
    "        l       = labels[sortedIndex];\n",
    "        cosine_ = cosine_[i][sortedIndex[i]];\n",
    "        \n",
    "        metrics= [{\n",
    "            'candidate':a,\n",
    "            'distanceToTruth':len(labels) - ii,\n",
    "            'distanceVector' :c            \n",
    "        }  for a,c, ii in zip(l, cosine_, sortedIndex[i])]\n",
    "        \n",
    "        metrics.sort(key=lambda k: (k['distanceToTruth'])/1.0, reverse=True  )\n",
    "        predictions.append(metrics[0]['candidate']);\n",
    "    return predictions;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svdTrainPred = cosine_similarity(documentVectorsLSA,svdProfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictLSA(svdProfile,documentVectorsLSA )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.reshape(np.round(np.random.rand(30)*100), (10,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predTestLSA = [np.array(profileLabels)[v][0] for v in  (np.argsort(svdTrainPred, axis=1)==19)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1,precision,recall,accuracy =  eval_measures(gt,{k: v for k,v in zip(test_filename, predTestLSA)  })\n",
    "pd.DataFrame([{\n",
    "                'macro-f1'       : round(f1,3),\n",
    "                'macro-precision': round(precision,3),\n",
    "                'macro-recall'   : round(recall,3),\n",
    "                'micro-accuracy' : round(accuracy,3)\n",
    "             }])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svdTrainPred[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argsort(svdTrainPred, axis=1)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t = [list('casa sapato'),list('terra casa, sapato')]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
