{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "#python basic libs\n",
    "from __future__ import print_function\n",
    "\n",
    "from tempfile import mkdtemp\n",
    "from shutil import rmtree\n",
    "import os;\n",
    "from os.path import join as pathjoin;\n",
    "\n",
    "import re;\n",
    "import glob;\n",
    "import json;\n",
    "import codecs;\n",
    "import itertools;\n",
    "from collections import defaultdict, Counter;\n",
    "import pprint;\n",
    "\n",
    "from pprint import pprint\n",
    "from time import time\n",
    "import logging\n",
    "\n",
    "\n",
    "#data analysis libs\n",
    "import numpy as np;\n",
    "import pandas as pd;\n",
    "import matplotlib.pyplot as plt;\n",
    "import random;\n",
    "np.set_printoptions(precision=4)\n",
    "\n",
    "#machine learning libs\n",
    "#feature extraction\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "\n",
    "#preprocessing and transformation\n",
    "from sklearn.preprocessing import normalize, MaxAbsScaler, MinMaxScaler, StandardScaler;\n",
    "from sklearn.preprocessing import LabelBinarizer;\n",
    "from sklearn.decomposition import PCA, KernelPCA, FastICA;\n",
    "from sklearn.metrics.pairwise import cosine_similarity;\n",
    "from scipy.spatial.distance import cosine;\n",
    "\n",
    "\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "\n",
    "#classifiers\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "#\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "\n",
    "#model valuation\n",
    "from sklearn.metrics import roc_auc_score, f1_score, precision_score, recall_score, accuracy_score;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 20 newsgroups dataset for categories:\n",
      "['alt.atheism', 'talk.religion.misc', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.space']\n",
      "data loaded\n",
      "Extracting features from the training data using a sparse vectorizer\n",
      "done in 0.607905s at 6.546MB/s\n",
      "n_samples: 2647, n_features: 28711\n",
      "\n",
      "Extracting features from the test data using the same vectorizer\n",
      "done in 0.296904s at 9.658MB/s\n",
      "n_samples: 1760, n_features: 28711\n",
      "\n",
      "Extracting 1000 best features by a chi-squared test\n",
      "done in 0.017791s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import logging\n",
    "import numpy as np\n",
    "import sys\n",
    "from time import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "# #############################################################################\n",
    "# Load some categories from the training set\n",
    "categories = [\n",
    "    'alt.atheism',\n",
    "    'talk.religion.misc',\n",
    " 'rec.sport.baseball',\n",
    " 'rec.sport.hockey',\n",
    "    'sci.space',\n",
    "]\n",
    "remove = ('headers', 'footers', 'quotes')\n",
    "\n",
    "\n",
    "print(\"Loading 20 newsgroups dataset for categories:\")\n",
    "print(categories if categories else \"all\")\n",
    "\n",
    "data_train = fetch_20newsgroups(subset='train', categories=categories,shuffle=True, random_state=42,remove=remove)\n",
    "data_test = fetch_20newsgroups(subset='test', categories=categories,shuffle=True, random_state=42,remove=remove)\n",
    "print('data loaded')\n",
    "\n",
    "# order of labels in `target_names` can be different from `categories`\n",
    "target_names = data_train.target_names\n",
    "\n",
    "# split a training set and a test set\n",
    "y_train, y_test = data_train.target, data_test.target\n",
    "\n",
    "print(\"Extracting features from the training data using a sparse vectorizer\")\n",
    "t0 = time()\n",
    "vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5,stop_words='english')\n",
    "X_train = vectorizer.fit_transform(data_train.data)\n",
    "\n",
    "duration = time() - t0\n",
    "print(\"done in %fs at %0.3fMB/s\" % (duration, data_train_size_mb / duration))\n",
    "print(\"n_samples: %d, n_features: %d\" % X_train.shape)\n",
    "print()\n",
    "\n",
    "print(\"Extracting features from the test data using the same vectorizer\")\n",
    "t0 = time()\n",
    "X_test = vectorizer.transform(data_test.data)\n",
    "duration = time() - t0\n",
    "print(\"done in %fs at %0.3fMB/s\" % (duration, data_test_size_mb / duration))\n",
    "print(\"n_samples: %d, n_features: %d\" % X_test.shape)\n",
    "print()\n",
    "\n",
    "# mapping from integer feature name to original token string\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "\n",
    "print(\"Extracting 1000 best features by a chi-squared test\" )\n",
    "t0 = time()\n",
    "ch2 = SelectKBest(chi2, k=1000)\n",
    "X_train = ch2.fit_transform(X_train, y_train)\n",
    "X_test = ch2.transform(X_test)\n",
    "\n",
    "feature_names = [feature_names[i] for i in ch2.get_support(indices=True)]\n",
    "print(\"done in %fs\" % (time() - t0))\n",
    "print()\n",
    "\n",
    "\n",
    "feature_names = np.asarray(feature_names)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/joseeleandrocustodio/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u\"Hi,\\n\\nI've noticed that if you only save a model (with all your mapping planes\\npositioned carefully) to a .3DS file that when you reload it after restarting\\n3DS, they are given a default position and orientation.  But if you save\\nto a .PRJ file their positions/orientation are preserved.  Does anyone\\nknow why this information is not stored in the .3DS file?  Nothing is\\nexplicitly said in the manual about saving texture rules in the .PRJ file. \\nI'd like to be able to read the texture rule information, does anyone have \\nthe format for the .PRJ file?\\n\\nIs the .CEL file format available from somewhere?\\n\\nRych\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitNgrams(doc, ngrams=3):\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    from nltk.stem.porter import PorterStemmer\n",
    "    stemmer = PorterStemmer()\n",
    "    doc = re.sub(r'\\_|\\/|=',' ',doc)\n",
    "    doc = re.sub(r'\\d+','*',doc)\n",
    "    #from nltk.corpus import stopwords\n",
    "    #stopwords.words('english')\n",
    "    #return [doc[i:(i+ngrams)] for i in range(len(doc)-ngrams)]\n",
    "    return [t for t in word_tokenize(doc.lower()) if len(t)>3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import TaggedDocument, Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-07-15 11:47:34,020 WARNING consider setting layer size to a multiple of 4 for greater performance\n",
      "2018-07-15 11:47:34,024 INFO collecting all words and their counts\n",
      "2018-07-15 11:47:34,025 INFO PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags\n",
      "2018-07-15 11:47:34,151 INFO collected 27488 word types and 315 unique tags from a corpus of 2647 examples and 268361 words\n",
      "2018-07-15 11:47:34,153 INFO Loading a fresh vocabulary\n",
      "2018-07-15 11:47:34,195 INFO min_count=10 retains 4038 unique words (14% of original 27488, drops 23450)\n",
      "2018-07-15 11:47:34,196 INFO min_count=10 leaves 216589 word corpus (80% of original 268361, drops 51772)\n",
      "2018-07-15 11:47:34,211 INFO deleting the raw counts dictionary of 27488 items\n",
      "2018-07-15 11:47:34,213 INFO sample=0.001 downsamples 43 most-common words\n",
      "2018-07-15 11:47:34,215 INFO downsampling leaves estimated 195558 word corpus (90.3% of prior 216589)\n",
      "2018-07-15 11:47:34,236 INFO estimated required memory for 4038 words and 150 dimensions: 7115600 bytes\n",
      "2018-07-15 11:47:34,237 INFO resetting layer weights\n",
      "2018-07-15 11:47:34,309 INFO training model with -1 workers on 4038 vocabulary and 150 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2018-07-15 11:47:34,314 INFO EPOCH - 1 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2018-07-15 11:47:34,320 WARNING EPOCH - 1 : supplied example count (0) did not equal expected count (2647)\n",
      "2018-07-15 11:47:34,329 INFO EPOCH - 2 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2018-07-15 11:47:34,331 WARNING EPOCH - 2 : supplied example count (0) did not equal expected count (2647)\n",
      "2018-07-15 11:47:34,342 INFO EPOCH - 3 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2018-07-15 11:47:34,344 WARNING EPOCH - 3 : supplied example count (0) did not equal expected count (2647)\n",
      "2018-07-15 11:47:34,352 INFO EPOCH - 4 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2018-07-15 11:47:34,358 WARNING EPOCH - 4 : supplied example count (0) did not equal expected count (2647)\n",
      "2018-07-15 11:47:34,370 INFO EPOCH - 5 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2018-07-15 11:47:34,372 WARNING EPOCH - 5 : supplied example count (0) did not equal expected count (2647)\n",
      "2018-07-15 11:47:34,373 INFO training on a 0 raw words (0 effective words) took 0.1s, 0 effective words/s\n",
      "2018-07-15 11:47:34,378 WARNING Effective 'alpha' higher than previous training cycles\n",
      "2018-07-15 11:47:34,380 INFO training model with -1 workers on 4038 vocabulary and 150 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2018-07-15 11:47:34,388 INFO EPOCH - 1 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2018-07-15 11:47:34,398 WARNING EPOCH - 1 : supplied example count (0) did not equal expected count (2647)\n",
      "2018-07-15 11:47:34,408 INFO EPOCH - 2 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2018-07-15 11:47:34,411 WARNING EPOCH - 2 : supplied example count (0) did not equal expected count (2647)\n",
      "2018-07-15 11:47:34,414 INFO EPOCH - 3 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2018-07-15 11:47:34,425 WARNING EPOCH - 3 : supplied example count (0) did not equal expected count (2647)\n",
      "2018-07-15 11:47:34,434 INFO EPOCH - 4 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2018-07-15 11:47:34,436 WARNING EPOCH - 4 : supplied example count (0) did not equal expected count (2647)\n",
      "2018-07-15 11:47:34,448 INFO EPOCH - 5 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2018-07-15 11:47:34,450 WARNING EPOCH - 5 : supplied example count (0) did not equal expected count (2647)\n",
      "2018-07-15 11:47:34,452 INFO training on a 0 raw words (0 effective words) took 0.1s, 0 effective words/s\n",
      "2018-07-15 11:47:34,456 WARNING Effective 'alpha' higher than previous training cycles\n",
      "2018-07-15 11:47:34,458 INFO training model with -1 workers on 4038 vocabulary and 150 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2018-07-15 11:47:34,467 INFO EPOCH - 1 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2018-07-15 11:47:34,468 WARNING EPOCH - 1 : supplied example count (0) did not equal expected count (2647)\n",
      "2018-07-15 11:47:34,479 INFO EPOCH - 2 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2018-07-15 11:47:34,481 WARNING EPOCH - 2 : supplied example count (0) did not equal expected count (2647)\n",
      "2018-07-15 11:47:34,487 INFO EPOCH - 3 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2018-07-15 11:47:34,491 WARNING EPOCH - 3 : supplied example count (0) did not equal expected count (2647)\n",
      "2018-07-15 11:47:34,499 INFO EPOCH - 4 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2018-07-15 11:47:34,505 WARNING EPOCH - 4 : supplied example count (0) did not equal expected count (2647)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 \n",
      "Iteration 1 \n",
      "Iteration 2 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-07-15 11:47:34,511 INFO EPOCH - 5 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2018-07-15 11:47:34,519 WARNING EPOCH - 5 : supplied example count (0) did not equal expected count (2647)\n",
      "2018-07-15 11:47:34,520 INFO training on a 0 raw words (0 effective words) took 0.1s, 0 effective words/s\n",
      "2018-07-15 11:47:34,525 WARNING Effective 'alpha' higher than previous training cycles\n",
      "2018-07-15 11:47:34,527 INFO training model with -1 workers on 4038 vocabulary and 150 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2018-07-15 11:47:34,538 INFO EPOCH - 1 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2018-07-15 11:47:34,540 WARNING EPOCH - 1 : supplied example count (0) did not equal expected count (2647)\n",
      "2018-07-15 11:47:34,543 INFO EPOCH - 2 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2018-07-15 11:47:34,553 WARNING EPOCH - 2 : supplied example count (0) did not equal expected count (2647)\n",
      "2018-07-15 11:47:34,557 INFO EPOCH - 3 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2018-07-15 11:47:34,566 WARNING EPOCH - 3 : supplied example count (0) did not equal expected count (2647)\n",
      "2018-07-15 11:47:34,572 INFO EPOCH - 4 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2018-07-15 11:47:34,578 WARNING EPOCH - 4 : supplied example count (0) did not equal expected count (2647)\n",
      "2018-07-15 11:47:34,581 INFO EPOCH - 5 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2018-07-15 11:47:34,591 WARNING EPOCH - 5 : supplied example count (0) did not equal expected count (2647)\n",
      "2018-07-15 11:47:34,593 INFO training on a 0 raw words (0 effective words) took 0.1s, 0 effective words/s\n",
      "2018-07-15 11:47:34,597 WARNING Effective 'alpha' higher than previous training cycles\n",
      "2018-07-15 11:47:34,599 INFO training model with -1 workers on 4038 vocabulary and 150 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2018-07-15 11:47:34,610 INFO EPOCH - 1 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2018-07-15 11:47:34,611 WARNING EPOCH - 1 : supplied example count (0) did not equal expected count (2647)\n",
      "2018-07-15 11:47:34,620 INFO EPOCH - 2 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2018-07-15 11:47:34,622 WARNING EPOCH - 2 : supplied example count (0) did not equal expected count (2647)\n",
      "2018-07-15 11:47:34,631 INFO EPOCH - 3 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2018-07-15 11:47:34,632 WARNING EPOCH - 3 : supplied example count (0) did not equal expected count (2647)\n",
      "2018-07-15 11:47:34,638 INFO EPOCH - 4 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2018-07-15 11:47:34,643 WARNING EPOCH - 4 : supplied example count (0) did not equal expected count (2647)\n",
      "2018-07-15 11:47:34,649 INFO EPOCH - 5 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2018-07-15 11:47:34,657 WARNING EPOCH - 5 : supplied example count (0) did not equal expected count (2647)\n",
      "2018-07-15 11:47:34,658 INFO training on a 0 raw words (0 effective words) took 0.1s, 0 effective words/s\n",
      "2018-07-15 11:47:34,663 WARNING Effective 'alpha' higher than previous training cycles\n",
      "2018-07-15 11:47:34,664 INFO training model with -1 workers on 4038 vocabulary and 150 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2018-07-15 11:47:34,669 INFO EPOCH - 1 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2018-07-15 11:47:34,678 WARNING EPOCH - 1 : supplied example count (0) did not equal expected count (2647)\n",
      "2018-07-15 11:47:34,682 INFO EPOCH - 2 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2018-07-15 11:47:34,688 WARNING EPOCH - 2 : supplied example count (0) did not equal expected count (2647)\n",
      "2018-07-15 11:47:34,697 INFO EPOCH - 3 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2018-07-15 11:47:34,699 WARNING EPOCH - 3 : supplied example count (0) did not equal expected count (2647)\n",
      "2018-07-15 11:47:34,704 INFO EPOCH - 4 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2018-07-15 11:47:34,710 WARNING EPOCH - 4 : supplied example count (0) did not equal expected count (2647)\n",
      "2018-07-15 11:47:34,719 INFO EPOCH - 5 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3 \n",
      "Iteration 4 \n",
      "Iteration 5 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-07-15 11:47:34,727 WARNING EPOCH - 5 : supplied example count (0) did not equal expected count (2647)\n",
      "2018-07-15 11:47:34,729 INFO training on a 0 raw words (0 effective words) took 0.1s, 0 effective words/s\n",
      "2018-07-15 11:47:34,734 WARNING Effective 'alpha' higher than previous training cycles\n",
      "2018-07-15 11:47:34,735 INFO training model with -1 workers on 4038 vocabulary and 150 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2018-07-15 11:47:34,738 INFO EPOCH - 1 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2018-07-15 11:47:34,749 WARNING EPOCH - 1 : supplied example count (0) did not equal expected count (2647)\n",
      "2018-07-15 11:47:34,757 INFO EPOCH - 2 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2018-07-15 11:47:34,761 WARNING EPOCH - 2 : supplied example count (0) did not equal expected count (2647)\n",
      "2018-07-15 11:47:34,766 INFO EPOCH - 3 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2018-07-15 11:47:34,775 WARNING EPOCH - 3 : supplied example count (0) did not equal expected count (2647)\n",
      "2018-07-15 11:47:34,780 INFO EPOCH - 4 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2018-07-15 11:47:34,784 WARNING EPOCH - 4 : supplied example count (0) did not equal expected count (2647)\n",
      "2018-07-15 11:47:34,793 INFO EPOCH - 5 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2018-07-15 11:47:34,799 WARNING EPOCH - 5 : supplied example count (0) did not equal expected count (2647)\n",
      "2018-07-15 11:47:34,801 INFO training on a 0 raw words (0 effective words) took 0.1s, 0 effective words/s\n",
      "2018-07-15 11:47:34,807 WARNING Effective 'alpha' higher than previous training cycles\n",
      "2018-07-15 11:47:34,809 INFO training model with -1 workers on 4038 vocabulary and 150 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2018-07-15 11:47:34,813 INFO EPOCH - 1 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2018-07-15 11:47:34,817 WARNING EPOCH - 1 : supplied example count (0) did not equal expected count (2647)\n",
      "2018-07-15 11:47:34,832 INFO EPOCH - 2 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2018-07-15 11:47:34,833 WARNING EPOCH - 2 : supplied example count (0) did not equal expected count (2647)\n",
      "2018-07-15 11:47:34,840 INFO EPOCH - 3 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2018-07-15 11:47:34,845 WARNING EPOCH - 3 : supplied example count (0) did not equal expected count (2647)\n",
      "2018-07-15 11:47:34,856 INFO EPOCH - 4 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2018-07-15 11:47:34,858 WARNING EPOCH - 4 : supplied example count (0) did not equal expected count (2647)\n",
      "2018-07-15 11:47:34,864 INFO EPOCH - 5 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2018-07-15 11:47:34,870 WARNING EPOCH - 5 : supplied example count (0) did not equal expected count (2647)\n",
      "2018-07-15 11:47:34,874 INFO training on a 0 raw words (0 effective words) took 0.1s, 0 effective words/s\n",
      "2018-07-15 11:47:34,878 WARNING Effective 'alpha' higher than previous training cycles\n",
      "2018-07-15 11:47:34,879 INFO training model with -1 workers on 4038 vocabulary and 150 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2018-07-15 11:47:34,890 INFO EPOCH - 1 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2018-07-15 11:47:34,892 WARNING EPOCH - 1 : supplied example count (0) did not equal expected count (2647)\n",
      "2018-07-15 11:47:34,902 INFO EPOCH - 2 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2018-07-15 11:47:34,904 WARNING EPOCH - 2 : supplied example count (0) did not equal expected count (2647)\n",
      "2018-07-15 11:47:34,915 INFO EPOCH - 3 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2018-07-15 11:47:34,918 WARNING EPOCH - 3 : supplied example count (0) did not equal expected count (2647)\n",
      "2018-07-15 11:47:34,925 INFO EPOCH - 4 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2018-07-15 11:47:34,929 WARNING EPOCH - 4 : supplied example count (0) did not equal expected count (2647)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 6 \n",
      "Iteration 7 \n",
      "Iteration 8 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-07-15 11:47:34,935 INFO EPOCH - 5 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2018-07-15 11:47:34,944 WARNING EPOCH - 5 : supplied example count (0) did not equal expected count (2647)\n",
      "2018-07-15 11:47:34,945 INFO training on a 0 raw words (0 effective words) took 0.1s, 0 effective words/s\n",
      "2018-07-15 11:47:34,952 WARNING Effective 'alpha' higher than previous training cycles\n",
      "2018-07-15 11:47:34,953 INFO training model with -1 workers on 4038 vocabulary and 150 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2018-07-15 11:47:34,961 INFO EPOCH - 1 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2018-07-15 11:47:34,967 WARNING EPOCH - 1 : supplied example count (0) did not equal expected count (2647)\n",
      "2018-07-15 11:47:34,973 INFO EPOCH - 2 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2018-07-15 11:47:34,980 WARNING EPOCH - 2 : supplied example count (0) did not equal expected count (2647)\n",
      "2018-07-15 11:47:34,988 INFO EPOCH - 3 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2018-07-15 11:47:34,995 WARNING EPOCH - 3 : supplied example count (0) did not equal expected count (2647)\n",
      "2018-07-15 11:47:34,998 INFO EPOCH - 4 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2018-07-15 11:47:35,010 WARNING EPOCH - 4 : supplied example count (0) did not equal expected count (2647)\n",
      "2018-07-15 11:47:35,013 INFO EPOCH - 5 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2018-07-15 11:47:35,025 WARNING EPOCH - 5 : supplied example count (0) did not equal expected count (2647)\n",
      "2018-07-15 11:47:35,027 INFO training on a 0 raw words (0 effective words) took 0.1s, 0 effective words/s\n",
      "2018-07-15 11:47:35,031 WARNING Effective 'alpha' higher than previous training cycles\n",
      "2018-07-15 11:47:35,033 INFO training model with -1 workers on 4038 vocabulary and 150 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2018-07-15 11:47:35,039 INFO EPOCH - 1 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2018-07-15 11:47:35,047 WARNING EPOCH - 1 : supplied example count (0) did not equal expected count (2647)\n",
      "2018-07-15 11:47:35,057 INFO EPOCH - 2 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2018-07-15 11:47:35,060 WARNING EPOCH - 2 : supplied example count (0) did not equal expected count (2647)\n",
      "2018-07-15 11:47:35,064 INFO EPOCH - 3 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2018-07-15 11:47:35,071 WARNING EPOCH - 3 : supplied example count (0) did not equal expected count (2647)\n",
      "2018-07-15 11:47:35,078 INFO EPOCH - 4 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2018-07-15 11:47:35,080 WARNING EPOCH - 4 : supplied example count (0) did not equal expected count (2647)\n",
      "2018-07-15 11:47:35,092 INFO EPOCH - 5 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2018-07-15 11:47:35,103 WARNING EPOCH - 5 : supplied example count (0) did not equal expected count (2647)\n",
      "2018-07-15 11:47:35,105 INFO training on a 0 raw words (0 effective words) took 0.1s, 0 effective words/s\n",
      "2018-07-15 11:47:35,110 WARNING Effective 'alpha' higher than previous training cycles\n",
      "2018-07-15 11:47:35,112 INFO training model with -1 workers on 4038 vocabulary and 150 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2018-07-15 11:47:35,117 INFO EPOCH - 1 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2018-07-15 11:47:35,125 WARNING EPOCH - 1 : supplied example count (0) did not equal expected count (2647)\n",
      "2018-07-15 11:47:35,129 INFO EPOCH - 2 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2018-07-15 11:47:35,139 WARNING EPOCH - 2 : supplied example count (0) did not equal expected count (2647)\n",
      "2018-07-15 11:47:35,147 INFO EPOCH - 3 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2018-07-15 11:47:35,148 WARNING EPOCH - 3 : supplied example count (0) did not equal expected count (2647)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 9 \n",
      "Iteration 10 \n",
      "Iteration 11 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-07-15 11:47:35,153 INFO EPOCH - 4 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2018-07-15 11:47:35,162 WARNING EPOCH - 4 : supplied example count (0) did not equal expected count (2647)\n",
      "2018-07-15 11:47:35,170 INFO EPOCH - 5 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2018-07-15 11:47:35,175 WARNING EPOCH - 5 : supplied example count (0) did not equal expected count (2647)\n",
      "2018-07-15 11:47:35,176 INFO training on a 0 raw words (0 effective words) took 0.1s, 0 effective words/s\n",
      "2018-07-15 11:47:35,181 WARNING Effective 'alpha' higher than previous training cycles\n",
      "2018-07-15 11:47:35,182 INFO training model with -1 workers on 4038 vocabulary and 150 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2018-07-15 11:47:35,192 INFO EPOCH - 1 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2018-07-15 11:47:35,194 WARNING EPOCH - 1 : supplied example count (0) did not equal expected count (2647)\n",
      "2018-07-15 11:47:35,197 INFO EPOCH - 2 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2018-07-15 11:47:35,206 WARNING EPOCH - 2 : supplied example count (0) did not equal expected count (2647)\n",
      "2018-07-15 11:47:35,212 INFO EPOCH - 3 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2018-07-15 11:47:35,215 WARNING EPOCH - 3 : supplied example count (0) did not equal expected count (2647)\n",
      "2018-07-15 11:47:35,224 INFO EPOCH - 4 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2018-07-15 11:47:35,226 WARNING EPOCH - 4 : supplied example count (0) did not equal expected count (2647)\n",
      "2018-07-15 11:47:35,231 INFO EPOCH - 5 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2018-07-15 11:47:35,238 WARNING EPOCH - 5 : supplied example count (0) did not equal expected count (2647)\n",
      "2018-07-15 11:47:35,239 INFO training on a 0 raw words (0 effective words) took 0.1s, 0 effective words/s\n",
      "2018-07-15 11:47:35,244 WARNING Effective 'alpha' higher than previous training cycles\n",
      "2018-07-15 11:47:35,246 INFO training model with -1 workers on 4038 vocabulary and 150 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2018-07-15 11:47:35,248 INFO EPOCH - 1 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2018-07-15 11:47:35,256 WARNING EPOCH - 1 : supplied example count (0) did not equal expected count (2647)\n",
      "2018-07-15 11:47:35,262 INFO EPOCH - 2 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2018-07-15 11:47:35,271 WARNING EPOCH - 2 : supplied example count (0) did not equal expected count (2647)\n",
      "2018-07-15 11:47:35,278 INFO EPOCH - 3 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2018-07-15 11:47:35,280 WARNING EPOCH - 3 : supplied example count (0) did not equal expected count (2647)\n",
      "2018-07-15 11:47:35,284 INFO EPOCH - 4 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2018-07-15 11:47:35,290 WARNING EPOCH - 4 : supplied example count (0) did not equal expected count (2647)\n",
      "2018-07-15 11:47:35,293 INFO EPOCH - 5 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2018-07-15 11:47:35,304 WARNING EPOCH - 5 : supplied example count (0) did not equal expected count (2647)\n",
      "2018-07-15 11:47:35,306 INFO training on a 0 raw words (0 effective words) took 0.1s, 0 effective words/s\n",
      "2018-07-15 11:47:35,310 WARNING Effective 'alpha' higher than previous training cycles\n",
      "2018-07-15 11:47:35,312 INFO training model with -1 workers on 4038 vocabulary and 150 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2018-07-15 11:47:35,319 INFO EPOCH - 1 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2018-07-15 11:47:35,321 WARNING EPOCH - 1 : supplied example count (0) did not equal expected count (2647)\n",
      "2018-07-15 11:47:35,324 INFO EPOCH - 2 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2018-07-15 11:47:35,332 WARNING EPOCH - 2 : supplied example count (0) did not equal expected count (2647)\n",
      "2018-07-15 11:47:35,337 INFO EPOCH - 3 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2018-07-15 11:47:35,347 WARNING EPOCH - 3 : supplied example count (0) did not equal expected count (2647)\n",
      "2018-07-15 11:47:35,354 INFO EPOCH - 4 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2018-07-15 11:47:35,356 WARNING EPOCH - 4 : supplied example count (0) did not equal expected count (2647)\n",
      "2018-07-15 11:47:35,363 INFO EPOCH - 5 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2018-07-15 11:47:35,368 WARNING EPOCH - 5 : supplied example count (0) did not equal expected count (2647)\n",
      "2018-07-15 11:47:35,370 INFO training on a 0 raw words (0 effective words) took 0.1s, 0 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 12 \n",
      "Iteration 13 \n",
      "Iteration 14 \n"
     ]
    }
   ],
   "source": [
    "sentences = [TaggedDocument(words=splitNgrams(w), tags=[t]) for w,t in  zip(data_train.data, data_train.target)]\n",
    "model = Doc2Vec(vector_size=150, \n",
    "                window=10,\n",
    "                min_count=10,\n",
    "                workers=-1,\n",
    "                alpha=0.025,\n",
    "                min_alpha=0.00025\n",
    "                ) # use fixed learning rate\n",
    "model.build_vocab(sentences);\n",
    "model.random.seed(0);\n",
    "random.seed(4);\n",
    "seq = np.arange(len(sentences));\n",
    "for epoch in range(15):\n",
    "    print (\"Iteration %s \"% epoch)\n",
    "    model.train(sentences,total_examples=model.corpus_count, epochs=model.epochs)\n",
    "    model.alpha -= 0.0002 # decrease the learning rate\n",
    "    model.min_alpha = model.alpha/100.0 # fix the learning rate, no deca\n",
    "    random.shuffle(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.random.seed(0)\n",
    "train_vector = np.vstack([\n",
    "     model.infer_vector( doc_words=splitNgrams(d),steps=40) for d in data_train.data\n",
    "]);\n",
    "test_vector = np.vstack([\n",
    "     model.infer_vector( doc_words=splitNgrams(d),steps=40) for d in data_test.data\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotModel(vectors,labels):\n",
    "    from sklearn.manifold import TSNE, Isomap\n",
    "    pca = PCA(2)\n",
    "    comps = np.round(pca.fit_transform(vectors),2)\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.scatter(comps[:,0],comps[:,1],cmap=plt.cm.jet,c=labels)\n",
    "    for i,label in  enumerate(labels):\n",
    "        plt.text(comps[i,0], comps[i,1], label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plotModel(train_vector, data_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plotModel(test_vector, data_test.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    #print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LogisticRegression();\n",
    "clf.fit(train_vector, data_train.target)\n",
    "pred = clf.predict(test_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix,precision_recall_fscore_support, classification_report\n",
    "plt.figure(figsize=(10,10))\n",
    "plot_confusion_matrix(confusion_matrix(pred, data_test.target),classes=[c[-2:] for c in clf.classes_])\n",
    "print(classification_report(pred, data_test.target))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
