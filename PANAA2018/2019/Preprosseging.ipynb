{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from  gensim.models import KeyedVectors\n",
    "import gzip\n",
    "import zipfile\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "import glob\n",
    "from time import time\n",
    "import pandas as pd\n",
    "from os.path import join as pathjoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseDir = '/Users/joseeleandrocustodio/dataScienceNotebook/PANAA2018';\n",
    "\n",
    "corpusTraining    = 'pan18-cross-domain-authorship-attribution-training-dataset-2017-12-02';\n",
    "corpusEvaluation  = 'pan18-cross-domain-authorship-attribution-test-dataset2-2018-04-20';\n",
    "corpusEach1 = 'AvaliacaoPT';\n",
    "\n",
    "currentCorpus = corpusEach1;\n",
    "\n",
    "inputDir= pathjoin(baseDir,currentCorpus);\n",
    "outputDir= pathjoin(baseDir,'out');\n",
    "if not os.path.exists(outputDir):\n",
    "    os.mkdir(outputDir);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def problemDoc(inputDir,problem):\n",
    "    train_docs, train_labels, _   = zip(*problem['candidates'])\n",
    "    test_docs, _, test_filename   = zip(*problem['unknown']);\n",
    "    test_labels = pan.readGroundTruh(pathjoin(inputDir, problem['problem'], 'ground-truth.json'),test_filename)\n",
    "    return train_docs, train_labels, test_docs, test_labels,test_filename;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex_cleaning = [re.compile(w) for w in[\n",
    "    r'[\\u4E00-\\u9FA5]+',  #chinese,\n",
    "    r'[\\u3040-\\u309F]+'  #hiragana\n",
    "]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def problemsToTokens(problems, inputDir):\n",
    "    def clean(d):\n",
    "        for r in regex_cleaning:\n",
    "            d = r.sub('',d);\n",
    "        return d;\n",
    "    tokens = {};\n",
    "    pattern = re.compile(r\"(?u)\\b\\w\\w+\\b\");\n",
    "    for problem in problems:\n",
    "        lang = problem['language'];\n",
    "        if lang == 'sp':\n",
    "            lang = 'es';\n",
    "        train_docs, _, test_docs, _, _ = problemDoc(inputDir,problem);\n",
    "        docs = list(train_docs)+list(test_docs);\n",
    "\n",
    "        if lang not in tokens:\n",
    "            tokens[lang] = list();\n",
    "        tokens[lang] +=list(set([w  for d in docs for w in pattern.findall(clean(d))]));\n",
    "        tokens[lang] = sorted(list(set(tokens[lang])));\n",
    "    return tokens;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "collectionsCorpus = [\n",
    "    pan.readCollectionsOfProblems(pathjoin(baseDir,c))\n",
    "    for c in [corpusTraining, corpusEvaluation, corpusEach1]\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabCorpuses = [\n",
    "    problemsToTokens(col,pathjoin(baseDir,c))\n",
    "    for col,c in zip(collectionsCorpus, [corpusTraining, corpusEvaluation, corpusEach1])\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['en', 'fr', 'it', 'pl', 'es'])\n",
      "dict_keys(['en', 'fr', 'it', 'pl', 'es'])\n",
      "dict_keys(['pt', 'en'])\n"
     ]
    }
   ],
   "source": [
    "for v in vocabCorpuses:\n",
    "    print(v.keys());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabTotal = {};\n",
    "\n",
    "for t in vocabCorpuses:\n",
    "    for k,v in t.items():\n",
    "        if k not in vocabTotal:\n",
    "            vocabTotal[k] = [];\n",
    "        vocabTotal[k] =  sorted(list(set(vocabTotal[k] + t[k])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en 13556 ['Jersey', 'Jerusalem', 'Jess', 'Jessica', 'Jesus'] \n",
      "\n",
      "fr 15656 ['ronge', 'rongeait', 'ronger', 'ronronna', 'ronronnant'] \n",
      "\n",
      "it 20284 ['osservando', 'osservandola', 'osservandolo', 'osservare', 'osservargli'] \n",
      "\n",
      "pl 34367 ['quidditcha', 'quidditchu', 'quo', 'ra', 'rabarbarowym'] \n",
      "\n",
      "es 21188 ['suspirar', 'suspiraron', 'suspiras', 'suspiro', 'suspiros'] \n",
      "\n",
      "en 13899 ['hilt', 'him', 'himself', 'hindered', 'hinges'] \n",
      "\n",
      "fr 17698 ['glaça', 'glissa', 'glissaient', 'glissait', 'glissant'] \n",
      "\n",
      "it 20690 ['mancanze', 'mancare', 'mancargli', 'mancasse', 'mancata'] \n",
      "\n",
      "pl 35761 ['graczy', 'grafficiarzy', 'graj', 'grajka', 'grają'] \n",
      "\n",
      "es 20625 ['autónoma', 'avalancha', 'avance', 'avancemos', 'avances'] \n",
      "\n",
      "pt 24161 ['Carro', 'Carta', 'Cartazes', 'Carteira', 'Cartilha'] \n",
      "\n",
      "en 19406 ['climate', 'climb', 'climbed', 'climbing', 'cling'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for t in vocabCorpuses:\n",
    "    for k,v in t.items():\n",
    "        s = random.randint(0,len(v));\n",
    "        print(k,len(v), v[s:(s+5)],'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en 30149 ['blasted', 'blasters', 'blastin', 'blasting', 'blatant'] \n",
      "\n",
      "fr 24698 ['mortelle', 'mortellement', 'mortels', 'mortes', 'mortifié'] \n",
      "\n",
      "it 30291 ['accarezzandole', 'accarezzandoli', 'accarezzandolo', 'accarezzandone', 'accarezzandosi'] \n",
      "\n",
      "pl 54181 ['niech', 'niechcenia', 'niechciana', 'niechciane', 'niechcianego'] \n",
      "\n",
      "es 30807 ['pequeñín', 'pequeñísima', 'pequeñísimo', 'per', 'percata'] \n",
      "\n",
      "pt 24161 ['abandoná', 'abano', 'abastecem', 'abate', 'abater'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for k,v in vocabTotal.items():\n",
    "    s = random.randint(0,len(v));\n",
    "    print(k,len(v), v[s:(s+5)],'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readNLPLFile(filename):\n",
    "    with zipfile.ZipFile(os.path.join(w2v_repository,filename), \"r\") as archive:\n",
    "        stream =  archive.open(\"model.txt\");\n",
    "        model = KeyedVectors.load_word2vec_format(stream, binary=False, unicode_errors='replace')\n",
    "    return model;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readGoogleFile():\n",
    "    return KeyedVectors.load_word2vec_format(\n",
    "        os.path.join(w2v_repository,'GoogleNews-vectors-negative300.bin.gz')\n",
    "        ,binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_repository = '/Users/joseeleandrocustodio/Downloads/w2v_repository/'\n",
    "embeddingsFiles = {\n",
    "    'it':'w2v Italian CoNLL17 corpus.zip',\n",
    "    'fr':'w2v French CoNLL17 corpus.zip',\n",
    "    #'de':'w2v Dutch CoNLL17 corpus.zip',\n",
    "    'pl':'w2v Polish CoNLL17 corpus.zip',\n",
    "    'it':'w2v Italian CoNLL17 corpus.zip',\n",
    "    'es': 'w2v Spanish CoNLL17 corpus.zip',\n",
    "    'pt': 'w2v Portuguese CoNLL17 corpus.zip',\n",
    "    'en':'GoogleNews-vectors-negative300.bin.gz'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Language it\n",
      "cached\n",
      "\n",
      "\n",
      "\n",
      "Language fr\n",
      "cached\n",
      "\n",
      "\n",
      "\n",
      "Language pl\n",
      "cached\n",
      "\n",
      "\n",
      "\n",
      "Language es\n",
      "cached\n",
      "\n",
      "\n",
      "\n",
      "Language pt\n",
      "cached\n",
      "\n",
      "\n",
      "\n",
      "Language en\n",
      "Loading zip Done in 118.733s\n",
      "Filtering model Done in 2.226s\n",
      "Language en - Vocab Total 30149 - Embedding found 27720\n",
      "Writing model Done in 20.645s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for lang, file in embeddingsFiles.items():\n",
    "    print(\"\\n\\n\\nLanguage %s\" % lang);\n",
    "    \n",
    "    if os.path.exists(pathjoin('embedding_cache','w2v_'+lang+'.txt.gz')):\n",
    "        print(\"cached\");\n",
    "        continue;\n",
    "    \n",
    "    print(\"Loading zip\",end=' ');t0 = time();\n",
    "    if '.zip' in file:\n",
    "        model = readNLPLFile(file);\n",
    "    else:\n",
    "        model = readGoogleFile();\n",
    "    print(\"Done in %0.3fs\" % (time() - t0))\n",
    "    \n",
    "    print(\"Filtering model\",end=' ');t0 = time();\n",
    "    vocabLang = vocabTotal[lang];\n",
    "    vocabLang = {w:model[w] for w in vocabLang if w in model};\n",
    "    print(\"Done in %0.3fs\" % (time() - t0));\n",
    "    embeddingSize = model.vector_size;\n",
    "    \n",
    "    \n",
    "    del model;\n",
    "    import gc;\n",
    "    gc.collect();\n",
    "    \n",
    "    print(\"Language %s - Vocab Total %s - Embedding found %s\" %(lang, len(vocabTotal[lang]), len(vocabLang)))\n",
    "\n",
    "    print(\"Writing model\",end=' ');t0 = time();\n",
    "    with gzip.open(pathjoin('embedding_cache','w2v_'+lang+'.txt.gz'), 'w') as f:\n",
    "        f.write((\"%s %s\\n\"%(len(vocabLang),embeddingSize)).encode('utf-8'))\n",
    "        for w in sorted(list(vocabLang.keys())):\n",
    "            a = \" \".join([str(f) for f in vocabLang[w]]);\n",
    "            line = \"%s %s\\n\" % (w, a)\n",
    "            f.write(line.encode('utf-8'))\n",
    "    print(\"Done in %0.3fs\" % (time() - t0));\n",
    "os.system( \"say finished\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Language pt - Vocab Total 24161 - Embedding found 16971\n",
    "Language en - Vocab Total 30149 - Embedding found 27720"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
