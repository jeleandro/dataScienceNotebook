{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from  gensim.models import KeyedVectors\n",
    "import gzip\n",
    "import zipfile\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "import glob\n",
    "from time import time\n",
    "import pandas as pd\n",
    "from os.path import join as pathjoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseDir = '/Users/joseeleandrocustodio/dataScienceNotebook/PANAA2018';\n",
    "\n",
    "corpusTraining    = 'pan18-cross-domain-authorship-attribution-training-dataset-2017-12-02';\n",
    "corpusEvaluation  = 'pan18-cross-domain-authorship-attribution-test-dataset2-2018-04-20';\n",
    "corpusEach1 = 'AvaliacaoPT';\n",
    "\n",
    "currentCorpus = corpusEach1;\n",
    "\n",
    "inputDir= pathjoin(baseDir,currentCorpus);\n",
    "outputDir= pathjoin(baseDir,'out');\n",
    "if not os.path.exists(outputDir):\n",
    "    os.mkdir(outputDir);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def problemDoc(inputDir,problem):\n",
    "    train_docs, train_labels, _   = zip(*problem['candidates'])\n",
    "    test_docs, _, test_filename   = zip(*problem['unknown']);\n",
    "    test_labels = pan.readGroundTruh(pathjoin(inputDir, problem['problem'], 'ground-truth.json'),test_filename)\n",
    "    return train_docs, train_labels, test_docs, test_labels,test_filename;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex_cleaning = [re.compile(w) for w in[\n",
    "    r'[\\u4E00-\\u9FA5]+',  #chinese,\n",
    "    r'[\\u3040-\\u309F]+'  #hiragana\n",
    "]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def problemsToTokens(problems, inputDir):\n",
    "    def clean(d):\n",
    "        for r in regex_cleaning:\n",
    "            d = r.sub('',d);\n",
    "        return d;\n",
    "    tokens = {};\n",
    "    pattern = re.compile(r\"(?u)\\b\\w\\w+\\b\");\n",
    "    for problem in problems:\n",
    "        lang = problem['language'];\n",
    "        if lang == 'sp':\n",
    "            lang = 'es';\n",
    "        train_docs, _, test_docs, _, _ = problemDoc(inputDir,problem);\n",
    "        docs = list(train_docs)+list(test_docs);\n",
    "\n",
    "        if lang not in tokens:\n",
    "            tokens[lang] = list();\n",
    "        tokens[lang] +=list(set([w  for d in docs for w in pattern.findall(clean(d))]));\n",
    "        tokens[lang] = sorted(list(set(tokens[lang])));\n",
    "    return tokens;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "collectionsCorpus = [\n",
    "    pan.readCollectionsOfProblems(pathjoin(baseDir,c))\n",
    "    for c in [corpusTraining, corpusEvaluation, corpusEach1]\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabCorpuses = [\n",
    "    problemsToTokens(col,pathjoin(baseDir,c))\n",
    "    for col,c in zip(collectionsCorpus, [corpusTraining, corpusEvaluation, corpusEach1])\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['en', 'fr', 'it', 'pl', 'es'])\n",
      "dict_keys(['en', 'fr', 'it', 'pl', 'es'])\n",
      "dict_keys(['pt', 'en'])\n"
     ]
    }
   ],
   "source": [
    "for v in vocabCorpuses:\n",
    "    print(v.keys());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabTotal = {};\n",
    "\n",
    "for t in vocabCorpuses:\n",
    "    for k,v in t.items():\n",
    "        if k not in vocabTotal:\n",
    "            vocabTotal[k] = [];\n",
    "        vocabTotal[k] =  sorted(list(set(vocabTotal[k] + t[k])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en 13556 ['Jersey', 'Jerusalem', 'Jess', 'Jessica', 'Jesus'] \n",
      "\n",
      "fr 15656 ['ronge', 'rongeait', 'ronger', 'ronronna', 'ronronnant'] \n",
      "\n",
      "it 20284 ['osservando', 'osservandola', 'osservandolo', 'osservare', 'osservargli'] \n",
      "\n",
      "pl 34367 ['quidditcha', 'quidditchu', 'quo', 'ra', 'rabarbarowym'] \n",
      "\n",
      "es 21188 ['suspirar', 'suspiraron', 'suspiras', 'suspiro', 'suspiros'] \n",
      "\n",
      "en 13899 ['hilt', 'him', 'himself', 'hindered', 'hinges'] \n",
      "\n",
      "fr 17698 ['glaça', 'glissa', 'glissaient', 'glissait', 'glissant'] \n",
      "\n",
      "it 20690 ['mancanze', 'mancare', 'mancargli', 'mancasse', 'mancata'] \n",
      "\n",
      "pl 35761 ['graczy', 'grafficiarzy', 'graj', 'grajka', 'grają'] \n",
      "\n",
      "es 20625 ['autónoma', 'avalancha', 'avance', 'avancemos', 'avances'] \n",
      "\n",
      "pt 24161 ['Carro', 'Carta', 'Cartazes', 'Carteira', 'Cartilha'] \n",
      "\n",
      "en 19406 ['climate', 'climb', 'climbed', 'climbing', 'cling'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for t in vocabCorpuses:\n",
    "    for k,v in t.items():\n",
    "        s = random.randint(0,len(v));\n",
    "        print(k,len(v), v[s:(s+5)],'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en 30149 ['blasted', 'blasters', 'blastin', 'blasting', 'blatant'] \n",
      "\n",
      "fr 24698 ['mortelle', 'mortellement', 'mortels', 'mortes', 'mortifié'] \n",
      "\n",
      "it 30291 ['accarezzandole', 'accarezzandoli', 'accarezzandolo', 'accarezzandone', 'accarezzandosi'] \n",
      "\n",
      "pl 54181 ['niech', 'niechcenia', 'niechciana', 'niechciane', 'niechcianego'] \n",
      "\n",
      "es 30807 ['pequeñín', 'pequeñísima', 'pequeñísimo', 'per', 'percata'] \n",
      "\n",
      "pt 24161 ['abandoná', 'abano', 'abastecem', 'abate', 'abater'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for k,v in vocabTotal.items():\n",
    "    s = random.randint(0,len(v));\n",
    "    print(k,len(v), v[s:(s+5)],'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readNLPLFile(filename):\n",
    "    with zipfile.ZipFile(os.path.join(w2v_repository,filename), \"r\") as archive:\n",
    "        stream =  archive.open(\"model.txt\");\n",
    "        model = KeyedVectors.load_word2vec_format(stream, binary=False, unicode_errors='replace')\n",
    "    return model;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readGoogleFile():\n",
    "    return KeyedVectors.load_word2vec_format(\n",
    "        os.path.join(w2v_repository,'GoogleNews-vectors-negative300.bin.gz')\n",
    "        ,binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_repository = '/Users/joseeleandrocustodio/Downloads/w2v_repository/'\n",
    "embeddingsFiles = {\n",
    "    'it':'w2v Italian CoNLL17 corpus.zip',\n",
    "    'fr':'w2v French CoNLL17 corpus.zip',\n",
    "    #'de':'w2v Dutch CoNLL17 corpus.zip',\n",
    "    'pl':'w2v Polish CoNLL17 corpus.zip',\n",
    "    'it':'w2v Italian CoNLL17 corpus.zip',\n",
    "    'es': 'w2v Spanish CoNLL17 corpus.zip',\n",
    "    'pt': 'w2v Portuguese CoNLL17 corpus.zip',\n",
    "    'en':'GoogleNews-vectors-negative300.bin.gz'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Language it\n",
      "cached\n",
      "\n",
      "\n",
      "\n",
      "Language fr\n",
      "cached\n",
      "\n",
      "\n",
      "\n",
      "Language pl\n",
      "cached\n",
      "\n",
      "\n",
      "\n",
      "Language es\n",
      "cached\n",
      "\n",
      "\n",
      "\n",
      "Language pt\n",
      "cached\n",
      "\n",
      "\n",
      "\n",
      "Language en\n",
      "Loading zip Done in 118.733s\n",
      "Filtering model Done in 2.226s\n",
      "Language en - Vocab Total 30149 - Embedding found 27720\n",
      "Writing model Done in 20.645s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for lang, file in embeddingsFiles.items():\n",
    "    print(\"\\n\\n\\nLanguage %s\" % lang);\n",
    "    \n",
    "    if os.path.exists(pathjoin('embedding_cache','w2v_'+lang+'.txt.gz')):\n",
    "        print(\"cached\");\n",
    "        continue;\n",
    "    \n",
    "    print(\"Loading zip\",end=' ');t0 = time();\n",
    "    if '.zip' in file:\n",
    "        model = readNLPLFile(file);\n",
    "    else:\n",
    "        model = readGoogleFile();\n",
    "    print(\"Done in %0.3fs\" % (time() - t0))\n",
    "    \n",
    "    print(\"Filtering model\",end=' ');t0 = time();\n",
    "    vocabLang = vocabTotal[lang];\n",
    "    vocabLang = {w:model[w] for w in vocabLang if w in model};\n",
    "    print(\"Done in %0.3fs\" % (time() - t0));\n",
    "    embeddingSize = model.vector_size;\n",
    "    \n",
    "    \n",
    "    del model;\n",
    "    import gc;\n",
    "    gc.collect();\n",
    "    \n",
    "    print(\"Language %s - Vocab Total %s - Embedding found %s\" %(lang, len(vocabTotal[lang]), len(vocabLang)))\n",
    "\n",
    "    print(\"Writing model\",end=' ');t0 = time();\n",
    "    with gzip.open(pathjoin('embedding_cache','w2v_'+lang+'.txt.gz'), 'w') as f:\n",
    "        f.write((\"%s %s\\n\"%(len(vocabLang),embeddingSize)).encode('utf-8'))\n",
    "        for w in sorted(list(vocabLang.keys())):\n",
    "            a = \" \".join([str(f) for f in vocabLang[w]]);\n",
    "            line = \"%s %s\\n\" % (w, a)\n",
    "            f.write(line.encode('utf-8'))\n",
    "    print(\"Done in %0.3fs\" % (time() - t0));\n",
    "os.system( \"say finished\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Language pt - Vocab Total 24161 - Embedding found 16971\n",
    "Language en - Vocab Total 30149 - Embedding found 27720"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os;\n",
    "import json;\n",
    "import glob;\n",
    "import codecs;\n",
    "import zipfile;\n",
    "from pprint import pprint\n",
    "\n",
    "from os.path import join as pathjoin;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = '../pan18-cross-domain-authorship-attribution-test-dataset2-2018-04-20.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "                \n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>candidates</th>\n",
       "      <th>encoding</th>\n",
       "      <th>language</th>\n",
       "      <th>n_authors</th>\n",
       "      <th>problem-name</th>\n",
       "      <th>unknown</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[(hairdryer currently turned on high.\\n       ...</td>\n",
       "      <td>UTF-8</td>\n",
       "      <td>en</td>\n",
       "      <td>20</td>\n",
       "      <td>problem00001</td>\n",
       "      <td>[(still wearing a cool smile. He turned it on ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[(hey, perhaps he'd like to test that authorit...</td>\n",
       "      <td>UTF-8</td>\n",
       "      <td>en</td>\n",
       "      <td>15</td>\n",
       "      <td>problem00002</td>\n",
       "      <td>[(It had seemed like a good idea at the time.\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[(MORE BLISSED OUT AT THE VERY FACT THAT THEY ...</td>\n",
       "      <td>UTF-8</td>\n",
       "      <td>en</td>\n",
       "      <td>10</td>\n",
       "      <td>problem00003</td>\n",
       "      <td>[(or even worse, accept them) – and you’d foll...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[(to feel Link inside of him anyways. Ever sin...</td>\n",
       "      <td>UTF-8</td>\n",
       "      <td>en</td>\n",
       "      <td>5</td>\n",
       "      <td>problem00004</td>\n",
       "      <td>[(Every night they brought them.  New sacrific...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[(Du coin de l'œil il pouvait voir le sourire ...</td>\n",
       "      <td>UTF-8</td>\n",
       "      <td>fr</td>\n",
       "      <td>20</td>\n",
       "      <td>problem00005</td>\n",
       "      <td>[(mêler de leurs affaires.\\n\\nMais voilà, elle...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[(ai pas vu s'agrandir et s'affirmer, jusqu'à ...</td>\n",
       "      <td>UTF-8</td>\n",
       "      <td>fr</td>\n",
       "      <td>15</td>\n",
       "      <td>problem00006</td>\n",
       "      <td>[(est quoi ? »  Il désigna de sa canne les bou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[(qui allait tout revomir ensuite, Varus serra...</td>\n",
       "      <td>UTF-8</td>\n",
       "      <td>fr</td>\n",
       "      <td>10</td>\n",
       "      <td>problem00007</td>\n",
       "      <td>[(était une sorte de monstre.  « Sirius, ne re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[(même expérience que toi par rapport aux vamp...</td>\n",
       "      <td>UTF-8</td>\n",
       "      <td>fr</td>\n",
       "      <td>5</td>\n",
       "      <td>problem00008</td>\n",
       "      <td>[(une entrée brusque et terrifiante, tel un va...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[(aveva usato anche uno squallidissimo doppio ...</td>\n",
       "      <td>UTF-8</td>\n",
       "      <td>it</td>\n",
       "      <td>20</td>\n",
       "      <td>problem00009</td>\n",
       "      <td>[( \\n\\n \\n\\n \\n\\nAlbus è così intelligente che...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[(\\n\\n\\n\\n\\n\\n\\n\\n'Sarai mio.'\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
       "      <td>UTF-8</td>\n",
       "      <td>it</td>\n",
       "      <td>15</td>\n",
       "      <td>problem00010</td>\n",
       "      <td>[(giorno della gita ad Hogsmeade, sarebbe venu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[(grafia frettolosa, sconnessa ed ispirata: qu...</td>\n",
       "      <td>UTF-8</td>\n",
       "      <td>it</td>\n",
       "      <td>10</td>\n",
       "      <td>problem00011</td>\n",
       "      <td>[(fuori.”\\n\\n \\n\\n \\n\\n \\n\\n \\n\\n§§§§§\\n\\n \\n\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[(circondandogli un polso con la mano per ferm...</td>\n",
       "      <td>UTF-8</td>\n",
       "      <td>it</td>\n",
       "      <td>5</td>\n",
       "      <td>problem00012</td>\n",
       "      <td>[(inclinate, le ‘o’ piccole, le ‘m’ con degli ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[(zmuszony do odsłaniania się w taki sposób – ...</td>\n",
       "      <td>UTF-8</td>\n",
       "      <td>pl</td>\n",
       "      <td>20</td>\n",
       "      <td>problem00013</td>\n",
       "      <td>[(kto skupi na nim swoją uwagę, zostawiając ws...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[(Nigdy nie sądził, że zazna bólu większego, n...</td>\n",
       "      <td>UTF-8</td>\n",
       "      <td>pl</td>\n",
       "      <td>15</td>\n",
       "      <td>problem00014</td>\n",
       "      <td>[(to wygrać, nie będzie to coś, co ludzie zwyk...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[(***\\nOgrody Siódmego Nieba są słoneczne i ci...</td>\n",
       "      <td>UTF-8</td>\n",
       "      <td>pl</td>\n",
       "      <td>10</td>\n",
       "      <td>problem00015</td>\n",
       "      <td>[(aptekarzem a krawcem, była mała kawiarnia z ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[(Wyobrażenie seksu jako aktu dokonującego się...</td>\n",
       "      <td>UTF-8</td>\n",
       "      <td>pl</td>\n",
       "      <td>5</td>\n",
       "      <td>problem00016</td>\n",
       "      <td>[(wzruszyła ramionami.\\n\\n— Członkowie starych...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>[(-¿No me deseas un buen viaje?\\n\\nLa empujó c...</td>\n",
       "      <td>UTF-8</td>\n",
       "      <td>sp</td>\n",
       "      <td>20</td>\n",
       "      <td>problem00017</td>\n",
       "      <td>[(Era muy normal despedirte de tu familia para...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>[(ropa delante de él.\\n\\nDespués que los dos e...</td>\n",
       "      <td>UTF-8</td>\n",
       "      <td>sp</td>\n",
       "      <td>15</td>\n",
       "      <td>problem00018</td>\n",
       "      <td>[(. Si le prestaba atención, solía significar ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>[(que es hora de irnos a casa.\\n\\n—Pero sí no ...</td>\n",
       "      <td>UTF-8</td>\n",
       "      <td>sp</td>\n",
       "      <td>10</td>\n",
       "      <td>problem00019</td>\n",
       "      <td>[(que decoraban sutilmente, era preciosa, adem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>[(por un momento rojos como la sangre, la rabi...</td>\n",
       "      <td>UTF-8</td>\n",
       "      <td>sp</td>\n",
       "      <td>5</td>\n",
       "      <td>problem00020</td>\n",
       "      <td>[(Harry quedó a cuadros cuando vio a Draco Mal...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           candidates encoding language  \\\n",
       "0   [(hairdryer currently turned on high.\\n       ...    UTF-8       en   \n",
       "1   [(hey, perhaps he'd like to test that authorit...    UTF-8       en   \n",
       "2   [(MORE BLISSED OUT AT THE VERY FACT THAT THEY ...    UTF-8       en   \n",
       "3   [(to feel Link inside of him anyways. Ever sin...    UTF-8       en   \n",
       "4   [(Du coin de l'œil il pouvait voir le sourire ...    UTF-8       fr   \n",
       "5   [(ai pas vu s'agrandir et s'affirmer, jusqu'à ...    UTF-8       fr   \n",
       "6   [(qui allait tout revomir ensuite, Varus serra...    UTF-8       fr   \n",
       "7   [(même expérience que toi par rapport aux vamp...    UTF-8       fr   \n",
       "8   [(aveva usato anche uno squallidissimo doppio ...    UTF-8       it   \n",
       "9   [(\\n\\n\\n\\n\\n\\n\\n\\n'Sarai mio.'\\n\\n\\n\\n\\n\\n\\n\\n...    UTF-8       it   \n",
       "10  [(grafia frettolosa, sconnessa ed ispirata: qu...    UTF-8       it   \n",
       "11  [(circondandogli un polso con la mano per ferm...    UTF-8       it   \n",
       "12  [(zmuszony do odsłaniania się w taki sposób – ...    UTF-8       pl   \n",
       "13  [(Nigdy nie sądził, że zazna bólu większego, n...    UTF-8       pl   \n",
       "14  [(***\\nOgrody Siódmego Nieba są słoneczne i ci...    UTF-8       pl   \n",
       "15  [(Wyobrażenie seksu jako aktu dokonującego się...    UTF-8       pl   \n",
       "16  [(-¿No me deseas un buen viaje?\\n\\nLa empujó c...    UTF-8       sp   \n",
       "17  [(ropa delante de él.\\n\\nDespués que los dos e...    UTF-8       sp   \n",
       "18  [(que es hora de irnos a casa.\\n\\n—Pero sí no ...    UTF-8       sp   \n",
       "19  [(por un momento rojos como la sangre, la rabi...    UTF-8       sp   \n",
       "\n",
       "    n_authors  problem-name                                            unknown  \n",
       "0          20  problem00001  [(still wearing a cool smile. He turned it on ...  \n",
       "1          15  problem00002  [(It had seemed like a good idea at the time.\\...  \n",
       "2          10  problem00003  [(or even worse, accept them) – and you’d foll...  \n",
       "3           5  problem00004  [(Every night they brought them.  New sacrific...  \n",
       "4          20  problem00005  [(mêler de leurs affaires.\\n\\nMais voilà, elle...  \n",
       "5          15  problem00006  [(est quoi ? »  Il désigna de sa canne les bou...  \n",
       "6          10  problem00007  [(était une sorte de monstre.  « Sirius, ne re...  \n",
       "7           5  problem00008  [(une entrée brusque et terrifiante, tel un va...  \n",
       "8          20  problem00009  [( \\n\\n \\n\\n \\n\\nAlbus è così intelligente che...  \n",
       "9          15  problem00010  [(giorno della gita ad Hogsmeade, sarebbe venu...  \n",
       "10         10  problem00011  [(fuori.”\\n\\n \\n\\n \\n\\n \\n\\n \\n\\n§§§§§\\n\\n \\n\\...  \n",
       "11          5  problem00012  [(inclinate, le ‘o’ piccole, le ‘m’ con degli ...  \n",
       "12         20  problem00013  [(kto skupi na nim swoją uwagę, zostawiając ws...  \n",
       "13         15  problem00014  [(to wygrać, nie będzie to coś, co ludzie zwyk...  \n",
       "14         10  problem00015  [(aptekarzem a krawcem, była mała kawiarnia z ...  \n",
       "15          5  problem00016  [(wzruszyła ramionami.\\n\\n— Członkowie starych...  \n",
       "16         20  problem00017  [(Era muy normal despedirte de tu familia para...  \n",
       "17         15  problem00018  [(. Si le prestaba atención, solía significar ...  \n",
       "18         10  problem00019  [(que decoraban sutilmente, era preciosa, adem...  \n",
       "19          5  problem00020  [(Harry quedó a cuadros cuando vio a Draco Mal...  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(problems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "def readCollectionsOfProblems(path):\n",
    "    # Reading information about the collection\n",
    "    infocollection = path+os.sep+'collection-info.json'\n",
    "    with open(infocollection, 'r') as f:\n",
    "        problems  = [\n",
    "            {\n",
    "                'problem': attrib['problem-name'],\n",
    "                'language': attrib['language'],\n",
    "                'encoding': attrib['encoding'],\n",
    "            }\n",
    "            for attrib in json.load(f)\n",
    "            \n",
    "        ]\n",
    "\n",
    "    for index,problem in enumerate(problems):\n",
    "        unk_folder, candidates_folder = readProblem(path, problem['problem']); \n",
    "        problem['candidates_folder_count'] = len(candidates_folder);\n",
    "        problem['candidates'] = [];\n",
    "        for candidate in candidates_folder:\n",
    "            problem['candidates'].extend(read_files(pathjoin(path, problem['problem']),candidate));\n",
    "        \n",
    "        problem['unknown'] = read_files(pathjoin(path, problem['problem']),unk_folder);    \n",
    "\n",
    "    return problems;\n",
    "\n",
    "\n",
    "def readProblem(path, problem):\n",
    "    # Reading information about the problem\n",
    "    infoproblem = path+os.sep+problem+os.sep+'problem-info.json'\n",
    "    candidates = []\n",
    "    with open(infoproblem, 'r') as f:\n",
    "        fj = json.load(f)\n",
    "        unk_folder = fj['unknown-folder']\n",
    "        for attrib in fj['candidate-authors']:\n",
    "            candidates.append(attrib['author-name'])\n",
    "    return unk_folder, candidates;\n",
    "\n",
    "\n",
    "def read_files(path,label):\n",
    "    # Reads all text files located in the 'path' and assigns them to 'label' class\n",
    "    files = glob.glob(pathjoin(path,label,'*.txt'))\n",
    "    texts=[]\n",
    "    for i,v in enumerate(files):\n",
    "        f=codecs.open(v,'r',encoding='utf-8')\n",
    "        texts.append([f.read(),label, os.path.basename(v)])\n",
    "        f.close()\n",
    "    return texts\n",
    "\n",
    "\n",
    "#*******************************************************************************************************\n",
    "import warnings\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "def eval_measures(gt, pred):\n",
    "    \"\"\"Compute macro-averaged F1-scores, macro-averaged precision, \n",
    "    macro-averaged recall, and micro-averaged accuracy according the ad hoc\n",
    "    rules discussed at the top of this file.\n",
    "    Parameters\n",
    "    ----------\n",
    "    gt : dict\n",
    "        Ground truth, where keys indicate text file names\n",
    "        (e.g. `unknown00002.txt`), and values represent\n",
    "        author labels (e.g. `candidate00003`)\n",
    "    pred : dict\n",
    "        Predicted attribution, where keys indicate text file names\n",
    "        (e.g. `unknown00002.txt`), and values represent\n",
    "        author labels (e.g. `candidate00003`)\n",
    "    Returns\n",
    "    -------\n",
    "    f1 : float\n",
    "        Macro-averaged F1-score\n",
    "    precision : float\n",
    "        Macro-averaged precision\n",
    "    recall : float\n",
    "        Macro-averaged recall\n",
    "    accuracy : float\n",
    "        Micro-averaged F1-score\n",
    "    \"\"\"\n",
    "\n",
    "    actual_authors = list(gt.values())\n",
    "    encoder = LabelEncoder().fit(['<UNK>'] + actual_authors)\n",
    "\n",
    "    text_ids, gold_authors, silver_authors = [], [], []\n",
    "    for text_id in sorted(gt):\n",
    "        text_ids.append(text_id)\n",
    "        gold_authors.append(gt[text_id])\n",
    "        try:\n",
    "            silver_authors.append(pred[text_id])\n",
    "        except KeyError:\n",
    "            # missing attributions get <UNK>:\n",
    "            silver_authors.append('<UNK>')\n",
    "\n",
    "    assert len(text_ids) == len(gold_authors)\n",
    "    assert len(text_ids) == len(silver_authors)\n",
    "\n",
    "    # replace non-existent silver authors with '<UNK>':\n",
    "    silver_authors = [a if a in encoder.classes_ else '<UNK>' \n",
    "                      for a in silver_authors]\n",
    "\n",
    "    gold_author_ints   = encoder.transform(gold_authors)\n",
    "    silver_author_ints = encoder.transform(silver_authors)\n",
    "\n",
    "    # get F1 for individual classes (and suppress warnings):\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter('ignore')\n",
    "        f1 = f1_score(gold_author_ints,\n",
    "                  silver_author_ints,\n",
    "                  labels=list(set(gold_author_ints)),\n",
    "                  average='macro')\n",
    "        precision = precision_score(gold_author_ints,\n",
    "                  silver_author_ints,\n",
    "                  labels=list(set(gold_author_ints)),\n",
    "                  average='macro')\n",
    "        recall = recall_score(gold_author_ints,\n",
    "                  silver_author_ints,\n",
    "                  labels=list(set(gold_author_ints)),\n",
    "                  average='macro')\n",
    "        accuracy = accuracy_score(gold_author_ints,\n",
    "                  silver_author_ints)\n",
    "\n",
    "    return f1,precision,recall,accuracy\n",
    "\n",
    "\n",
    "def readGroundTruh(ground_truth_file, unkowndocs):\n",
    "    gt = {}\n",
    "    with open(ground_truth_file, 'r') as f:\n",
    "        for attrib in json.load(f)['ground_truth']:\n",
    "            gt[attrib['unknown-text']] = attrib['true-author'];\n",
    "\n",
    "    return [gt[d]  for d in unkowndocs];\n",
    "\n",
    "\n",
    "\n",
    "def evaluate(ground_truth_file,predictions_file):\n",
    "    # Calculates evaluation measures for a single attribution problem\n",
    "    gt = {}\n",
    "    with open(ground_truth_file, 'r') as f:\n",
    "        for attrib in json.load(f)['ground_truth']:\n",
    "            gt[attrib['unknown-text']] = attrib['true-author']\n",
    "\n",
    "    pred = {}\n",
    "    with open(predictions_file, 'r') as f:\n",
    "        for attrib in json.load(f):\n",
    "            if attrib['unknown-text'] not in pred:\n",
    "                pred[attrib['unknown-text']] = attrib['predicted-author']\n",
    "    f1,precision,recall,accuracy =  eval_measures(gt,pred)\n",
    "    return f1, precision, recall, accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
