{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook para o PAN - Atribuição Autoral - 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "#python basic libs\n",
    "from __future__ import print_function\n",
    "\n",
    "from tempfile import mkdtemp\n",
    "from shutil import rmtree\n",
    "import os;\n",
    "from os.path import join as pathjoin;\n",
    "\n",
    "import re;\n",
    "import glob;\n",
    "import json;\n",
    "import codecs;\n",
    "from collections import defaultdict;\n",
    "import pprint;\n",
    "\n",
    "\n",
    "from pprint import pprint\n",
    "from time import time\n",
    "import logging\n",
    "\n",
    "\n",
    "#data analysis libs\n",
    "import numpy as np;\n",
    "import pandas as pd;\n",
    "import matplotlib.pyplot as plt;\n",
    "import random;\n",
    "\n",
    "#machine learning libs\n",
    "#feature extraction\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "#preprocessing and transformation\n",
    "from sklearn.preprocessing import normalize, MaxAbsScaler, MinMaxScaler;\n",
    "from sklearn.preprocessing import LabelBinarizer;\n",
    "from sklearn.decomposition import PCA;\n",
    "from sklearn.metrics.pairwise import cosine_similarity;\n",
    "\n",
    "\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "\n",
    "#classifiers\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.multiclass import OneVsOneClassifier, OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from sklearn.feature_selection import RFE,SelectFpr,SelectPercentile, chi2;\n",
    "\n",
    "#\n",
    "from sklearn.ensemble import AdaBoostClassifier, BaggingClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "#model valuation\n",
    "from sklearn.model_selection import train_test_split;\n",
    "from sklearn.metrics import roc_auc_score, f1_score, precision_score, recall_score, accuracy_score;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Darwin-17.5.0-x86_64-i386-64bit\n",
      "NumPy 1.14.2\n",
      "SciPy 1.0.1\n",
      "Scikit-Learn 0.19.1\n"
     ]
    }
   ],
   "source": [
    "import platform; print(platform.platform())\n",
    "print(\"NumPy\", np.__version__)\n",
    "import scipy; print(\"SciPy\", scipy.__version__)\n",
    "import sklearn; print(\"Scikit-Learn\", sklearn.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### paths configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseDir = '/Users/joseeleandrocustodio/Dropbox/mestrado/02 - Pesquisa/code';\n",
    "\n",
    "inputDir= pathjoin(baseDir,'pan18aa');\n",
    "outputDir= pathjoin(baseDir,'out',\"oficial\");\n",
    "if not os.path.exists(outputDir):\n",
    "    os.mkdir(outputDir);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readCollectionsOfProblems(path):\n",
    "    # Reading information about the collection\n",
    "    infocollection = path+os.sep+'collection-info.json'\n",
    "    with open(infocollection, 'r') as f:\n",
    "        problems  = [\n",
    "            {\n",
    "                'problem': attrib['problem-name'],\n",
    "                'language': attrib['language'],\n",
    "                'encoding': attrib['encoding'],\n",
    "            }\n",
    "            for attrib in json.load(f)\n",
    "            \n",
    "        ]\n",
    "    return problems;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "problems = readCollectionsOfProblems(inputDir);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'encoding': u'UTF-8', 'language': u'en', 'problem': u'problem00001'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "problems[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readProblem(path, problem):\n",
    "    # Reading information about the problem\n",
    "    infoproblem = path+os.sep+problem+os.sep+'problem-info.json'\n",
    "    candidates = []\n",
    "    with open(infoproblem, 'r') as f:\n",
    "        fj = json.load(f)\n",
    "        unk_folder = fj['unknown-folder']\n",
    "        for attrib in fj['candidate-authors']:\n",
    "            candidates.append(attrib['author-name'])\n",
    "    return unk_folder, candidates;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_files(path,label):\n",
    "    # Reads all text files located in the 'path' and assigns them to 'label' class\n",
    "    files = glob.glob(pathjoin(path,label,'*.txt'))\n",
    "    texts=[]\n",
    "    for i,v in enumerate(files):\n",
    "        f=codecs.open(v,'r',encoding='utf-8')\n",
    "        texts.append((f.read(),label, os.path.basename(v)))\n",
    "        f.close()\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for index,problem in enumerate(problems):\n",
    "    unk_folder, candidates_folder = readProblem(inputDir, problem['problem']); \n",
    "    problem['candidates_folder_count'] = len(candidates_folder);\n",
    "    problem['candidates'] = [];\n",
    "    for candidate in candidates_folder:\n",
    "        problem['candidates'].extend(read_files(pathjoin(inputDir, problem['problem']),candidate));\n",
    "    \n",
    "    problem['unknown'] = read_files(pathjoin(inputDir, problem['problem']),unk_folder);    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>candidates</th>\n",
       "      <th>candidates_folder_count</th>\n",
       "      <th>encoding</th>\n",
       "      <th>language</th>\n",
       "      <th>problem</th>\n",
       "      <th>unknown</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[(graceful ones.\\n\\n\"One more,\" Marvelous said...</td>\n",
       "      <td>20</td>\n",
       "      <td>UTF-8</td>\n",
       "      <td>en</td>\n",
       "      <td>problem00001</td>\n",
       "      <td>[(after all, his best friends. And what in the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[(a mission.\"\\n\\nJensen just raises an eyebrow...</td>\n",
       "      <td>5</td>\n",
       "      <td>UTF-8</td>\n",
       "      <td>en</td>\n",
       "      <td>problem00002</td>\n",
       "      <td>[(“Potter was attractive,” Draco thought, sigh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[(qui l'avait tué mais tout était de la faute ...</td>\n",
       "      <td>20</td>\n",
       "      <td>UTF-8</td>\n",
       "      <td>fr</td>\n",
       "      <td>problem00003</td>\n",
       "      <td>[(son réveil. Sa main pulse et Draco frotte l'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[(. Le canapé est vide et lorsqu'il passe deva...</td>\n",
       "      <td>5</td>\n",
       "      <td>UTF-8</td>\n",
       "      <td>fr</td>\n",
       "      <td>problem00004</td>\n",
       "      <td>[(abasourdie.\\n\\nTout d'abord, elle crut que s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[(Eppure lui la mappa l’aveva stampata, dannaz...</td>\n",
       "      <td>20</td>\n",
       "      <td>UTF-8</td>\n",
       "      <td>it</td>\n",
       "      <td>problem00005</td>\n",
       "      <td>[(– Oh. Cazzo.\\nSirius era così sconvolto che ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[(Yato ha trovato una lettera sul suo comodino...</td>\n",
       "      <td>5</td>\n",
       "      <td>UTF-8</td>\n",
       "      <td>it</td>\n",
       "      <td>problem00006</td>\n",
       "      <td>[(così la tua vista, Moony?\\n– Cercavo di esse...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[(zmienił zdanie. Niech się stworzonko pobawi....</td>\n",
       "      <td>20</td>\n",
       "      <td>UTF-8</td>\n",
       "      <td>pl</td>\n",
       "      <td>problem00007</td>\n",
       "      <td>[(dawniej pełna radości i ciepła, a teraz wiec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[(Słowem, które Sherlock najczęściej słyszał w...</td>\n",
       "      <td>5</td>\n",
       "      <td>UTF-8</td>\n",
       "      <td>pl</td>\n",
       "      <td>problem00008</td>\n",
       "      <td>[(, uderzającego o żebra niczym dzwon- niemal ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[(pero no lo ama como ama a Guignol –explicó e...</td>\n",
       "      <td>20</td>\n",
       "      <td>UTF-8</td>\n",
       "      <td>sp</td>\n",
       "      <td>problem00009</td>\n",
       "      <td>[(–La nariz puntiaguda del elfo casi rozaba el...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[(incapaz de señalar un momento exacto, un pun...</td>\n",
       "      <td>5</td>\n",
       "      <td>UTF-8</td>\n",
       "      <td>sp</td>\n",
       "      <td>problem00010</td>\n",
       "      <td>[(tan parecidas hizo que su trasero latiese de...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          candidates  candidates_folder_count  \\\n",
       "0  [(graceful ones.\\n\\n\"One more,\" Marvelous said...                       20   \n",
       "1  [(a mission.\"\\n\\nJensen just raises an eyebrow...                        5   \n",
       "2  [(qui l'avait tué mais tout était de la faute ...                       20   \n",
       "3  [(. Le canapé est vide et lorsqu'il passe deva...                        5   \n",
       "4  [(Eppure lui la mappa l’aveva stampata, dannaz...                       20   \n",
       "5  [(Yato ha trovato una lettera sul suo comodino...                        5   \n",
       "6  [(zmienił zdanie. Niech się stworzonko pobawi....                       20   \n",
       "7  [(Słowem, które Sherlock najczęściej słyszał w...                        5   \n",
       "8  [(pero no lo ama como ama a Guignol –explicó e...                       20   \n",
       "9  [(incapaz de señalar un momento exacto, un pun...                        5   \n",
       "\n",
       "  encoding language       problem  \\\n",
       "0    UTF-8       en  problem00001   \n",
       "1    UTF-8       en  problem00002   \n",
       "2    UTF-8       fr  problem00003   \n",
       "3    UTF-8       fr  problem00004   \n",
       "4    UTF-8       it  problem00005   \n",
       "5    UTF-8       it  problem00006   \n",
       "6    UTF-8       pl  problem00007   \n",
       "7    UTF-8       pl  problem00008   \n",
       "8    UTF-8       sp  problem00009   \n",
       "9    UTF-8       sp  problem00010   \n",
       "\n",
       "                                             unknown  \n",
       "0  [(after all, his best friends. And what in the...  \n",
       "1  [(“Potter was attractive,” Draco thought, sigh...  \n",
       "2  [(son réveil. Sa main pulse et Draco frotte l'...  \n",
       "3  [(abasourdie.\\n\\nTout d'abord, elle crut que s...  \n",
       "4  [(– Oh. Cazzo.\\nSirius era così sconvolto che ...  \n",
       "5  [(così la tua vista, Moony?\\n– Cercavo di esse...  \n",
       "6  [(dawniej pełna radości i ciepła, a teraz wiec...  \n",
       "7  [(, uderzającego o żebra niczym dzwon- niemal ...  \n",
       "8  [(–La nariz puntiaguda del elfo casi rozaba el...  \n",
       "9  [(tan parecidas hizo que su trasero latiese de...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(problems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#*******************************************************************************************************\n",
    "import warnings\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "def eval_measures(gt, pred):\n",
    "    \"\"\"Compute macro-averaged F1-scores, macro-averaged precision, \n",
    "    macro-averaged recall, and micro-averaged accuracy according the ad hoc\n",
    "    rules discussed at the top of this file.\n",
    "    Parameters\n",
    "    ----------\n",
    "    gt : dict\n",
    "        Ground truth, where keys indicate text file names\n",
    "        (e.g. `unknown00002.txt`), and values represent\n",
    "        author labels (e.g. `candidate00003`)\n",
    "    pred : dict\n",
    "        Predicted attribution, where keys indicate text file names\n",
    "        (e.g. `unknown00002.txt`), and values represent\n",
    "        author labels (e.g. `candidate00003`)\n",
    "    Returns\n",
    "    -------\n",
    "    f1 : float\n",
    "        Macro-averaged F1-score\n",
    "    precision : float\n",
    "        Macro-averaged precision\n",
    "    recall : float\n",
    "        Macro-averaged recall\n",
    "    accuracy : float\n",
    "        Micro-averaged F1-score\n",
    "    \"\"\"\n",
    "\n",
    "    actual_authors = list(gt.values())\n",
    "    encoder = LabelEncoder().fit(['<UNK>'] + actual_authors)\n",
    "\n",
    "    text_ids, gold_authors, silver_authors = [], [], []\n",
    "    for text_id in sorted(gt):\n",
    "        text_ids.append(text_id)\n",
    "        gold_authors.append(gt[text_id])\n",
    "        try:\n",
    "            silver_authors.append(pred[text_id])\n",
    "        except KeyError:\n",
    "            # missing attributions get <UNK>:\n",
    "            silver_authors.append('<UNK>')\n",
    "\n",
    "    assert len(text_ids) == len(gold_authors)\n",
    "    assert len(text_ids) == len(silver_authors)\n",
    "\n",
    "    # replace non-existent silver authors with '<UNK>':\n",
    "    silver_authors = [a if a in encoder.classes_ else '<UNK>' \n",
    "                      for a in silver_authors]\n",
    "\n",
    "    gold_author_ints   = encoder.transform(gold_authors)\n",
    "    silver_author_ints = encoder.transform(silver_authors)\n",
    "\n",
    "    # get F1 for individual classes (and suppress warnings):\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter('ignore')\n",
    "        f1 = f1_score(gold_author_ints,\n",
    "                  silver_author_ints,\n",
    "                  labels=list(set(gold_author_ints)),\n",
    "                  average='macro')\n",
    "        precision = precision_score(gold_author_ints,\n",
    "                  silver_author_ints,\n",
    "                  labels=list(set(gold_author_ints)),\n",
    "                  average='macro')\n",
    "        recall = recall_score(gold_author_ints,\n",
    "                  silver_author_ints,\n",
    "                  labels=list(set(gold_author_ints)),\n",
    "                  average='macro')\n",
    "        accuracy = accuracy_score(gold_author_ints,\n",
    "                  silver_author_ints)\n",
    "\n",
    "    return f1,precision,recall,accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(ground_truth_file,predictions_file):\n",
    "    # Calculates evaluation measures for a single attribution problem\n",
    "    gt = {}\n",
    "    with open(ground_truth_file, 'r') as f:\n",
    "        for attrib in json.load(f)['ground_truth']:\n",
    "            gt[attrib['unknown-text']] = attrib['true-author']\n",
    "\n",
    "    pred = {}\n",
    "    with open(predictions_file, 'r') as f:\n",
    "        for attrib in json.load(f):\n",
    "            if attrib['unknown-text'] not in pred:\n",
    "                pred[attrib['unknown-text']] = attrib['predicted-author']\n",
    "    f1,precision,recall,accuracy =  eval_measures(gt,pred)\n",
    "    return f1, precision, recall, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "from scipy.sparse import issparse\n",
    "\n",
    "\n",
    "class DenseTransformer(BaseEstimator):\n",
    "    \"\"\"Convert a sparse array into a dense array.\"\"\"\n",
    "\n",
    "    def __init__(self, return_copy=True):\n",
    "        self.return_copy = return_copy\n",
    "        self.is_fitted = False\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        \"\"\" Return a dense version of the input array.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n",
    "            Training vectors, where n_samples is the number of samples and\n",
    "            n_features is the number of features.\n",
    "        y : array-like, shape = [n_samples] (default: None)\n",
    "        Returns\n",
    "        ---------\n",
    "        X_dense : dense version of the input X array.\n",
    "        \"\"\"\n",
    "        if issparse(X):\n",
    "            return X.toarray()\n",
    "        elif self.return_copy:\n",
    "            return X.copy()\n",
    "        else:\n",
    "            return X\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\" Mock method. Does nothing.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n",
    "            Training vectors, where n_samples is the number of samples and\n",
    "            n_features is the number of features.\n",
    "        y : array-like, shape = [n_samples] (default: None)\n",
    "        Returns\n",
    "        ---------\n",
    "        self\n",
    "        \"\"\"\n",
    "        self.is_fitted = True\n",
    "        return self\n",
    "\n",
    "    def fit_transform(self, X, y=None):\n",
    "        \"\"\" Return a dense version of the input array.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n",
    "            Training vectors, where n_samples is the number of samples and\n",
    "            n_features is the number of features.\n",
    "        y : array-like, shape = [n_samples] (default: None)\n",
    "        Returns\n",
    "        ---------\n",
    "        X_dense : dense version of the input X array.\n",
    "        \"\"\"\n",
    "        return self.transform(X=X, y=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "from scipy.sparse import issparse\n",
    "\n",
    "\n",
    "class ObfuscationTransformer(BaseEstimator):\n",
    "    def __init__(self,re_from=r'(\\b)(\\w{0,2})\\w+(\\w{1,3})(\\b)', re_to=r'\\1\\2XX\\3\\4', return_copy=True):\n",
    "        self.re_from = re_from\n",
    "        self.re_to = re_to\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        X = np.array(X).copy();\n",
    "        for i in range(len(X)):\n",
    "            X[i] = re.sub(self.re_from,self.re_to, X[i])\n",
    "        \n",
    "        return X;\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def fit_transform(self, X, y=None):\n",
    "        return self.transform(X=X, y=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runML(problem):\n",
    "    print (\"\\nProblem: %s,  language: %s, \" %(problem['problem'],problem['language']))\n",
    "    \n",
    "    train_docs, train_labels, _   = zip(*problem['candidates'])\n",
    "    problem['training_docs_size'] = len(train_docs);\n",
    "    test_docs, _, test_filename   = zip(*problem['unknown'])\n",
    "    \n",
    "    cachedir = mkdtemp()\n",
    "    pipeline = Pipeline([\n",
    "        ('obs',ObfuscationTransformer(re_from=r'\\w',re_to='x')),\n",
    "        ('vect',   TfidfVectorizer(analyzer='char',\n",
    "                                   min_df=0.05,\n",
    "                                   max_df=1.0,\n",
    "                                   norm='l2',\n",
    "                                   lowercase =False,\n",
    "                                   sublinear_tf=True)),\n",
    "        ('dense',  DenseTransformer()),\n",
    "        ('scaler', MaxAbsScaler()),\n",
    "        ('transf', PCA(0.99)),\n",
    "        ('clf', LogisticRegression(random_state=0,multi_class='multinomial', solver='newton-cg')),\n",
    "    ], memory=cachedir)\n",
    "    \n",
    "    \n",
    "    # uncommenting more parameters will give better exploring power but will\n",
    "    # increase processing time in a combinatorial way\n",
    "    parameters = {\n",
    "        'vect__ngram_range':((1,2),(2,3),(2,4),(2,5),(3,5)),\n",
    "        'transf__n_components': (0.1,0.5,0.9,0.99),\n",
    "    }\n",
    "    \n",
    "    grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=False)\n",
    "    \n",
    "    print(\"Performing grid search...\")\n",
    "    t0 = time()\n",
    "    grid_search.fit(train_docs, train_labels)\n",
    "    print(\"done in %0.3fs\" % (time() - t0))\n",
    "\n",
    "    print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "    print(\"Best parameters set:\")\n",
    "    best_parameters = grid_search.best_estimator_.get_params()\n",
    "    for param_name in sorted(parameters.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "        \n",
    "    train_pred=grid_search.predict(train_docs);\n",
    "    test_pred=grid_search.predict(test_docs);\n",
    "    \n",
    "    \n",
    "    # Writing output file\n",
    "    out_data=[]\n",
    "    for i,v in enumerate(test_pred):\n",
    "        out_data.append({'unknown-text': test_filename[i],'predicted-author': v})\n",
    "    answerFile = pathjoin(outputDir,'answers-'+problem['problem']+'.json');\n",
    "    with open(answerFile, 'w') as f:\n",
    "        json.dump(out_data, f, indent=4)\n",
    "        #allProblems.extend(out_data)\n",
    "    \n",
    "    \n",
    "    #evaluation train\n",
    "    f1,precision,recall,accuracy=evaluate(\n",
    "                pathjoin(inputDir, problem['problem'], 'ground-truth.json'),\n",
    "                answerFile)\n",
    "    rmtree(cachedir)\n",
    "    return {\n",
    "                'problem-name'  :       problem['problem'],\n",
    "                \"language\"      :       problem['language'],\n",
    "                'AuthorCount'   :       len(set(train_labels)),\n",
    "                \"train_doc_size\":       len(train_docs),\n",
    "                \"train_caract_per_doc\": sum([len(l) for l in train_docs])/len(train_docs),\n",
    "                \"test_doc_size\" :       len(test_docs),\n",
    "                \"test_caract_per_doc\":  sum([len(l) for l in test_docs])/len(test_docs),\n",
    "                \n",
    "                'macro-f1'       : round(f1,3),\n",
    "                'macro-precision': round(precision,3),\n",
    "                'macro-recall'   : round(recall,3),\n",
    "                'micro-accuracy' : round(accuracy,3),\n",
    "                \n",
    "        }, grid_search.cv_results_;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Problem: problem00001,  language: en, \n",
      "Performing grid search...\n",
      "done in 54.120s\n",
      "Best score: 0.621\n",
      "Best parameters set:\n",
      "\ttransf__n_components: 0.99\n",
      "\tvect__ngram_range: (2, 4)\n",
      "\n",
      "Problem: problem00002,  language: en, \n",
      "Performing grid search...\n",
      "done in 17.451s\n",
      "Best score: 0.943\n",
      "Best parameters set:\n",
      "\ttransf__n_components: 0.99\n",
      "\tvect__ngram_range: (2, 3)\n",
      "\n",
      "Problem: problem00003,  language: fr, \n",
      "Performing grid search...\n",
      "done in 56.372s\n",
      "Best score: 0.607\n",
      "Best parameters set:\n",
      "\ttransf__n_components: 0.99\n",
      "\tvect__ngram_range: (2, 5)\n",
      "\n",
      "Problem: problem00004,  language: fr, \n",
      "Performing grid search...\n",
      "done in 15.224s\n",
      "Best score: 0.629\n",
      "Best parameters set:\n",
      "\ttransf__n_components: 0.99\n",
      "\tvect__ngram_range: (1, 2)\n",
      "\n",
      "Problem: problem00005,  language: it, \n",
      "Performing grid search...\n",
      "done in 59.542s\n",
      "Best score: 0.657\n",
      "Best parameters set:\n",
      "\ttransf__n_components: 0.99\n",
      "\tvect__ngram_range: (2, 5)\n",
      "\n",
      "Problem: problem00006,  language: it, \n",
      "Performing grid search...\n",
      "done in 17.214s\n",
      "Best score: 0.829\n",
      "Best parameters set:\n",
      "\ttransf__n_components: 0.9\n",
      "\tvect__ngram_range: (1, 2)\n",
      "\n",
      "Problem: problem00007,  language: pl, \n",
      "Performing grid search...\n",
      "done in 70.862s\n",
      "Best score: 0.764\n",
      "Best parameters set:\n",
      "\ttransf__n_components: 0.99\n",
      "\tvect__ngram_range: (2, 5)\n",
      "\n",
      "Problem: problem00008,  language: pl, \n",
      "Performing grid search...\n",
      "done in 19.236s\n",
      "Best score: 0.943\n",
      "Best parameters set:\n",
      "\ttransf__n_components: 0.9\n",
      "\tvect__ngram_range: (2, 3)\n",
      "\n",
      "Problem: problem00009,  language: sp, \n",
      "Performing grid search...\n",
      "done in 67.142s\n",
      "Best score: 0.664\n",
      "Best parameters set:\n",
      "\ttransf__n_components: 0.99\n",
      "\tvect__ngram_range: (1, 2)\n",
      "\n",
      "Problem: problem00010,  language: sp, \n",
      "Performing grid search...\n",
      "done in 19.442s\n",
      "Best score: 0.771\n",
      "Best parameters set:\n",
      "\ttransf__n_components: 0.99\n",
      "\tvect__ngram_range: (2, 3)\n"
     ]
    }
   ],
   "source": [
    "result = [];\n",
    "cv_result = [];\n",
    "for problem in problems:\n",
    "    r, c = runML(problem);\n",
    "    result.append(r);\n",
    "    cv_result.append(c);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame(result)[['problem-name',\n",
    "                     \"language\",\n",
    "                     'AuthorCount',\n",
    "                     \"train_doc_size\",\"train_caract_per_doc\",\n",
    "                     \"test_doc_size\", \"test_caract_per_doc\",\n",
    "                     'macro-f1','macro-precision','macro-recall' ,'micro-accuracy']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>problem-name</th>\n",
       "      <th>language</th>\n",
       "      <th>AuthorCount</th>\n",
       "      <th>train_doc_size</th>\n",
       "      <th>train_caract_per_doc</th>\n",
       "      <th>test_doc_size</th>\n",
       "      <th>test_caract_per_doc</th>\n",
       "      <th>macro-f1</th>\n",
       "      <th>macro-precision</th>\n",
       "      <th>macro-recall</th>\n",
       "      <th>micro-accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>problem00001</td>\n",
       "      <td>en</td>\n",
       "      <td>20</td>\n",
       "      <td>140</td>\n",
       "      <td>4327</td>\n",
       "      <td>105</td>\n",
       "      <td>4370</td>\n",
       "      <td>0.486</td>\n",
       "      <td>0.470</td>\n",
       "      <td>0.681</td>\n",
       "      <td>0.524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>problem00002</td>\n",
       "      <td>en</td>\n",
       "      <td>5</td>\n",
       "      <td>35</td>\n",
       "      <td>4342</td>\n",
       "      <td>21</td>\n",
       "      <td>4296</td>\n",
       "      <td>0.220</td>\n",
       "      <td>0.172</td>\n",
       "      <td>0.380</td>\n",
       "      <td>0.190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>problem00003</td>\n",
       "      <td>fr</td>\n",
       "      <td>20</td>\n",
       "      <td>140</td>\n",
       "      <td>4492</td>\n",
       "      <td>49</td>\n",
       "      <td>4508</td>\n",
       "      <td>0.545</td>\n",
       "      <td>0.537</td>\n",
       "      <td>0.618</td>\n",
       "      <td>0.551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>problem00004</td>\n",
       "      <td>fr</td>\n",
       "      <td>5</td>\n",
       "      <td>35</td>\n",
       "      <td>4522</td>\n",
       "      <td>21</td>\n",
       "      <td>4532</td>\n",
       "      <td>0.270</td>\n",
       "      <td>0.367</td>\n",
       "      <td>0.323</td>\n",
       "      <td>0.333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>problem00005</td>\n",
       "      <td>it</td>\n",
       "      <td>20</td>\n",
       "      <td>140</td>\n",
       "      <td>4720</td>\n",
       "      <td>80</td>\n",
       "      <td>4787</td>\n",
       "      <td>0.506</td>\n",
       "      <td>0.485</td>\n",
       "      <td>0.614</td>\n",
       "      <td>0.650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>problem00006</td>\n",
       "      <td>it</td>\n",
       "      <td>5</td>\n",
       "      <td>35</td>\n",
       "      <td>4847</td>\n",
       "      <td>46</td>\n",
       "      <td>4765</td>\n",
       "      <td>0.609</td>\n",
       "      <td>0.599</td>\n",
       "      <td>0.691</td>\n",
       "      <td>0.804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>problem00007</td>\n",
       "      <td>pl</td>\n",
       "      <td>20</td>\n",
       "      <td>140</td>\n",
       "      <td>5145</td>\n",
       "      <td>103</td>\n",
       "      <td>5200</td>\n",
       "      <td>0.496</td>\n",
       "      <td>0.544</td>\n",
       "      <td>0.543</td>\n",
       "      <td>0.485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>problem00008</td>\n",
       "      <td>pl</td>\n",
       "      <td>5</td>\n",
       "      <td>35</td>\n",
       "      <td>5049</td>\n",
       "      <td>15</td>\n",
       "      <td>5214</td>\n",
       "      <td>0.556</td>\n",
       "      <td>0.550</td>\n",
       "      <td>0.711</td>\n",
       "      <td>0.667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>problem00009</td>\n",
       "      <td>sp</td>\n",
       "      <td>20</td>\n",
       "      <td>140</td>\n",
       "      <td>4794</td>\n",
       "      <td>117</td>\n",
       "      <td>4788</td>\n",
       "      <td>0.470</td>\n",
       "      <td>0.453</td>\n",
       "      <td>0.538</td>\n",
       "      <td>0.470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>problem00010</td>\n",
       "      <td>sp</td>\n",
       "      <td>5</td>\n",
       "      <td>35</td>\n",
       "      <td>4955</td>\n",
       "      <td>64</td>\n",
       "      <td>4827</td>\n",
       "      <td>0.687</td>\n",
       "      <td>0.679</td>\n",
       "      <td>0.709</td>\n",
       "      <td>0.719</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   problem-name language  AuthorCount  train_doc_size  train_caract_per_doc  \\\n",
       "0  problem00001       en           20             140                  4327   \n",
       "1  problem00002       en            5              35                  4342   \n",
       "2  problem00003       fr           20             140                  4492   \n",
       "3  problem00004       fr            5              35                  4522   \n",
       "4  problem00005       it           20             140                  4720   \n",
       "5  problem00006       it            5              35                  4847   \n",
       "6  problem00007       pl           20             140                  5145   \n",
       "7  problem00008       pl            5              35                  5049   \n",
       "8  problem00009       sp           20             140                  4794   \n",
       "9  problem00010       sp            5              35                  4955   \n",
       "\n",
       "   test_doc_size  test_caract_per_doc  macro-f1  macro-precision  \\\n",
       "0            105                 4370     0.486            0.470   \n",
       "1             21                 4296     0.220            0.172   \n",
       "2             49                 4508     0.545            0.537   \n",
       "3             21                 4532     0.270            0.367   \n",
       "4             80                 4787     0.506            0.485   \n",
       "5             46                 4765     0.609            0.599   \n",
       "6            103                 5200     0.496            0.544   \n",
       "7             15                 5214     0.556            0.550   \n",
       "8            117                 4788     0.470            0.453   \n",
       "9             64                 4827     0.687            0.679   \n",
       "\n",
       "   macro-recall  micro-accuracy  \n",
       "0         0.681           0.524  \n",
       "1         0.380           0.190  \n",
       "2         0.618           0.551  \n",
       "3         0.323           0.333  \n",
       "4         0.614           0.650  \n",
       "5         0.691           0.804  \n",
       "6         0.543           0.485  \n",
       "7         0.711           0.667  \n",
       "8         0.538           0.470  \n",
       "9         0.709           0.719  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{rr}\n",
      "\\toprule\n",
      " index &  macro-f1 \\\\\n",
      "\\midrule\n",
      " 0 & 0.486 \\\\\n",
      " 1 & 0.220 \\\\\n",
      " 2 & 0.545 \\\\\n",
      " 3 & 0.270 \\\\\n",
      " 4 & 0.506 \\\\\n",
      " 5 & 0.609 \\\\\n",
      " 6 & 0.496 \\\\\n",
      " 7 & 0.556 \\\\\n",
      " 8 & 0.470 \\\\\n",
      " 9 & 0.687 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(df[[\"macro-f1\"]].reset_index().to_latex(index=False).replace(\"     \",\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>macro-f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.484500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.142227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.220000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.474000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.501000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.553250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.687000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        macro-f1\n",
       "count  10.000000\n",
       "mean    0.484500\n",
       "std     0.142227\n",
       "min     0.220000\n",
       "25%     0.474000\n",
       "50%     0.501000\n",
       "75%     0.553250\n",
       "max     0.687000"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(result)[['macro-f1']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1a0d200dd0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.DataFrame(result)\\\n",
    "    .sort_values(by=['language','problem-name'])[['language','problem-name','macro-f1']]\\\n",
    "    .plot(kind='bar', x=['language','problem-name'], legend=True, figsize=(20,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xxxxxx xxxxx xxxxxxx ç ç\n",
      "\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "print(ObfuscationTransformer(re_from=r'\\w',re_to='x').fit_transform([\"Verdes mares bravios ç ç\\n\\n.\"])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pero no lo ama como ama a Guignol –explicó el niño sin sospechar las implicaciones inmorales de todo el cuento.\n",
      "\n",
      "-¿Y cómo sabes que no lo ama? –Inglaterra se preguntó a su vez si habría un muñeco del esposo también.\n",
      "\n",
      "-Francia me lo contó, dijo que era un romance –Canadá sonrió como si zanjara el asunto al nombrarlo-. ¿Sabe? Francia me los ha regalado, y se ha disculpado y luego me ha prometido muchos dulces y juegos y más dulces.\n",
      "\n",
      "¿Francia había tenido la misma idea mucho antes que él? Qué desgr\n"
     ]
    }
   ],
   "source": [
    "print(problems[8]['candidates'][0][0][0:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xxxx xx xx xxx xxxx xxx x xxxxxxx –xxxxxxó xx xxñx xxx xxxxxxxxx xxx xxxxxxxxxxxxx xxxxxxxxx xx xxxx xx xxxxxx.\n",
      "\n",
      "-¿x xóxx xxxxx xxx xx xx xxx? –xxxxxxxxxx xx xxxxxxxó x xx xxx xx xxxxíx xx xxñxxx xxx xxxxxx xxxxxéx.\n",
      "\n",
      "-xxxxxxx xx xx xxxxó, xxxx xxx xxx xx xxxxxxx –xxxxxá xxxxxó xxxx xx xxxxxxx xx xxxxxx xx xxxxxxxxx-. ¿xxxx? xxxxxxx xx xxx xx xxxxxxxx, x xx xx xxxxxxxxxx x xxxxx xx xx xxxxxxxxx xxxxxx xxxxxx x xxxxxx x xáx xxxxxx.\n",
      "\n",
      "¿xxxxxxx xxxíx xxxxxx xx xxxxx xxxx xxxxx xxxxx xxx éx? xxé xxxxx\n"
     ]
    }
   ],
   "source": [
    "print(ObfuscationTransformer(re_from=r'\\w',re_to='x').fit_transform([problems[8]['candidates'][0][0][0:500]])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
